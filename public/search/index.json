[{"content":"","date":"2023-03-27T00:00:00Z","permalink":"/p/rshiny/","title":"开发我的Rshiny应用"},{"content":"Introduction 宏基因组（Metagenome）是指对一个生态系统中的所有微生物进行DNA分析的过程，可以帮助研究人员了解微生物的多样性、功能和互作关系。\n宏基因组的应用非常广泛，包括：\n生物多样性研究：通过对宏基因组进行分析，可以了解不同生态系统中微生物的多样性和分布情况。\n生态学研究：宏基因组可以帮助研究人员了解微生物在生态系统中的功能、互作关系和生态位等。\n生物技术：宏基因组可以用于筛选具有特定功能的微生物，例如，寻找能够降解有害物质的微生物。\n宏基因组的分析一般包括以下步骤：\nDNA提取与建库。\n高通量测序：使用高通量测序技术对扩增后的DNA进行测序，得到原始序列数据。\n数据清洗和组装：对原始数据进行质量控制、去除低质量序列和冗余序列，将序列拼接成较长的连续序列（contigs）。\n基因注释：将contigs中的基因进行注释，得到基因功能信息。\n数据分析：了解微生物多样性、群落结构、功能特征等信息（更多是指获取了物种丰度表或功能丰度表之后的进一步分析）。\nMAGs binning， 进化动态等进一步分析\n这是我常用的一套基本流程(Figure 1)，当然在面对不同项目时应该有不同的侧重点和适用的分析方法，可以在此基础上添加或修改。\n最早这方面的分析我都是参考刘永鑫老师的EasyMetagenome,现在这套流程也发文章了 (1)，值得参考，对上手16S测序数据或宏基因组数据都很有帮助。\nFigure 1: Basic workflow preprocess 绝大多数这里介绍的软件都是仅支持linux平台的，我们做测序文件的上游分析也肯定是在服务器上做，个人PC一般很难满足需求，所以在做这些分析前必须先学习linux基础知识如文件系统，shell脚本编写，软件安装等。\n安装软件建议使用conda或mamba（新建环境和管理），有很多参考方法。\n我们服务器使用的是slurm作业管理系统，尽量先学习一下slurm的使用再尝试提交作业。\n一般把所有样本的测序双端文件放在一个文件夹下\n质控：fastp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #!/bin/bash #SBATCH --job-name=fastp #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/fastp/log/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/fastp/log/%x_%a.err #SBATCH --array=1-33 #SBATCH --partition=short #SBATCH --cpus-per-task=8 echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` echo \u0026#34;SLURM_ARRAY_TASK_ID: \u0026#34; $SLURM_ARRAY_TASK_ID sample=$(head -n $SLURM_ARRAY_TASK_ID ~/work/asthma/data/namelist | tail -1) #sample=$(head -n 1 namelist | tail -1) echo handling: $sample #################### fastp -w 8 -i ~/work/asthma/data/$sample/$sample\u0026#39;_f1.fastq\u0026#39; -o ${i}_1 \\ -I ~/work/asthma/data/$sample/$sample\u0026#39;_r2.fastq\u0026#39; -O ${i}_2 -j ~/work/asthma/fastp/${i}.json #delete outputfile rm -rf ${i}_1 ${i}_2 #################### echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s 后面接一个python脚本就可以统计常用指标了。\n把所有的.json文件移到一个文件夹里，report/下，就可以统计了。\n去宿主：bowtie2 其实就是将序列比对到人类基因组上，没有比对到的序列整合成新文件就是去宿主后的了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #!/bin/bash #SBATCH --job-name=rm_human #SBATCH --output=/share/home/jianglab/pengchen/work/meta/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/meta/%x_%a.err #SBATCH --cpus-per-task=32 #SBATCH --partition=short echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` ############# for i in C1 C2 do bowtie2 -p 32 -x ~/db/humangenome/hg38 -1 seq/${i}_1.fq.gz \\ -2 seq/${i}_2.fq.gz -S ${i}.sam --un-conc ${i}.fq --very-sensitive done ############## echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s 基本信息统计 可以用FastqCount：\n1 2 3 4 ~/biosoft/FastqCount-master/FastqCount_v0.5 xx.fastq.gz Total Reads Total Bases N Bases Q20 Q30 GC 11568822 (11.57 M) 1702829127 (1.70 G) 0.00% 98.00% 94.00% 54.00% reads-based 物种注释：kraken2 Kraken 2是一个用于对高通量测序数据进行分类和标识物种的软件。它使用参考数据库中的基因组序列来进行分类，并使用k-mer方法来实现快速和准确的分类。\n使用Kraken 2进行基本分类的简单步骤：\n准备参考数据库：Kraken 2需要一个参考数据库，以便对测序数据进行分类。可以从NCBI、Ensembl或其他数据库下载相应的基因组序列，并使用Kraken 2内置的工具来构建数据库。\n安装Kraken 2：可以从Kraken 2官方网站下载并安装Kraken 2软件。\n运行Kraken 2：使用Kraken 2对测序数据进行分类需要使用以下命令：\nkraken2 \\--db \\\u0026lt;path_to_database\\\u0026gt; \\\u0026lt;input_file\\\u0026gt; \\--output \\\u0026lt;output_file\\\u0026gt;\n这里，**\u0026lt;path_to_database\u0026gt;是参考数据库的路径，\u0026lt;input_file\u0026gt;是需要进行分类的输入文件，\u0026lt;output_file\u0026gt;**是输出文件的名称。Kraken 2将输出一个分类报告文件和一个序列文件。\nkraken2-build --standard --threads 24 --db ./\n\u0026ndash;standard标准模式下只下载5种数据库：古菌archaea、细菌bacteria、人类human、载体UniVec_Core、病毒viral。也可选直接下载作者构建的索引，还包括bracken的索引。\n这个kraken数据库是可以自己构建的，所以适用于各种项目的物种注释，我做的比较多的是环境样本的宏基因组，就可能需要更全面的物种数据库（甚至除了各种微生物，还要动植物数据等），实验室的WX师姐收集构建了一个超大的物种库。\n需要注意的是kraken运行至少要提供数据库大小的内存大小（运行内存），因为它会把整个数据库载入内存后进行序列的注释，所以如果发现无法载入数据库的报错，可以尝试调大内存资源。\nkraken软件运行时载入数据库是一个十分耗时的步骤，而每条序列的鉴定时间差不多，所以我们可以将很多样本的fastq文件合并成一个大文件后输入kraken注释，之后再按照序列的数量拆分结果文件，这样多个样本也只需要载入一次数据库，节省时间。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #!/bin/bash #SBATCH --job-name=kraken2M #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/kraken/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/kraken/%x_%a.err #SBATCH --time=14-00:00:00 #SBATCH --partition=mem #SBATCH --cpus-per-task=32 #SBATCH --mem-per-cpu=100G fqp=~/work/asthma/data/CRR205159/ python /share/home/jianglab/shared/krakenDB/K2ols/kraken2M.py -t 32 \\ -i ${fqp} \\ -c 0.05 \\ -s _f1.fastq,_r2.fastq \\ -o ~/work/asthma/kraken/ \\ -d /share/home/jianglab/shared/krakenDB/mydb2 \\ -k ~/miniconda3/envs/waste/bin/kraken2 \\ -kt /share/home/jianglab/shared/krakenDB/K2ols/KrakenTools 输出文件格式 Kraken标准输出格式\n五列表 output\nC/U代表分类classified或非分类unclassifed\n序列ID\n物种注释\n比序列注释的区域，如98|94代表左端98bp，右端94bp比对至数据库\nLCA比对结果，如”562:13 561:4”代表13 k-mer比对至物种#562，4 k-mer比对至#561物种\n报告输出格式 report\n包括6列，方便整理下游分析。\n百分比\ncount\ncount最优\n(U)nclassified, (R)oot, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. “G2”代表位于属一种间\nNCBI物种ID\n科学物种名\n常用的物种丰度表格式除了kraken report，还有mpa，spf，krona等格式，关于kraken结果的整理以及格式转换方式，有一些现成的脚本或者自己写。\nKrakenTools (jhu.edu) 就是一套很好用的kraken工具包，其中常用的有：\nextract_kraken_reads.py 此程序提取读取在任何用户指定的分类id处分类的内容。用户必须指定Kraken输出文件、序列文件和至少一个分类法ID。下面指定了其他选项。截至2021年4月19日，此脚本与KrakenUniq/Kraken2Uniq报告兼容。\ncombine_kreports.py This script combines multiple Kraken reports into a combined report file.\npython combine_kreports.py\n-r 1.KREPORT 2.KREPORT\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;Kraken-style reports to combine\n-o COMBINED.KREPORT\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;Output file\nkreport2krona.py This program takes a Kraken report file and prints out a krona-compatible TEXT file\n换成krona文件好画图。嘿嘿\nkrona装了一个excel的插件可以很容易画图\npython kreport2krona.py\n-r/\u0026ndash;report MYFILE.KREPORT\u0026hellip;.….Kraken report file\n-o/\u0026ndash;output MYFILE.KRONA\u0026hellip;\u0026hellip;.…Output Krona text file\nthen, ktImportText MYSAMPLE.krona -o MYSAMPLE.krona.html\n好看的网页就出来了。\nkreport2mpa.py This program takes a Kraken report file and prints out a mpa (MetaPhlAn) -style TEXT file\npython kreport2mpa.py\n-r/\u0026ndash;report MYFILE.KREPORT\u0026hellip;.….Kraken report file\n-o/\u0026ndash;output MYFILE.MPA.TXT\u0026hellip;.….Output MPA-STYLE text file\ncombine_mpa.py python combine_mpa.py\n-i/\u0026ndash;input MYFILE1.MPA MYFILE2.MPA\u0026hellip;.…Multiple MPA-STYLE text files (separated by spaces)\n-o/\u0026ndash;output MYFILE.COMBINED.MPA\u0026hellip;\u0026hellip;.…Output MPA-STYLE text file\nThis program combines multiple outputs from kreport2mpa.py. Files to be combined must have been generated using the same kreport2mpa.py options.\npython combine_mpa.py -i -o --intermediate-ranks\nHUMAnN HUMAnN2（The HMP Unified Metabolic Analysis Network 2）是一款用于分析人类微生物组的功能和代谢能力的工具。它通过将宏基因组序列与参考基因组数据库比对，利用MetaCyc代谢通路数据库和UniRef蛋白质序列数据库，分析微生物组在功能和代谢通路水平上的组成和活性。HUMAnN2还提供了多样性分析、关联分析和可视化工具，可用于深入研究人类微生物组对宿主健康的影响和治疗策略的制定等方面。\nHUMAnN2是由美国国家人类微生物组计划（HMP）开发的，目前最新版本为HUMAnN3，于2020年发布。与HUMAnN2相比，HUMAnN3改进了基因家族注释的方法，提高了注释精度和速度，并提供了新的功能和工具，如功能韧度分析、代谢指纹识别和多样性分析等。\n但是HUMAnN2的数据库基本都是与人相关的微生物，比较适合做各种人体微生物组（肠道，肺部，口腔，皮肤等等），对于环境样本可能unclassified比较多。\nHUMAnN2要求双端序列合并的文件作为输入，for循环根据实验设计样本名批量双端序列合并。\n物种组成调用MetaPhlAn2, bowtie2比对至核酸序列，解决有哪些微生物存在的问题；\n功能组成为humann2调用diamond比对至蛋白库11Gb，解决这些微生物参与哪些功能通路的问题；\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 cd alldata for i in `cat ~/work/asthma/data/namelist` do echo $i cat ${i}_f1.fastq ${i}_r2.fastq \u0026gt;${i}_paired.fastq done #!/bin/bash #SBATCH --job-name=humann2 #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/humann/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/humann/%x_%a.err #SBATCH --array=1-32 #SBATCH --cpus-per-task=24 #SBATCH --partition=cpu echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` ############## myarray=(`cat ~/work/asthma/data/namelist`) echo $SLURM_ARRAY_TASK_ID #this is your single file name sample=${myarray[${SLURM_ARRAY_TASK_ID}]} echo handling: $sample humann2 --input data/alldata/${sample}_paired.fastq \\ --output temp/humann2/ --threads 24 ## 链接重要文件至humann2目录 ln temp/humann2/${sample}_paired_humann2_temp/${sample}_paired_metaphlan_bugs_list.tsv temp/humann2/ ## 删除临时文件 rm -rf temp/humann2/${sample}_paired_humann2_temp ############## echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s ## 合并、修正样本名、预览 merge_metaphlan_tables2.py \\ temp/humann2/*_metaphlan_bugs_list.tsv | \\ sed \u0026#39;s/_metaphlan_bugs_list//g\u0026#39; \\ \u0026gt; metaphlan2/taxonomy.tsv contigs-based 组装：megahit MegaHit是一个用于对高通量测序数据进行de novo组装的软件。它使用了一种基于短读比对和图形构建的算法来组装基因组，可以高效地处理大规模的数据集。以下是MegaHit的一些优点和适用情况：\n速度快：MegaHit的算法非常高效，可以处理大规模的数据集，通常比其他de novo组装工具更快。\n高质量的组装：MegaHit在组装结果的连通性和准确性方面表现优异，尤其在处理高GC含量基因组时效果显著。\n适用于不同类型的测序数据：MegaHit支持多种不同类型的测序数据，包括 Illumina HiSeq/MiSeq、IonTorrent和PacBio等平台。\n易于使用：MegaHit具有简单的命令行语法，方便用户进行组装操作，且具有中断点，避免失败后全部重跑。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #!/bin/bash #SBATCH --job-name=asthma_megahit #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/megahit/log/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/megahit/log/%x_%a.err #SBATCH --array=1-33 #SBATCH --partition=cpu #SBATCH --cpus-per-task=32 echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` echo \u0026#34;SLURM_ARRAY_TASK_ID: \u0026#34; $SLURM_ARRAY_TASK_ID sample=$(head -n $SLURM_ARRAY_TASK_ID ~/work/asthma/data/namelist | tail -1) #sample=$(head -n 1 namelist | tail -1) echo handling: $sample\t#################### megahit -t 32 -1 ~/work/asthma/data/$sample/$sample\u0026#39;_f1.fastq\u0026#39; \\ -2 ~/work/asthma/data/$sample/$sample\u0026#39;_r2.fastq\u0026#39; -o ~/work/asthma/megahit/$sample --out-prefix $sample #################### echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s 组装评估：QUAST QUAST代表质量评估工具。 QUAST可以使用参考基因组以及不使用参考基因组来评估装配。 QUAST生成详细的报告，表格和图解，以显示装配的不同方面。\n基因预测：Prodigal 输入文件：拼装好的序列文件 megahit/final.contigs.fa\n输出文件：prodigal预测的基因序列 prodigal/gene.fa\nprodigal不支持多线程运行，所以我们可以自行分割序列文件调用多个prodigal程序分别跑实现伪多线程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #!/bin/bash #SBATCH --job-name=prodigal #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/prodigal/log/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/prodigal/log/%x_%a.err #SBATCH --array=1-33 #SBATCH --partition=cpu #SBATCH --cpus-per-task=1 echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` echo \u0026#34;SLURM_ARRAY_TASK_ID: \u0026#34; $SLURM_ARRAY_TASK_ID sample=$(head -n $SLURM_ARRAY_TASK_ID ~/work/asthma/data/namelist | tail -1) #sample=$(head -n 1 namelist | tail -1) echo handling: $sample #################### prodigal -i ~/work/asthma/megahit/contigs/$sample.fa \\ -d ~/work/asthma/prodigal/$sample.gene.fa \\ -o ~/work/asthma/prodigal/$sample.gene.gff \\ -p meta -f gff grep \u0026#39;partial=00\u0026#39; ~/work/asthma/prodigal/$sample.gene.fa | cut -f1 -d \u0026#39; \u0026#39;| sed \u0026#39;s/\u0026gt;//\u0026#39; \u0026gt; ~/work/asthma/prodigal/$sample.fullid seqkit grep -f ~/work/asthma/prodigal/$sample.fullid ~/work/asthma/prodigal/$sample.gene.fa \u0026gt; ~/work/asthma/prodigal/fullgene/$sample.gene.fa #################### echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s ============================================================================ ## 统计基因数量 grep -c \u0026#39;\u0026gt;\u0026#39; temp/prodigal/gene.fa ## 统计完整基因数量，数据量大可只用完整基因部分 grep -c \u0026#39;partial=00\u0026#39; temp/prodigal/gene.fa ## 提取完整基因(完整片段获得的基因全为完整，如成环的细菌基因组) grep \u0026#39;partial=00\u0026#39; temp/prodigal/gene.fa | cut -f1 -d \u0026#39; \u0026#39;| sed \u0026#39;s/\u0026gt;//\u0026#39; \u0026gt; temp/prodigal/full_length.id seqkit grep -f temp/prodigal/full_length.id temp/prodigal/gene.fa \u0026gt; temp/prodigal/full_length.fa seqkit stat temp/prodigal/full_length.fa 去冗余： Cd-hit 上面产生了n个样本的基因预测结果文件，gene.fa文件要想办法整合为一个文件再去去冗余。\n1 2 3 4 5 6 7 8 9 10 #!/bin/bash #修改每条序列的名称，加上样本名 for i in `cat ~/work/asthma/data/namelist` do echo $i sed -i \u0026#34;/\u0026gt;/s/\u0026gt;/\u0026gt;${i}_/\u0026#34; $i.gene.fa done echo \u0026#39;start merge\u0026#39; cat *.gene.fa\u0026gt;all.fullgene.fa echo \u0026#39;done\u0026#39; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #!/bin/bash #SBATCH --job-name=cdhit #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/%x_%a.err #SBATCH --cpus-per-task=32 #SBATCH --partition=short echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` ############## ## 输入文件：prodigal预测的基因序列 all.fullgene.fa ## 输出文件：去冗余后的基因和蛋白序列：NR/nucleotide.fa;NR/protein.fa mkdir NR ## aS覆盖度，c相似度，G局部比对，g最优解，T多线程，M内存0不限制 ## 2万基因2m，2千万需要2000h，多线程可加速\tcd-hit-est -i prodigal/fullgene/all.fullgene.fa \\ -o NR/nucleotide.fa \\ -aS 0.9 -c 0.9 -G 0 -g 0 -T 0 -M 0 ## 统计非冗余基因数量，单次拼接结果数量下降不大，多批拼接冗余度高 echo \u0026#39;after remove, the number of genes: \u0026#39; grep -c \u0026#39;\u0026gt;\u0026#39; NR/nucleotide.fa ## 翻译核酸为对应蛋白序列，emboss ## emboss transeq工具，93.9 MB conda install emboss -y transeq -sequence NR/nucleotide.fa \\ -outseq NR/protein.fa -trim Y ## 序列名自动添加了_1，为与核酸对应要去除 sed -i \u0026#39;s/_1 / /\u0026#39; NR/protein.fa ############## echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s 基因定量：salmon 建立索引 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/bash #SBATCH --job-name=salmon-index #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/%x_%a.err #SBATCH --cpus-per-task=32 #SBATCH --partition=short echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` ############## mkdir -p temp/salmon ## 建索引, -t序列, -i 索引，10s salmon index \\ -t NR/nucleotide.fa \\ -p 32 \\ -i temp/salmon/index ############## echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s 对每个样本定量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #!/bin/bash #SBATCH --job-name=salmon #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/salmon/log/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/salmon/log/%x_%a.err #SBATCH --array=1-33 #SBATCH --partition=cpu #SBATCH --cpus-per-task=32 echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` echo \u0026#34;SLURM_ARRAY_TASK_ID: \u0026#34; $SLURM_ARRAY_TASK_ID sample=$(head -n $SLURM_ARRAY_TASK_ID ~/work/asthma/data/namelist | tail -1) #sample=$(head -n 1 namelist | tail -1) echo handling: $sample #################### ## 输入文件：去冗余后的基因和蛋白序列：NR/nucleotide.fa ## 输出文件：Salmon定量后的结果：salmon/gene.count;salmon/gene.TPM ## 定量，l文库类型自动选择，p线程，--meta宏基因组模式 salmon quant \\ -i temp/salmon/index -l A -p 32 --meta \\ -1 data/alldata/${sample}_f1.fastq \\ -2 data/alldata/${sample}_r2.fastq \\ -o temp/salmon/${sample}.quant #################### echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s 合并各样本结果 1 2 3 4 5 6 7 8 9 10 11 12 ## 合并 mkdir -p salmon salmon quantmerge \\ --quants temp/salmon/*.quant \\ -o salmon/gene.TPM salmon quantmerge \\ --quants temp/salmon/*.quant \\ --column NumReads -o salmon/gene.count sed -i \u0026#39;1 s/.quant//g\u0026#39; salmon/gene.* ## 预览结果表格 head -n3 salmon/gene.* 功能基因注释 上一步已经有了所有的基因和每个样本所有基因的read count定量结果，我们只需要对上一步的基因序列（或蛋白质序列）进行不同数据库的注释（很多软件都是用diamond比对，如果没有专用软件的数据库我们也可以自己用diamond比对），合并注释结果得到的就是功能丰度表。\ndiamond选择\u0026ndash;outfmt 6的输出结果和blastp一样：\n1. qseqid query sequence id 2. sseqid subject (e.g., reference genome) sequence id 3. pident percentage of identical matches 4. length alignment length 5. mismatch number of mismatches 6. gapopen number of gap openings 7. qstart start of alignment in query 8. qend end of alignment in query 9. sstart start of alignment in subject 10. send end of alignment in subject 11. evalue expect value 12. bitscore bit score 1 eggNOG(COG/KEGG/CAZy) EggNOG数据库收集了COG（Clusters of Orthologous Groups of proteins，直系同源蛋白簇）,构成每个COG的蛋白都是被假定为来自于一个祖先蛋白，因此是orthologs或者是paralogs。通过把所有完整基因组的编码蛋白一个一个的互相比较确定的。在考虑来自一个给定基因组的蛋白时，这种比较将给出每个其他基因组的一个最相似的蛋白（因此需要用完整的基因组来定义COG），这些基因的每一个都轮番地被考虑。如果在这些蛋白（或子集）之间一个相互的最佳匹配关系被发现，那么那些相互的最佳匹配将形成一个COG。这样，一个COG中的成员将与这个COG中的其他成员比起被比较的基因组中的其他蛋白更相像。\nEggNOG里面包含了GO，KEGG，CAZy等。\n1 2 3 4 5 6 7 8 9 ## 下载常用数据库，注意设置下载位置 mkdir -p ${db}/eggnog5 \u0026amp;\u0026amp; cd ${db}/eggnog5 ## -y默认同意，-f强制下载，eggnog.db.gz 7.9G+4.9G download_eggnog_data.py -y -f --data_dir ./ ## 下载方式2(可选)：链接直接下载 wget -c http://eggnog5.embl.de/download/emapperdb-5.0.0/eggnog.db.gz ## 7.9G wget -c http://eggnog5.embl.de/download/emapperdb-5.0.0/eggnog_proteins.dmnd.gz ## 4.9G gunzip *.gz 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #!/bin/bash #SBATCH --job-name=eggo #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/%x_%j.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/%x_%j.err #SBATCH --cpus-per-task=32 #SBATCH --partition=cpu echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` ############## #切换环境 ## diamond比对基因至eggNOG 5.0数据库, 1~9h，默认diamond 1e-3 mkdir -p temp/eggnog emapper.py --no_annot --no_file_comments --override \\ --data_dir ~/db/eggnog5 \\ -i NR/protein.fa \\ --cpu 32 -m diamond \\ -o temp/eggnog/protein ## 比对结果功能注释, 1h emapper.py \\ --annotate_hits_table temp/eggnog/protein.emapper.seed_orthologs \\ --data_dir ~/db/eggnog5 \\ --cpu 32 --no_file_comments --override \\ -o temp/eggnog/output ## 添表头, 1列为ID，9列KO，16列CAZy，21列COG，22列描述 sed \u0026#39;1 i Name\\tortholog\\tevalue\\tscore\\ttaxonomic\\tprotein\\tGO\\tEC\\tKO\\tPathway\\tModule\\tReaction\\trclass\\tBRITE\\tTC\\tCAZy\\tBiGG\\ttax_scope\\tOG\\tbestOG\\tCOG\\tdescription\u0026#39; \\ temp/eggnog/output.emapper.annotations \\ \u0026gt; temp/eggnog/output ############## echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s 2 碳水化合物dbCAN2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## dbCAN2 http://bcb.unl.edu/dbCAN2 ## 创建数据库存放目录并进入 mkdir -p ${db}/dbCAN2 \u0026amp;\u0026amp; cd ${db}/dbCAN2 ## 下载序列和描述 wget -c http://bcb.unl.edu/dbCAN2/download/CAZyDB.07312020.fa wget -c http://bcb.unl.edu/dbCAN2/download/Databases/CAZyDB.07302020.fam-activities.txt ## 备用数据库下载地址并解压 #wget -c http://210.75.224.110/db/dbcan2/CAZyDB.07312020.fa.gz #gunzip CAZyDB.07312020.fa.gz ## diamond建索引，800M，1m diamond --version ## 0.8.22/2.0.5 time diamond makedb \\ --in CAZyDB.07312020.fa \\ --db CAZyDB.07312020 1 2 3 4 5 6 7 8 ## 比对CAZy数据库, 用时2~18m; 加--sensitive更全但慢至1h mkdir -p temp/dbcan2 diamond blastp \\ --db ~/db/dbcan2/CAZyDB.07312020 \\ --query NR/protein.fa \\ --threads 64 -e 1e-5 --outfmt 6 \\ --max-target-seqs 1 --quiet \\ --out temp/dbcan2/gene_diamond.f6 3 ARGs（CARD） ## Github: https://github.com/arpcard/rgi\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #!/bin/bash #SBATCH --job-name=rgi #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/%x_%a.err #SBATCH --cpus-per-task=32 #SBATCH --partition=cpu echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` ############## mkdir -p card rgi main --input_sequence ~/work/asthma/temp/protein.fa \\ --output_file card/protein \\ --input_type protein --num_threads 32 \\ --clean --alignment_tool DIAMOND ############## echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s 4 毒力因子VFDB 官网地址：http://www.mgc.ac.cn/VFs/ 在官网下载数据库时，带有setA 的库为VFDB数据库核心库(set A)，而setB为全库(setB), 其中setA仅包含经实验验证过的毒力基因，而setB则在setA的基础上增加了预测的毒力基因，选择好数据库后，直接用blast/diamond即可完成注释。\n1 2 3 4 5 6 7 mkdir -p temp/vfdb diamond blastp \\ --db ~/db/VFDB/VFDB_setB_pro \\ --query NR/protein.fa \\ --threads 32 -e 1e-5 --outfmt 6 \\ --max-target-seqs 1 --quiet \\ --out temp/vfdb/gene_diamond.f6 5 其他各种数据库 功能注释合并 写一个python脚本，将表1（基因-功能的对应表）与表2（基因丰度表）合并，即不同基因可能注释到相同功能，把它们的丰度加在一起得到新表3（功能丰度表）\nbinning 宏基因组binning是指将不同的序列集合（如metagenome序列集合）根据它们的物种归类到不同的bins中，以便进一步研究它们的组成和功能。这个过程可以将类似的序列组合在一起，形成代表不同物种或基因组的bins，以便进行后续分析，如物种注释、基因组组装等。\n以下是常用的宏基因组binning方法：\n基于聚类的方法：该方法使用序列聚类将相似序列分到同一个bin中。一般来说，聚类算法可分为两类：无监督聚类（如k-means、DBSCAN等）和有监督聚类（如CAMI、MyCC等）。\n基于组装的方法：该方法使用de novo组装来将相似序列组装成连续的序列，再根据这些序列的基因组信息来将其分类到不同的bins中。这种方法的优点是可以更好地处理重复序列，缺点是需要大量的计算资源和时间。\n基于分类器的方法：该方法使用机器学习分类器来将序列分配到不同的bins中。这种方法的优点是可以自动学习特征并在处理大规模数据时效率高，缺点是需要先建立一个分类器并进行训练。\n在进行宏基因组binning时，通常需要使用多个方法进行比较，以选择最适合数据集的方法。可以使用一些流行的工具来进行binning，如MetaBAT、MaxBin、CONCOCT和MEGAN等。这些工具通常包含各种binning方法，可以根据数据集和分析目的选择适合的方法。\n篇幅限制，具体的方法放在另一篇里面讲解吧。\nReference 1. Y.-X. Liu, Y. Qin, T. Chen, M. Lu, X. Qian, X. Guo, Y. Bai, A practical guide to amplicon and metagenomic analysis of microbiome data. Protein \u0026amp; Cell. 12, 315–330 (2021).\n","date":"2023-03-26T00:00:00Z","permalink":"/p/metagenomic-workflow/","title":"Metagenomic workflow"},{"content":"春分刚过，许多花儿都争相绽放。天气还是有点冷，但还是想跟群青同学一起逛逛紫金港，赏赏花，观观鸟。\n这个时候校园里最多的便是蔷薇科的花了，虽然我向来是”桃李杏梅樱，傻傻分不清”的😂，这次在群青同学的帮助下收集到了九宫格：菊花桃，碧桃，垂丝海棠，西府海棠，湖北海棠，大岛樱，日本晚樱，日本樱花，蓬蘽\n湖心岛出发，二月兰，刚冒芽的水杉，偷偷绽放的水仙和蝴蝶花。\n鸟儿大多还是老朋友，一只歌声婉转的乌鸫，快步走的白鹡鸰，悠闲的黑水鸡和斑嘴鸭，大喜鹊和红嘴蓝鹊。本来这次最想看到的就是罕见旅客黑翅长脚鹬的，但是这儿果然只是它们歇歇脚的地方，没能见到😭，偷浙大官微的美图，哈哈。南花园还是夜鹭与白鹭们的领地。\n生物实验中心之前每年这个时候都是大片油菜花，打卡圣地，但今年的试验田搬到了银泉，这里居然种满了麦子。绿油油的麦田非常治愈。这个我叫做单双子叶的碰撞。惊喜出现在麦田的拐角处，几只可爱的啾啾正在偷麦子吃，原来是白腰文鸟，上次见它也是在偷吃生科院的实验水稻。站在紫荆丛中的文鸟超有气质。\n其他一些，医学院的一排樱花，LY同学最赞的一株樱花，已经吃了四年还要再吃四年的麦香…\n感谢群青同学的摄影，越来越强了\n","date":"2023-03-25T00:00:00Z","permalink":"/p/%E7%B4%AB%E9%87%91%E6%98%A5%E6%97%A5/","title":"紫金春日"},{"content":"Introduction 统计分析在生物信息学中具有非常重要的意义，因为生物信息学研究的数据量庞大、复杂性高，而统计分析可以帮助我们更好地理解和解释这些数据。下面是统计分析对生物信息学的几个重要意义：\n数据清洗和预处理：生物信息学研究中经常需要处理大规模的数据，而这些数据可能存在噪声、错误和缺失值等问题。统计分析可以帮助我们对数据进行清洗和预处理，以确保数据的质量和可靠性。 数据可视化：统计分析可以帮助我们将复杂的数据转化为可视化图形，从而更好地理解数据的分布、关系和趋势。这些图形可以帮助我们发现隐藏在数据中的模式和规律。 数据分析：生物信息学研究中需要对大量的数据进行分析，例如比较基因组学、转录组学、蛋白质组学等。统计分析可以帮助我们对数据进行建模和预测，从而深入探究生物学的复杂现象和机制。 数据挖掘：生物信息学研究中需要挖掘大量的数据来发现新的生物学现象和机制。统计分析可以帮助我们从数据中提取出有用的信息和知识，进而推动生物学的研究和发展。 R语言是一个专门用于数据分析和统计建模的编程语言，它有以下几个优点，使其成为做统计分析的理想选择：\n免费和开源：R语言是一个免费和开源的软件，可以在不付出额外成本的情况下使用和定制。这使得许多学生、学者和数据分析师选择R语言作为他们的首选统计分析工具。\n强大的数据处理能力：R语言具有强大的数据处理能力，支持多种数据结构和数据类型，可以轻松地进行数据清洗、整合、变换和分析。\n丰富的统计分析函数库：R语言具有丰富的统计分析函数库，包括线性回归、逻辑回归、聚类分析、主成分分析、时间序列分析等等。这些函数库提供了许多常用的统计分析方法，可以满足不同数据分析需求。\n图形可视化功能：R语言具有强大的图形可视化功能，可以轻松地创建各种类型的图表，包括散点图、条形图、折线图、热图等。这些图表可以帮助数据分析师更好地理解数据、发现规律和提取信息。\n社区支持和生态系统：R语言拥有庞大的用户社区和生态系统，用户可以轻松地找到并使用数千种可用的统计分析工具和R包，这些工具和R包可以帮助用户更加高效地完成统计分析任务。\n我想在这里稍微记录一下我使用R常用的一些初等统计分析方法，如回归，方差分析，广义线性模型等，主要参考资料是《R语言教程》 VII部分统计分析的内容。\nStatistics 基础用法 单样本均值检验 1 2 3 4 5 6 7 ?t.test() #install.packages(\u0026#34;ggstatsplot\u0026#34;,dependencies = T) library(ggstatsplot) #这个包会在画图的过程中计算很多统计量，帮我们更好地把握数据的性质 #比如我们想看tooth length均值是否和25有显著差异 t.test(ToothGrowth$len,mu = 25,alternative = \u0026#34;two.sided\u0026#34;) ## ## One Sample t-test ## ## data: ToothGrowth$len ## t = -6.2648, df = 59, p-value = 0.00000004681 ## alternative hypothesis: true mean is not equal to 25 ## 95 percent confidence interval: ## 16.83731 20.78936 ## sample estimates: ## mean of x ## 18.81333 1 2 3 4 5 6 gghistostats( data = ToothGrowth, x = len, xlab = \u0026#34;Tooth length\u0026#34;, test.value = 25 ) 检验的功效， 是指对立假设成立时检验拒绝$H_0$的概率$1-\\beta$， 其中$\\beta$是第二类错误， 即当对立假设成立时错误地接受$H_0$的概率。 需要足够大的样本量才能使得检验能够发现实际存在的显著差异。\n1 2 3 4 5 6 7 pwr::pwr.t.test( type = \u0026#34;one.sample\u0026#34;, alternative=\u0026#34;greater\u0026#34;, d = (7.25 - 7.0)/1.052, sig.level = 0.05, power = 0.80 ) |\u0026gt; plot() 均值比较 独立两样本t检验\n1 t.test(mpg~am,data = mtcars) ## ## Welch Two Sample t-test ## ## data: mpg by am ## t = -3.7671, df = 18.332, p-value = 0.001374 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -11.280194 -3.209684 ## sample estimates: ## mean in group 0 mean in group 1 ## 17.14737 24.39231 1 ggbetweenstats(mtcars, am, mpg) 比例检验 1 2 #抽查400个样本100个异常，异常比例是否显著大于0.2 prop.test(100, 400, p=0.20, alternative = \u0026#34;greater\u0026#34;) ## ## 1-sample proportions test with continuity correction ## ## data: 100 out of 400, null probability 0.2 ## X-squared = 5.9414, df = 1, p-value = 0.007395 ## alternative hypothesis: true p is greater than 0.2 ## 95 percent confidence interval: ## 0.2149649 1.0000000 ## sample estimates: ## p ## 0.25 1 2 #两次抽查的比例是否一致 prop.test(c(35,27), c(250,300), alternative = \u0026#34;two.sided\u0026#34;) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: c(35, 27) out of c(250, 300) ## X-squared = 2.9268, df = 1, p-value = 0.08712 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.007506845 0.107506845 ## sample estimates: ## prop 1 prop 2 ## 0.14 0.09 方差的假设检验 检查两组数据的方差有误显著差异？\n1 2 3 4 var.test(c( 20.5, 18.8, 19.8, 20.9, 21.5, 19.5, 21.0, 21.2), c( 17.7, 20.3, 20.0, 18.8, 19.0, 20.1, 20.2, 19.1), alternative = \u0026#34;two.sided\u0026#34;) ## ## F test to compare two variances ## ## data: c(20.5, 18.8, 19.8, 20.9, 21.5, 19.5, 21, 21.2) and c(17.7, 20.3, 20, 18.8, 19, 20.1, 20.2, 19.1) ## F = 1.069, num df = 7, denom df = 7, p-value = 0.9322 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.214011 5.339386 ## sample estimates: ## ratio of variances ## 1.068966 拟合优度检验 1 2 #6类face的count比例是否相等，卡方检验 chisq.test(c(168, 159, 168, 180, 167, 158)) ## ## Chi-squared test for given probabilities ## ## data: c(168, 159, 168, 180, 167, 158) ## X-squared = 1.892, df = 5, p-value = 0.8639 1 2 3 4 5 6 7 ggpiestats( data = data.frame(face=1:6, counts=c(168, 159, 168, 180, 167, 158)), x = face, counts = counts, title = \u0026#34;Dice equality\u0026#34; ) 1 2 #各类比例是否为指定值 chisq.test(c(48, 98, 54), p=c(0.3, 0.5, 0.2)) ## ## Chi-squared test for given probabilities ## ## data: c(48, 98, 54) ## X-squared = 7.34, df = 2, p-value = 0.02548 检验分布类型 vcd包提供了一个goodfit函数， 可以用来拟合指定的某种理论分布(包括泊松、二项、负二项分布）， 并检验服从该理论分布的零假设。\n1 2 3 set.seed(101) datax \u0026lt;- rpois(100, 2) summary(vcd::goodfit(datax, \u0026#34;poisson\u0026#34;)) ## ## Goodness-of-fit test for poisson distribution ## ## X^2 df P(\u0026gt; X^2) ## Likelihood Ratio 4.289456 5 0.5085374 独立性卡方检验 1 2 3 4 5 6 ctab.beer \u0026lt;- rbind(c( 20, 40, 20), c(30,30,10)) colnames(ctab.beer) \u0026lt;- c(\u0026#34;Light\u0026#34;, \u0026#34;Regular\u0026#34;, \u0026#34;Dark\u0026#34;) rownames(ctab.beer) \u0026lt;- c(\u0026#34;Male\u0026#34;, \u0026#34;Female\u0026#34;) addmargins(ctab.beer) ## Light Regular Dark Sum ## Male 20 40 20 80 ## Female 30 30 10 70 ## Sum 50 70 30 150 1 2 #列联表独立性检验： chisq.test(ctab.beer) ## ## Pearson's Chi-squared test ## ## data: ctab.beer ## X-squared = 6.1224, df = 2, p-value = 0.04683 1 #在0.05水平下认为啤酒类型偏好与性别有关 非参数检验 常用的有独立两样本比较的Wilcoxon秩和检验， 单样本的符号秩检验和符号检验等\n1 2 3 x \u0026lt;- c(0.80, 0.83, 1.89, 1.04, 1.45, 1.38, 1.91, 1.64, 0.73, 1.46) y \u0026lt;- c(1.15, 0.88, 0.90, 0.74, 1.21) wilcox.test(x,mu = 0.7) ## ## Wilcoxon signed rank exact test ## ## data: x ## V = 55, p-value = 0.001953 ## alternative hypothesis: true location is not equal to 0.7 1 wilcox.test(x, y, alternative = \u0026#34;g\u0026#34;) ## ## Wilcoxon rank sum exact test ## ## data: x and y ## W = 35, p-value = 0.1272 ## alternative hypothesis: true location shift is greater than 0 回归分析 相关分析 Pearson相关系数： $$ \\rho(X,Y)=\\frac{E[(X-E(X))(Y-E(Y))]}{\\sqrt{Var(X)Var(Y)}} $$ 相关系数绝对值在0.8以上认为高度相关。 在0.5到0.8之间认为中度相关。 在0.3到0.5之间认为低度相关。 在0.3以下认为不相关或相关性很弱以至于没有实际价值。 当然，在特别重要的问题中， 只要经过检验显著不等于零的相关都认为是有意义的。\n相关系数检验：\n检验统计量: $$ t=\\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}} $$ p值为：$P(|t(n-2)|\u0026gt;|t_0|)$\n1 2 3 4 5 6 7 set.seed(1) x \u0026lt;- runif(30, 0, 10) xx \u0026lt;- seq(0, 10, length.out = 100) y \u0026lt;- 40 - (x-7)^2 + rnorm(30) yy \u0026lt;- 40 - (xx-7)^2 plot(x, y, pch=16) lines(xx, yy) 1 cor(x,y,method = \u0026#34;pearson\u0026#34;) ## [1] 0.8244374 1 cor.test(x,y,method = \u0026#34;pearson\u0026#34;) ## ## Pearson's product-moment correlation ## ## data: x and y ## t = 7.7083, df = 28, p-value = 0.00000002136 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.6602859 0.9134070 ## sample estimates: ## cor ## 0.8244374 1 2 3 4 5 ggstatsplot::ggscatterstats( data = data.frame(x,y), x = x, y = y ) 相关性矩阵，`n*n`或`n*m`的$r$ 和$p-value$矩阵 1 2 3 ggstatsplot::ggcorrmat( data = mtcars ) 1 pcutils::cor_plot(mtcars) 1 corrplot::corrplot(cor(mtcars)) 1 ggcorrplot::ggcorrplot(cor(mtcars),method = \u0026#34;circle\u0026#34;) Spearman秩相关系数 Spearman rho系数， 是两个变量的秩统计量的相关系数\nKendall tau系数 当变量正相关性很强时， 任意两个观测的X值的大小顺序应该与Y值的大小顺序相同； 如果独立， 一对观测的X值比较和Y值比较顺序相同与顺序相反的数目应该基本相同。Kandall tau系数也是取值于区间[-1,1]， 用这样的思想表示两个变量的相关性和正负。\n1 cor.test(x,y,method = \u0026#34;spearman\u0026#34;) ## ## Spearman's rank correlation rho ## ## data: x and y ## S = 922, p-value = 0.000001339 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.7948832 1 cor.test(x,y,method = \u0026#34;kendall\u0026#34;) ## ## Kendall's rank correlation tau ## ## data: x and y ## T = 354, p-value = 0.000000159 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 0.6275862 一元回归 $$ Y=a+bX+\\varepsilon, \\varepsilon \\sim N(0,\\sigma^2) $$\n最小二乘法 $$ \\hat{b}=\\frac{\\sum_i(x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_i{(x-x_i)}^2}=r_{xy}\\frac{S_y}{S_x} $$\n$$ \\hat{a}=\\overline{y}-\\hat{b}\\overline{x} $$ 回归有效性可以用$R^2$和$p-valuie$来度量， $R^2=1-\\frac{SSE}{SST}$\n统计量$F=\\frac{SSR}{SSE/(n-2)}$,$p-value$为$P(F(1,n-2)\u0026gt;c)$,c为F的值。\n1 2 lm1 \u0026lt;- lm(y ~ x) summary(lm1) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.856 -4.549 2.141 6.048 9.664 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 9.9855 2.6930 3.708 0.000914 *** ## x 3.5396 0.4592 7.708 0.0000000214 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 7.302 on 28 degrees of freedom ## Multiple R-squared: 0.6797,\tAdjusted R-squared: 0.6683 ## F-statistic: 59.42 on 1 and 28 DF, p-value: 0.00000002136 1 2 #prediction predict(lm1,newdata =data.frame(x=c(5,10,15))) ## 1 2 3 ## 27.68364 45.38182 63.08000 1 pcutils::my_lm(y,x) 多元回归 1 2 lm2 \u0026lt;- lm(mpg ~ cyl + disp, data=mtcars) summary(lm2) ## ## Call: ## lm(formula = mpg ~ cyl + disp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4213 -2.1722 -0.6362 1.1899 7.0516 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 34.66099 2.54700 13.609 4.02e-14 *** ## cyl -1.58728 0.71184 -2.230 0.0337 * ## disp -0.02058 0.01026 -2.007 0.0542 . ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 3.055 on 29 degrees of freedom ## Multiple R-squared: 0.7596,\tAdjusted R-squared: 0.743 ## F-statistic: 45.81 on 2 and 29 DF, p-value: 0.000000001058 1 ggstatsplot::ggcoefstats(lm2) 1 2 #回归自变量筛选 lm3 \u0026lt;- step(lm(mpg ~ cyl + disp+hp+drat+vs, data=mtcars)) ## Start: AIC=77.08 ## mpg ~ cyl + disp + hp + drat + vs ## ## Df Sum of Sq RSS AIC ## - vs 1 0.3134 244.90 75.124 ## - cyl 1 7.6839 252.27 76.073 ## - drat 1 14.3330 258.92 76.905 ## - disp 1 14.6709 259.26 76.947 ## \u0026lt;none\u0026gt; 244.59 77.083 ## - hp 1 19.8255 264.41 77.577 ## ## Step: AIC=75.12 ## mpg ~ cyl + disp + hp + drat ## ## Df Sum of Sq RSS AIC ## - cyl 1 8.444 253.35 74.209 ## - disp 1 14.765 259.67 74.997 ## \u0026lt;none\u0026gt; 244.90 75.124 ## - drat 1 16.467 261.37 75.206 ## - hp 1 19.613 264.51 75.589 ## ## Step: AIC=74.21 ## mpg ~ disp + hp + drat ## ## Df Sum of Sq RSS AIC ## \u0026lt;none\u0026gt; 253.35 74.209 ## - drat 1 30.148 283.49 75.806 ## - disp 1 38.107 291.45 76.693 ## - hp 1 49.550 302.90 77.925 多重共线性\n狭义的多重共线性（multicollinearity）： 自变量的数据存在线性组合近似地等于零， 使得解线性方程组求解回归系数时结果不稳定， 回归结果很差。\n广义的多重共线性： 自变量之间存在较强的相关性， 这样自变量是联动的， 互相之间有替代作用。 甚至于斜率项的正负号都因为这种替代作用而可能是错误的方向。\n1 2 #car包的vif()函数计算方差膨胀因子 car::vif(lm3) ## disp hp drat ## 4.621988 2.868264 2.166843 非参数回归 所谓参数回归， 是指回归函数有预先确定的公式， 仅需要估计的未知参数； 非参数回归， 就是没有预先确定的公式， 的形式本身也依赖于输入的样本, 。 下面描述的核回归就是这样典型的非参数回归， 样条平滑、样条函数回归一般也看作是非参数回归。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #样条平滑 set.seed(1) nsamp \u0026lt;- 30 x \u0026lt;- runif(nsamp, -10, 10) xx \u0026lt;- seq(-10, 10, length.out=100) x \u0026lt;- sort(x) y \u0026lt;- 10*sin(x/10*pi)^2 + rnorm(nsamp,0,0.3) plot(x, y) curve(10*sin(x/10*pi)^2, -10, 10, add=TRUE, lwd=2) library(splines) res \u0026lt;- smooth.spline(x, y) lines(spline(res$x, res$y), col=\u0026#34;red\u0026#34;) res2 \u0026lt;- loess(y ~ x, degree=2, span=0.3) lines(xx, predict(res2, newdata=data.frame(x=xx)), col=\u0026#34;blue\u0026#34;) legend(\u0026#34;top\u0026#34;, lwd=c(2,1,1), col=c(\u0026#34;black\u0026#34;, \u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;), legend=c(\u0026#34;real data\u0026#34;, \u0026#34;smooth.spline\u0026#34;, \u0026#34;local lm\u0026#34;)) 1 2 3 4 ## 线性可加模型 ## R扩展包mgcv的gam()函数可以执行这样的可加模型的非参数回归拟合。 lm.rock \u0026lt;- lm(log(perm) ~ area + peri + shape, data=rock) summary(lm.rock) ## ## Call: ## lm(formula = log(perm) ~ area + peri + shape, data = rock) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.8092 -0.5413 0.1734 0.6493 1.4788 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 5.33314499 0.54867792 9.720 1.59e-12 *** ## area 0.00048498 0.00008657 5.602 1.29e-06 *** ## peri -0.00152661 0.00017704 -8.623 5.24e-11 *** ## shape 1.75652601 1.75592362 1.000 0.323 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.8521 on 44 degrees of freedom ## Multiple R-squared: 0.7483,\tAdjusted R-squared: 0.7311 ## F-statistic: 43.6 on 3 and 44 DF, p-value: 3.094e-13 1 2 gam.rock1 \u0026lt;- mgcv::gam(log(perm) ~ s(area) + s(peri) + s(shape), data=rock) summary(gam.rock1) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## log(perm) ~ s(area) + s(peri) + s(shape) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 5.1075 0.1222 41.81 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(area) 1.000 1.000 29.13 0.00000307 *** ## s(peri) 1.000 1.000 71.30 \u0026lt; 2e-16 *** ## s(shape) 1.402 1.705 0.58 0.437 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## R-sq.(adj) = 0.735 Deviance explained = 75.4% ## GCV = 0.78865 Scale est. = 0.71631 n = 48 1 plot(gam.rock1) ##方差分析\n单因素方差分析可以看成基础统计中两样本t检验的一个推广， 要比较试验观测值的某个因变量（称为“指标”）按照一个分组变量（称为“因素”）分组后， 各组的因变量均值有无显著差异。\n1 2 3 mtcars$cyl=as.factor(mtcars$cyl) aov.manu \u0026lt;- aov(mpg ~ cyl, data=mtcars) summary(aov.manu) ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## cyl 2 824.8 412.4 39.7 0.00000000498 *** ## Residuals 29 301.3 10.4 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 pcutils::group_box(mtcars[\u0026#34;mpg\u0026#34;],group = \u0026#34;cyl\u0026#34;,metadata = mtcars) 1 2 #非参数形式 kruskal.test(mpg ~ cyl, data=mtcars) ## ## Kruskal-Wallis rank sum test ## ## data: mpg by cyl ## Kruskal-Wallis chi-squared = 25.746, df = 2, p-value = 0.000002566 进行多个假设检验（如均值比较）的操作称为*“多重比较”*（multiple comparison， 或multiple testing）， 多次检验会使得总第一类错误概率增大。\n1 pcutils::multitest(mtcars$mpg,mtcars$cyl) ## ====================================1.ANOVA:==================================== ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## group 2 824.8 412.4 39.7 0.00000000498 *** ## Residuals 29 301.3 10.4 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ================================2.Kruskal.test:================================ ## ## Kruskal-Wallis rank sum test ## ## data: var by group ## Kruskal-Wallis chi-squared = 25.746, df = 2, p-value = 0.000002566 ## ## ==========================3.LSDtest, bonferroni p-adj:========================== ## var groups ## 4 26.66364 a ## 6 19.74286 b ## 8 15.10000 c ## ==================================4.tukeyHSD:================================== ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = var ~ group) ## ## $group ## diff lwr upr p adj ## 6-4 -6.920779 -10.769350 -3.0722086 0.0003424 ## 8-4 -11.563636 -14.770779 -8.3564942 0.0000000 ## 8-6 -4.642857 -8.327583 -0.9581313 0.0112287 ## ## =================================5.Wilcox-test:================================= ## 4 6 8 ## 4 1.00000000000 0.0006658148 0.00002774715 ## 6 0.00066581478 1.0000000000 0.00101304469 ## 8 0.00002774715 0.0010130447 1.00000000000 ##广义线性模型\n泊松回归 1 2 3 4 5 6 counts \u0026lt;- c(18,17,15,20,10,20,25,13,12) outcome \u0026lt;- gl(3,1,9) treatment \u0026lt;- gl(3,3) D93=data.frame(treatment, outcome, counts) ## showing data ggplot(data = D93, mapping = aes(x = counts)) +geom_bar() 1 2 glm.D93 \u0026lt;- glm(counts ~ outcome + treatment,data = D93, family = poisson()) summary(glm.D93) ## ## Call: ## glm(formula = counts ~ outcome + treatment, family = poisson(), ## data = D93) ## ## Deviance Residuals: ## 1 2 3 4 5 6 7 8 ## -0.67125 0.96272 -0.16965 -0.21999 -0.95552 1.04939 0.84715 -0.09167 ## 9 ## -0.96656 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 3.045e+00 1.709e-01 17.815 \u0026lt;2e-16 *** ## outcome2 -4.543e-01 2.022e-01 -2.247 0.0246 * ## outcome3 -2.930e-01 1.927e-01 -1.520 0.1285 ## treatment2 -3.242e-16 2.000e-01 0.000 1.0000 ## treatment3 -2.148e-16 2.000e-01 0.000 1.0000 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 10.5814 on 8 degrees of freedom ## Residual deviance: 5.1291 on 4 degrees of freedom ## AIC: 56.761 ## ## Number of Fisher Scoring iterations: 4 逻辑斯谛回归 ….\n","date":"2023-03-24T00:00:00Z","permalink":"/p/r-statistics/","title":"R-statistics"},{"content":"在Rblogdown使用stack-theme遇到了几个问题，应该是Rblogdown与原生hugo间的不太兼容。\n我很快发现了问题是Rmd产生的md文件可能跟stack主题不太匹配，一开始想看看yaml文件能不能修改解决，但找了不少地方都没找到😭。\n因为问题还比较确定，可以试试自己写脚本转换Rmd产生的md文件。\n公式问题 rblogdown 渲染Rmd成md会把我写的公式语法改变：\n例如： $a=sum_i^2$变成了\\(a=sum_i^2\\)\n但是stack主题识别不了后面那种语法，这个比较简单：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import re # 定义一个正则表达式，匹配LaTeX数学公式中的起始和结束行内数学模式符号 pattern = r\u0026#39;\\\\\\((.*?)\\\\\\)\u0026#39; # 定义一个字符串，包含要处理的LaTeX字符串 latex_string = r\u0026#39;For the equation `\\(\\sum_{i=1}^n i^2\\)`\u0026#39; # 使用sub()函数替换匹配到的文本 processed_string = re.sub(pattern, r\u0026#39;$\\1$\u0026#39;, latex_string) # 输出处理后的字符串 print(processed_string) a=re.sub(r\u0026#39;`\\\\\\((.*?)\\\\\\)`\u0026#39;,r\u0026#39;$\\1$\u0026#39;,\u0026#34;`\\(a=sum_i^2\\)`jjjhg`\\(a=sum_i^2\\)`\u0026#34;) 图片大小调整 因为![from data to viz website](images/data2viz.png){width=60%}这种语法是stack主题不支持的， 但是\n1 2 3 \u0026#39;\u0026lt;p style=\u0026#34;text-align: center;\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;images/data2viz.png\u0026#34; width=\u0026#34;60%\u0026#34; title=\u0026#34;from data to viz website\u0026#34;/\u0026gt; \u0026lt;/p\u0026gt;\u0026#39; 这种语法是支持的，所以想写一个小脚本转换一下\n自己不太熟悉正则（上面那个公式转换的也是他帮我写的，哈哈），所以试试问了ChatGPT ：\n可以帮我写一个将插入图片的markdown语法转换为html语法的小脚本吗？ 比如：![A](B.png){width=60%}转换为\u0026lt;img src=\u0026quot;B.png\u0026quot; width=\u0026quot;60%\u0026quot; title=\u0026quot;A\u0026quot;/\u0026gt;\n它立马用python帮我写了：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import re def convert_markdown_to_html(markdown_text): # 将插入图片的Markdown语法转换为HTML语法 pattern = r\u0026#34;!\\[(.*?)\\]\\((.*?)\\){(.*?)}\u0026#34; matches = re.findall(pattern, markdown_text) for match in matches: title = match[0] src = match[1] attributes = match[2].split(\u0026#34;,\u0026#34;) attr_dict = dict([tuple(attribute.split(\u0026#34;=\u0026#34;)) for attribute in attributes]) html_tag = f\u0026#39;\u0026lt;img src=\u0026#34;{src}\u0026#34; title=\u0026#34;{title}\u0026#34;\u0026#39; for key, value in attr_dict.items(): html_tag += f\u0026#39; {key}=\u0026#34;{value}\u0026#34;\u0026#39; html_tag += \u0026#34;/\u0026gt;\u0026#34; markdown_text = markdown_text.replace(f\u0026#34;![{title}]({src}){{{match[2]}}}\u0026#34;, html_tag) return markdown_text def convert_latex_to_md(markdown_text): html_text=re.sub(r\u0026#39;`\\\\\\((.*?)\\\\\\)`\u0026#39;,r\u0026#39;$\\1$\u0026#39;,markdown_text) return html_text def convert_markdown_file_to_html(file_path): # 读取Markdown文件 with open(file_path, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: markdown_text = f.read() # 将Markdown语法转换为HTML语法 html_text = convert_markdown_to_html(markdown_text) html_text = convert_latex_to_md(html_text) # 将HTML文本写入文件 with open(file_path, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: f.write(html_text) if __name__ == \u0026#34;__main__\u0026#34;: file_path = \u0026#34;index.md\u0026#34; convert_markdown_file_to_html(file_path) 牛牛牛！ 再问一下，可以用R语言写一下吗？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 convert_markdown_to_html \u0026lt;- function(markdown_text) { # 通过正则表达式匹配图片语法 matches \u0026lt;- gregexpr(\u0026#34;!\\\\[(.*?)\\\\]\\\\((.*?)\\\\){(.*?)}\u0026#34;, markdown_text, perl = TRUE) # 遍历匹配结果，将Markdown语法替换为HTML语法 for (i in seq_along(matches[[1]])) { # 判断匹配结果是否为空列表 if (length(matches[[1]][i]) == 0) { next } match \u0026lt;- regmatches(markdown_text, matches[[1]][i]) title \u0026lt;- match[[2]] src \u0026lt;- match[[3]] attributes \u0026lt;- strsplit(match[[4]], \u0026#34;,\u0026#34;)[[1]] attr_dict \u0026lt;- setNames(sapply(strsplit(attributes, \u0026#34;=\u0026#34;), \u0026#34;[\u0026#34;, 2), sapply(strsplit(attributes, \u0026#34;=\u0026#34;), \u0026#34;[\u0026#34;, 1)) # 构造HTML标签 html_tag \u0026lt;- sprintf(\u0026#39;\u0026lt;img src=\u0026#34;%s\u0026#34; title=\u0026#34;%s\u0026#34;\u0026#39;, src, title) for (key in names(attr_dict)) { value \u0026lt;- attr_dict[[key]] html_tag \u0026lt;- paste(html_tag, sprintf(\u0026#39;%s=\u0026#34;%s\u0026#34;\u0026#39;, key, value), sep = \u0026#34; \u0026#34;) } html_tag \u0026lt;- paste(html_tag, \u0026#34;/\u0026gt;\u0026#34;, sep = \u0026#34;\u0026#34;) # 将Markdown语法替换为HTML语法 markdown_text \u0026lt;- sub(\u0026#34;!\\\\[(.*?)\\\\]\\\\((.*?)\\\\){(.*?)}\u0026#34;, html_tag, markdown_text, fixed = TRUE) } return(markdown_text) } markdown_text \u0026lt;- \u0026#34;![A](B.png){width=60%}\u0026#34; html_text \u0026lt;- convert_markdown_to_html(markdown_text) print(html_text) 唉，实际上这个代码没有用，我也不知道bug怎么搞，R的正则表达式一直感觉没有python好用。\n还是用写好的python脚本吧。\n修改新的md文件 我们还需要找到最新render出来的md文件按上述规则进行修改（否则一口气改全部的话，害怕会越来越慢，甚至损坏以前的md文件）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import os import datetime # 定义要查找的目录 directory = \u0026#39;./content/post/\u0026#39; # 存储所有的md文件 md_files = [] # 递归查找目录下的所有文件 for root, dirs, files in os.walk(directory): for file in files: if file.endswith(\u0026#39;.md\u0026#39;): file_path = os.path.join(root, file) md_files.append((file_path, os.path.getmtime(file_path))) # 找到最新修改的文件 latest_file = max(md_files, key=lambda x: x[1])[0] print(\u0026#34;最新保存的.md文件是：\u0026#34;, latest_file) 再问问，可以找到目录下所有新保存的.md文件吗，就是修改时间与上次不一致的？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import os import datetime # 定义要查找的目录 directory = \u0026#39;./content/\u0026#39; # 存储所有新保存的md文件 new_md_files = [] # 读取上一次检查的时间戳 if os.path.isfile(\u0026#39;last_check_time.txt\u0026#39;): with open(\u0026#39;last_check_time.txt\u0026#39;, \u0026#39;r\u0026#39;) as f: last_check_time = datetime.datetime.strptime(f.read(), \u0026#39;%Y-%m-%d %H:%M:%S.%f\u0026#39;) else: last_check_time = datetime.datetime.now() # 递归查找目录下的所有文件 for root, dirs, files in os.walk(directory): for file in files: if file.endswith(\u0026#39;.md\u0026#39;): file_path = os.path.join(root, file) mod_time = datetime.datetime.fromtimestamp(os.path.getmtime(file_path)) if mod_time \u0026gt; last_check_time: new_md_files.append(file_path) # 更新上一次检查的时间戳 last_check_time = datetime.datetime.now() # 将上一次检查的时间戳保存到文件中 with open(\u0026#39;last_check_time.txt\u0026#39;, \u0026#39;w\u0026#39;) as f: f.write(last_check_time.strftime(\u0026#39;%Y-%m-%d %H:%M:%S.%f\u0026#39;)) # 打印所有新保存的md文件 for file_path in new_md_files: print(\u0026#34;新保存的.md文件：\u0026#34;, file_path) #更新所有新保存的md文件 for file_path in new_md_files: convert_markdown_file_to_html(latest_file) 这个思路真不错，哈哈\n解决方法 调教了一下ChatGPT后，就得到了整个更新问题的方法了： 首先整理一个refresh_md_to_fit_stack.py放在网站根目录下， 然后在R文件夹的（如果你是用Rblogdown生成的网站，那就会有这个文件夹）的build2.R中加入一行：\nsystem(\u0026quot;python refresh_md_to_fit_stack.py\u0026quot;)\n即可。 因为Rblogdown会在每次实时渲染我们的网站后运行build2.R文件，相当于运行了refresh_md_to_fit_stack.py文件，而refresh_md_to_fit_stack.py文件将搜索所有更新的md文件并修改以上两个问题，这样hugo建立的网站就能够契合stack-theme。\n看看结果： 如果调试成功的话，下面显示的即是正常的latex公式:\n$y=\\frac{1}{2} $\n如果不是使用的Rblogdown生成的网站，这种思路的方法也可以帮助改变stack-theme。\n此外，如果是Rblogdown生成的其他主题的网站有类似问题，也可以这样修改。\n最后，ChatGPT真不错！再来一个攻略\n","date":"2023-03-23T00:00:00Z","image":"/p/chatgpt-stack/ChatGPT_huce8d906dd473ab038e4f784f810328ac_7886_120x120_fill_q75_box_smart1.jpg","permalink":"/p/chatgpt-stack/","title":"ChatGPT帮我调整stack主题"},{"content":"Introduction Zhou, J. \u0026amp; Ning, D. Stochastic Community Assembly: Does It Matter in Microbial Ecology? Microbiol Mol Biol Rev 81, e00002-17 (2017). This review is very comprehensive (1)！\n周集中老师实验室的长期研究兴趣集中在从基因组到生态系统的不同组织层面的环境微生物学，其中一项就是宏基因组学和微生物生态学：利用高通量基因组测序和相关基因组学技术检查不同栖息地的微生物群落多样性、微生物生物地理学和形成微生物多样性模式、分布和动态的机制；\nUnderstanding the mechanisms controlling community diversity, functions, succession, and biogeography is a central, but poorly understood, topic in ecology, particularly in microbial ecology. Traditional niche-based theory hypothesizes that deterministic factors such as species traits, inter-species interactions (e.g., competition, predation, mutualisms, and trade-offs), and environmental conditions (e.g., pH, temperature, salt, and moisture) govern community structure, which are often referred to as deterministic processes.\nIn contrast, neutral theory assumes that community structures are independent of species traits and governed by stochastic processes of birth, death, colonization, extinction, and speciation.\nAlthough, recently, it has been generally accepted that both deterministic and stochastic processes occur simultaneously in the assembly of local communities, a central debate is on their relative importance in controlling community structure, succession, and biogeography.\nMethods 目前在文章中普遍出现的计算群落构建的方法主要有三套：\n1.Stegen （βNTI \u0026amp; RCbray-based） 原始的文章 (2), (3)\n在确定性与随机性二分法的背景下塑造微生物群落多样性的生态过程。该方案显示了在文中讨论的假设下，基于系统发育和分类多样性划分各种生态过程的不同步骤。 NTI（最近分类单元指数）基于系统发育多样性指数 MNTD（平均最近分类单元距离）的空模型检验，RCBray（修正的 Raup-Crick 指数）基于 Bray-Curtis 的空模型检验分类多样性指数。这两个框分别表示确定性选择和未支配部分的主要组成部分。除了影响较小的选择外，未支配部分的弱选择也可能是由于抵消了有影响的选择因素和/或不同分类群的对比选择。该图主要是根据 Stegen 等人先前报告的数据制作的。\nbeta-NTI（nearest taxon index）： 使用 NTI/NRI 指示单个群落内共存的分类单元相比偶然预期的关系更为紧密还是分散，使用 βNTI/βNRI 指示两两群落间的变化受确定性或随机性因素影响的大小。 MNTD 即mean-nearest-taxon-distance (最近种间平均进化距离) ， 关于 NTI/NRI、βNTI/βNRI 的结果解释时其显著性的判断依据是其临界值是否大于|2|，实际上这是依据标准正态分布的 95%置信区间得到的，一般认为 NRI 或者 NTI 大于 1.96 或者小于-1.96 的结果，在 95%的水平是显著的。\nRCbray: 相异指数的一种。\ncode https://blog.csdn.net/weixin_43367441/article/details/118515090\n非常重要的一点：这类方法是基于系统发育树推断的，所以一定要有跟tax表对应的系统发育树。 但事实上，除非用的是16S/ITS测序，否则我们很难给自己的数据建一个树。 所以，替代方法是利用阶层分类关系（界门纲目科属种）来做一个拟系统发育树（枝长都为1） 参见自己写的df2tree函数\n1 2 3 4 5 data(otutab) df2tree(taxonomy)-\u0026gt;phylo #计算 nti_rc(otutab,phylo,metadata[,\u0026#34;Group\u0026#34;,drop=F])-\u0026gt;nti_res ## [1] \u0026quot;Result saved as nti_rc_res.rda\u0026quot; 1 2 3 4 5 #可视化 nti_res$type=factor(nti_res$type,levels = c(\u0026#34;Homo_S\u0026#34;,\u0026#34;Heter_S\u0026#34;,\u0026#34;Homo_D\u0026#34;,\u0026#34;D_limit\u0026#34;,\u0026#34;Undominated\u0026#34;)) table(nti_res$type,nti_res$variable)%\u0026gt;%reshape2::melt()-\u0026gt;com_p colnames(com_p)=c(\u0026#34;type\u0026#34;,\u0026#34;variable\u0026#34;,\u0026#34;n\u0026#34;) ggplot(com_p,aes(x=variable,y=n))+geom_bar(stat = \u0026#34;identity\u0026#34;,aes(fill=type),position = \u0026#34;fill\u0026#34;) iCAMP 现在大家经常用的iCAMP包就是基于这个理论框架进行分析的 (4)。\n2.ST，NST，MST （stochasticity ratio） PNAS：NST方法定量生态过程中的随机性\n周老师的文章，开发出新的方法对生态过程中的随机性进行了定量。提出了一个新的指数，normalized stochasticity ratio (NST)，作为确定性主导（deterministic, \u0026lt;50%）和随机性主导（stochastic, \u0026gt;50%）的边界点 (5)。\ncode 已经有相应的包了,NST\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #install.packages(\u0026#34;NST\u0026#34;) library(NST) data(tda) comm=tda$comm bray=beta.g(comm,dist.method=\u0026#34;bray\u0026#34;) bray.3col=dist.3col(bray) group=tda$group tnst=tNST(comm=comm, group=group, rand=20, output.rand=TRUE, nworker=1) #检验各组ST、NST的分布情况及各组ST、NST差异的显著性。 nst.bt=nst.boot(nst.result=tnst, group=NULL, rand=99, trace=TRUE, two.tail=FALSE, out.detail=FALSE, between.group=FALSE, nworker=1) #ST和NST组间进行Permutational multivariate ANOVA nst.pova=nst.panova(nst.result=tnst, rand=99) #可视化 pcutils::group_box(tnst$index.pair.grp,col = 8,group = tnst$index.pair.grp$group) example 地下水微生物群落演替过程中估计的 NST 动态变化对乳化植物油注入的响应。 NST 是基于 (A) Jaccard 和 (B) Ru ziˇ cka 指标使用空模型算法 PF 计算的。在零模型 PF 中，类群发生的概率与观察到的发生频率成正比，并且每个样本中的类群丰富度是固定的（19）。当使用基于丰度的指标 Ru ziˇ cka 时，每个样本中的空分类群丰度被计算为观察到的个体数量的随机抽取，其概率与样本中空分类群的区域相对丰度成比例（26）。 W8 是植物油对其没有影响或影响最小的对照井。\n3.Solan NCM 只有当物种死亡或离开这个系统时，群落结构才会发生改变。此时，离开个体的生态位就会空余出来，其它个体会通过来自群落外的迁移或群落内部的繁殖来填补空出的生态位。因此可以把群落的动态描述为死亡——繁殖/扩散——死亡这样的循环 (6)。\n计算公式：\n-$Pr(\\frac{N_i+1}{N_i})=(\\frac{N_T-N_i}{N_T})[mp_i+(1+\\alpha_i)(1-m)(\\frac{N_i}{N_T-1})$ -$Pr(\\frac{N_i}{N_i})=\\frac{N_i}{N_T}[mp_i+{(1+\\alpha_i)}(1-m)(\\frac{N_i-1}{N_T-1})]+(\\frac{N_T-N_i}{N_T})[m(1-p_i)+{red}{(1-\\alpha_i)}(1-m)(\\frac{N_T-N_i-1}{N_T-1})]$ -$Pr(\\frac{N_i-1}{N_i})=\\frac{N_i}{N_T}[m(1-p_i)+(1-\\alpha_i)(1-m)(\\frac{N_T-N_i}{N_T-1})]$\n物种 i 占居的频率（occurrence frequency: row sums of binary OTU table/number of sites）为其概率密度函数的积分。 此时该分布是一个beta分布，我们就可以在R语言中利用beta分布对其进行[拟合]，获得参数m的评估值。\ncode 1 2 3 4 library(devtools) install_github(\u0026#34;Russel88/MicEco\u0026#34;) library(MicEco) neutral.fit(t(otutab))#使用的是最大似然估计拟合模型，R2计算方法也不同 另一个代码来自https://mp.weixin.qq.com/s/opFXl-TvkJfmPcWKFwhCFA，比较经典，用的是非线性模型:\nN描述了宏群落规模（metacommunity size），在本文中为每个样本中所有OTU的总丰度。 m量化了群落层面的迁移率（migration rate），该值对于每个群落成员都是统一的（与物种无关），m值越小说明整个群落中物种扩散越受限制，反之m值越高则表明物种受到扩散限制越低。 Nm是元群落规模（N）与迁移率（m）的乘积 (Nm = N*m)，量化了对群落之间扩散的估计，决定了发生频率和区域相对丰度之间的相关性。\n自己的绘图代码：\n1 2 3 data(otutab) ncm(otutab)-\u0026gt;ncm_res plot(ncm_res) R2代表了中性群落模型的整体拟合优度，R2越高表明越接近中性模型，即群落的构建受随机性过程的影响越大，受确定性过程的影响越小。\n值得注意，R方不是某个数值的平方，可以是负值。因为拟合程度没有下限，可以无限差，R方的范围是(−∞,1]。R方的取值，有以下的可能性：\n等于1。理想状况，该模型对所有的真值预测准确，没有偏差。泼个冷水，如果某篇文章里出现了R方=1，要么是问题过于简单没有研究价值，要么是模型过于复杂，对数据进行了过度拟合。\n小于1大于0。这是常见状况，表明该模型的拟合水平比均值模型好。\n等于0。该模型的拟合水平接近于均值模型。该模型没有价值。\n小于0。该模型的拟合水平不如均值模型。同样，该模型没有价值。\nexample **随机过程，例如散布、出生、死亡、灭绝和移民，在呼气细菌和真菌群落的组装中发挥作用。**真菌微生物群的 SNM 拟合性能（图 2）优于细菌（细菌 R2 = 0.353，真菌 R2 = 0.683）。这种差异表明随机过程对于 EBC 真菌群落的组装可能相对更重要，而对 EBC 细菌群落的组装则不太重要。这种现象可能与细菌和真菌之间的大小差异有关，较小的细菌受扩散限制的影响较小，而受确定性过程的影响更大 (7)。\nStochastic processes played a role in assembling expiratory bacterial (A) and fungal (B) communities based on Sloan neutral model fitting. The black solid line represents the best fit, and the dotted lines represent the 95% CI (confidence interval) around the model fit. The blue dots refer to taxa that occur more frequently than predicted, and the red dots refer to taxa that occur less frequently than predicted. The green dots refer to taxa that occur in a manner consistent with predicted values.\n原核生物细胞直径大概在0.5 ~ 2.0 μm； 微真核生物个体大小大概在1 ~ 200 mm； 之前的研究已经报道了较小的生物不太可能受到扩散限制的影响，因为它们与较大的生物相比具有更高的扩散能力。因此假设在细菌群落中生态位过程的相对影响可能比在微真核生物群落中更强。微真核生物个体大，可能会影响其扩散过程。\nOthers 参考自历神公众号文章：https://mp.weixin.qq.com/s/nwNuPlY7x6VScJA44c0MjQ\nCompetitive lottery 基于竞争彩票模型(competitive lottery model)的群落构建 假设一个两步模型:在第一步中，样本的总丰度(100%)根据某个未知的过程在组之间分配。 然后在第二步中，分配给每个小组的丰度会根据一个竞争彩票模式在小组成员之间进行分配。 竞争彩票模型的概念说明:\n在第一阶段，每个样本的总丰度(100%)在一组预定义的组之间进行分割。\n在第二阶段，每一组的丰度分配根据竞争彩票模型在子组之间进行分配，其中一个子组获得了大部分的丰度。\nDNCI 在PER-SIMPER方法的基础上，提出了一个新的度量指标: dispersal–niche continuum index (DNCI)，该指数可估计是扩散过程还是生态位过程主导群落的构建，并便于不同数据集之间的比较。\nPER-SIMPER利用物种在不同站点之间的矩阵，在排列过程中生成三种不同的零模型： 通过约束行(生态位构建)、约束列(扩散构建)或两者都约束。 PER-SIMPER利用SIMPER方法对原始群落矩阵组成相似度模式进行建模，并与三个零模型比较。 PER-SIMPER通过识别哪个零模型与经验分析最匹配来进行定性评估。然而，大多数群落都是由生态位和扩散过程共同构成的，这限制了构建机制定性方法的敏感性。此外简单决策过程的定性特性使不同群落之间的构建过程难以进行精确比较。\nPER-SIMPER分析返回三个E-metric分布,这与SIMPER经验值与三个PER-SIMPER零模型之间的偏差相关。本文提出的新的DNCI是由这些计算的E值推导而来的。 定量结果是基于标准效应量En(即来自”生态位”模型的E-metric分布)减去标准效应量Ed (即来自”扩散”模型的E-metric分布)。 DNCI提供了一种方法来量化和比较跨数据集构建过程的强度。DNCI值正或负表明生态位或分散过程分别是群组构建的主要过程。指数的绝对值越高，代表占主导地位的构建过程的潜力越大。 $DNCI=SES_d-SES_n=\\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{E_{d(i)}-\\overline {E_{dn}}}{\\sigma E_{dn}}\\right)-\\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{E_{n(i)}-\\overline {E_{dn}}}{\\sigma E_{dn}}\\right)$\n如果DNCI与0差异不显著，则可以认为扩散过程和生态位过程对群落构建的影响是相等的。 当DNCI显著低于0时，扩散过程是群落构建的主导驱动因素; 如果DNCI显著高于0，生态位过程是群落构建的主要决定因素。 注意，表明扩散过程优势的负DNCI值并不能提供实际扩散速率的信息。\nReferences 1. J. Zhou, D. Ning, Stochastic Community Assembly: Does It Matter in Microbial Ecology? Microbiology and Molecular Biology Reviews. 81, e00002–17 (2017).\n2. J. C. Stegen, X. Lin, J. K. Fredrickson, X. Chen, D. W. Kennedy, C. J. Murray, M. L. Rockhold, A. Konopka, Quantifying community assembly processes and identifying features that impose them. The ISME Journal. 7, 2069–2079 (2013).\n3. J. C. Stegen, X. Lin, A. E. Konopka, J. K. Fredrickson, Stochastic and deterministic assembly processes in subsurface microbial communities. The ISME Journal. 6, 1653–1664 (2012).\n4. D. Ning, M. Yuan, L. Wu, Y. Zhang, X. Guo, X. Zhou, Y. Yang, A. P. Arkin, M. K. Firestone, J. Zhou, A quantitative framework reveals ecological drivers of grassland microbial community assembly in response to warming. Nature Communications. 11, 4717 (2020).\n5. D. Ning, Y. Deng, J. M. Tiedje, J. Zhou, A general framework for quantitatively assessing ecological stochasticity. Proceedings of the National Academy of Sciences. 116, 16892–16898 (2019).\n6. W. T. Sloan, M. Lunn, S. Woodcock, I. M. Head, S. Nee, T. P. Curtis, Quantifying the roles of immigration and chance in shaping prokaryote community structure. Environmental Microbiology. 8, 732–740 (2006).\n7. Y. Zhang, F. Shen, Y. Yang, M. Niu, D. Chen, L. Chen, S. Wang, Y. Zheng, Y. Sun, F. Zhou, H. Qian, Y. Wu, T. Zhu, Insights into the Profile of the Human Expiratory Microbiota and Its Associations with Indoor Microbiotas. Environmental Science \u0026amp; Technology. 56, 6282–6293 (2022).\n","date":"2023-03-17T00:00:00Z","image":"/p/community-assembly/comm_hub260f10f12bf64e0acb74c7ee8e930d5_36974_120x120_fill_q75_box_smart1.jpg","permalink":"/p/community-assembly/","title":"微生物群落构建（community assembly）"},{"content":"简介 linux, python和R语言应该是生信学习中最重要的几个部分。\nlinux是一种操作系统，由于许多生物信息学软件仅提供支持linux平台的命令行版本，而不是图形化界面，所以掌握该操作系统的基础文件管理，安装使用软件，输入输出逻辑，简单shell脚本编写等十分重要。\npython与R都是免费开源多平台的编程语言，并且都是较为容易学习上手的模仿自然语言语法的编程语言，且支持面向对象编程，两者在数据科学上都有着非常重要的应用。Python的生态系统非常丰富，拥有大量的数据科学库和框架，例如：NumPy、Pandas、Matplotlib、Seaborn、Scikit-learn、PyTorch、TensorFlow等，这些库可以帮助数据科学家快速处理和分析数据，构建模型。\nR是一种专门用于统计分析和数据可视化的语言，其生态系统也非常丰富，拥有大量的统计分析库和可视化工具，例如：ggplot2、dplyr、tidyr、shiny等。 R拥有非常强大的数据分析和可视化能力，它提供了很多专业的统计分析函数和图形，这些函数和图形可以直接用于数据分析和可视化，使分析和可视化变得更加轻松和高效。\n在此，我想先记录一下我使用的最多的R语言。\n常用功能 全面教程 小白上手首推北大李老师的课程讲义《R语言教程》，中文讲义，简单易读，甚至整本书就是用Rbookdown编写的，电子书也比较好复制代码进行学习。\n《R语言实战》,也有中文版，介绍地更加全面且有不少进阶知识，值得一看！\n然后是著名的英文书和参考手册：\n《An Introduction to R》 《R Cookbook》 更多关于R的书可以在Home | Bookdown这里找到。\n数据处理 我目前用的最多的是dplyr包进行数据处理，最开始是冲着类似linux下的|管道符去的，%\u0026gt;%符号真的挺好用的，可以让我们无需产生许多中间结果 (准确来说%\u0026gt;%来自magrittr包，并且R4.1+也支持了原生管道符|\u0026gt;，但用起来还是%\u0026gt;%顺手，还有%\u0026lt;\u0026gt;%等变体)：\n1 2 library(dplyr) head(iris)%\u0026gt;%select(Species)%\u0026gt;%unique() 随后便发现了tidyverse全家桶，其中包含ggplot2，tibble，dplyr，readr，stringr等常用的全面的数据处理包，能够很好的满足我们下有数据处理需求：\n读取数据\n清洗数据\n转换数据\n合并数据\n筛选数据\n可视化\n这里给出官方教程：《R for Data Science》，非常建议熟读并使用。\n可视化 可视化一直是R的突出优点，basic graphic和ggplot是我最常用的绘图系统（偶尔会用plotly画一些交互图形，但大多数是只需要presentation和文章里要用的静态图形），两者的绘图逻辑有所差异，推荐在学会R的基础使用和数据处理后就体验ggplot。\nggplot是由Hadley Wickham创建的，并被包含在R语言的ggplot2包中。主要思想是将数据可视化看作是图层的堆叠，其中每个图层代表了一个数据属性的可视化。ggplot提供了一系列的函数和参数，让用户能够轻松地组合这些图层，以及调整颜色、标签、轴等其他属性。\nggplot支持的图形类型包括散点图、折线图、直方图、密度图、箱线图等等，可以满足大多数数据可视化的需求。优点在于其语法简洁、易于理解和使用，并且生成的图表质量高、美观、易于理解。它也被广泛地应用于学术研究、数据分析、商业报告等领域。\n同样给出官方教程：《ggplot2: Elegant Graphics for Data Analysis》，优雅，太优雅了。这本书也是可以找到大佬们翻译的中文版的，可以当作作图工具书使用，许多无法直接Google到的细节问题可能能在书中找到。\n此外要提一下ggpubr包，其名为Publication Ready Plots，提供一些方便的做出出版级figure的函数。ggpubr: Publication Ready Plots - Articles - STHDA 这个网站提供了不少好看图的绘制流程。\n当掌握了基本的ggplot绘制逻辑后，还可以尝试各种基于同样逻辑的扩展包，如ggcor，ggtree等等。\ngg_extensions 这个网站收集了很多此类拓展包并支持制作上传自创的gg拓展（当然暂时有点难😮‍💨）\n最后，强烈推荐一个为可视化而建的网站From data to Viz | Find the graphic you need (data-to-viz.com)，就像网站名一样，丰富多彩的可视化形式帮我们找到适合自己数据的样式，并提供画某一类图的python，R甚至D3.js代码，真的非常方便地copy代码，替换我们的数据，少量的修改就能有很不错的效果😄。\nR编程 R是一个宝库，几乎总能在上面找到合适的包实现我们想要的功能，但他也是一种编程语言，熟悉其编程语法对进阶使用十分重要。\n基本的数据读取与输出，if else、switch、for循环与简单函数编写需要掌握。\n同时因为R是一个比较\u0026quot;慢\u0026quot;的语言，尽量使用向量式编程、apply替代for循环、RCpp编写函数等方法提高其效率（后面可能会专门讲讲）。\n这里推荐大佬的《Advanced R》，对R高级编程技术进行讲解。\nRmarkdown R Markdown是一种文本格式和工具，用于创建具有代码、文本、图形和输出结果的可重复性报告。它是在R语言环境中开发的，但也可以与其他编程语言（如Python）结合使用。\n使用R Markdown，您可以将代码、文本和图形组合到一个文件中，并使用R Markdown语法将它们组织成结构化的文档。然后，您可以使用R Markdown编译器将文档转换为HTML、PDF、Word或其他格式的报告。\nR Markdown具有以下优点：\n可重复性：报告中的所有代码和结果都是可重复的，这有助于确保研究结果的可靠性和透明度。\n效率：使用R Markdown，您可以在单个文件中组织和记录所有内容，从而节省时间和精力。\n灵活性：R Markdown支持多种格式和输出选项，因此您可以根据需要自定义报告的样式和布局。\nYihui Xie大佬的官方文档介绍：R Markdown: The Definitive Guide (bookdown.org)\n以及参考书：R Markdown Cookbook (bookdown.org)\nRbookdown R的bookdown扩展包(https://github.com/rstudio/bookdown) 是继knitr和rmarkdown扩展包之后， 另一个增强markdown格式的扩展， 使得Rmd格式可以支持公式、定理、图表自动编号和引用、链接， 文献引用和链接等适用于编写书籍的功能。相当方便，我也稍微尝试写了一下：MetaNet Tutorial (asa12138.github.io)。\n参考书：bookdown: Authoring Books and Technical Documents with R Markdown\nRblogdown R扩展包blogdown可以与Hugo软件配合制作简单的静态网站。 网站的所有文件都存在于一个目录中， 只要上传到任意的网站服务器就可以发布， 没有任何限制。这篇博客就是基于Rblogdown写的，还是非常友好易上手的。\n参考书：blogdown: Creating Websites with R Markdown (bookdown.org)\n3.21更新：\n大多数网站的修改要在themes中实现，比如我现在使用的主题不支持latex渲染，想要加上这个功能，我先找到了yihui大佬的主题实现这个功能的Javascript代码，即hugo-lithium/layouts/partials/footer_mathjax.html，然后把它复制到目前用的主题的footer目录下（可能没有这个目录，而是footer.html同目录下），记得在include.html或者其他layout文件中加入{{ partial \u0026quot;footer/footer_mathjax.html\u0026quot; . }}，这样就能顺利调用mathjax了。\nR Presentation R Markdown文件(.Rmd)支持生成网页格式的幻灯片(slidy_presentation)， 以及LaTeX beamer格式的PDF幻灯片(beamer_presentation)， 和Microsoft Office的PowerPoint幻灯片(powerpoint_presentation)格式。\n目前稍微尝试了一些Yihui大佬的\u0026quot;写轮眼\u0026quot;包，但没有试着做自己的slides，以后有机会一定试试。\n写R包 积极尝试\n参考书：R Packages (2e) (r-pkgs.org)\n制作shiny应用 积极尝试\n官方教程：Shiny Learning Resources\n参考书：Mastering Shiny\n","date":"2023-03-16T00:00:00Z","image":"/p/r/images/data2viz_hu2949f787b14e5432e8dbe93255a87ee8_671473_120x120_fill_box_smart1_3.png","permalink":"/p/r/","title":"R语言学习"},{"content":"最近倒腾了不少新玩意，比如这个博客系统，在此写下第一篇博客。\n从刚开始接触生信到现在已经3年了，我也从本科生变成了博士生，专业从生物科学变成了生物信息学。走上科研道路，可能是种种巧合与选择的结果，但现在脑子里想的就是将二十来年的各种知识运用在研究上，做一些有意思的东西。\n希望自己有空可以多更新，帮助自己整理思路，记录一些实验和数据分析的方法技巧，调节一下心情😂。\n","date":"2023-03-14T00:00:00Z","image":"/p/%E6%98%A5%E4%B9%8B%E4%BC%8A%E5%A7%8B/spring_hu0533df8c600b6ebf5e775c57536fef00_394558_120x120_fill_q75_box_smart1.jpeg","permalink":"/p/%E6%98%A5%E4%B9%8B%E4%BC%8A%E5%A7%8B/","title":"春之伊始"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nFormula $a=sum_i^2$ $a=sum_i$\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nHyperlinked image The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-03-11T00:00:00Z","image":"/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash_hud7e36f7e20e71be184458283bdae4646_55974_120x120_fill_q75_box_smart1.jpg","permalink":"/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"}]