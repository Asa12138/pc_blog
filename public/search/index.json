[{"content":" Introduction Functional enrichment analysis is a computational method for analyzing the degree of enrichment of functional patterns in gene collections or genomic data. It can help reveal which functional modules, metabolic pathways, gene families, etc. are statistically enriched or significantly overrepresented in a specific biological context.\nTable 1: Methods for microbial enrichment analysis. Method Tools Notes Hypergeometric test / Fisher’ exact test [DAVID](https://david.ncifcrf.gov/) (website) [clusterProfiler](https://bioconductor.org/packages/release/bioc/html/clusterProfiler.html) (R package) , etc. The most common method used in enrichment analysis. The Database for Annotation, Visualization and Integrated Discovery (DAVID) provides a comprehensive set of functional annotation tools for investigators to understand the biological meaning behind large lists of genes. ClusterProfiler automates the process of biological-term classification and the enrichment analysis of gene clusters, which calculates enrichment test for GO terms and KEGG pathways based on hypergeometric distribution. Gene set enrichment analysis (GSEA) [GSEA](https://www.gsea-msigdb.org/gsea/index.jsp) (website) [clusterProfiler](https://bioconductor.org/packages/release/bioc/html/clusterProfiler.html) (R package) Gene Set Enrichment Analysis (GSEA) is a computational method that determines whether an a priori defined set of genes shows statistically significant, concordant differences between two biological states. ClusterProfiler can also do GSEA. Reporter score analysis [Reporterscore](https://github.com/Asa12138/ReporterScore) (R package) The plus or minus sign of reporter score does not represent regulation direction in the \"mixed\" mode but useful in \"directed\" mode. Reporter feature analysis [Piano](https://www.bioconductor.org/packages/release/bioc/html/piano.html) (R package) Reporter feature can achieve enrichment ananlysis for non-directional, mixed-directional up/down-regulation, and distinct-directional up/down-regulation classes. Reporter score is a new method of improved microbial enrichment analysis. This method was originally developed to reveal transcriptional regulation patterns in metabolic networks, and has been introduced into microbial research for functional enrichment analysis.\nThe Reporter score algorithm was originally developed by Patil and Nielsen in 2005 to identify metabolites in metabolic regulatory hotspots (1).\nA recent article discussed the misuse of the positive and negative sign of reporter-score (2)：\nhttps://mp.weixin.qq.com/s?__biz=MzUzMjA4Njc1MA==\u0026mid=2247507105\u0026idx=1\u0026sn=d5a0f0aaf176e245de7976f0a48f87a8#rd\nThe main conclusion is that the reporter score algorithm (above) is an enrichment method that ignores the up/down regulation information of KOs in the pathway, and it is incorrect to directly regard the sign of the reporter score as the regulation direction of the pathway.\nHere we have implement the fast calculation of the classic reporterscore (mixed mode), and developed an algorithm in the directed mode, which can give the biological meaning of plus or minus sign of the reporterscore.\nUsage 1 2 3 4 5 install.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#39;Asa12138/pcutils\u0026#39;,dependencies=T) devtools::install_github(\u0026#39;Asa12138/ReporterScore\u0026#39;,dependencies=T) library(ReporterScore) Method mixed “mixed” mode is the original reporter-score method from Patil, K. R. et al. PNAS 2005.\nIn this mode, the reporter score is Undirected, and the larger the reporter score, the more significant the enrichment, but it cannot indicate the up-and-down regulation information of the pathway！(Liu, L. et al. iMeta 2023.)\nsteps: 1. Use the Wilcoxon rank sum test to obtain the P value of the significance of each KO difference between the two groups (ie$P_{koi}$, i represents a certain KO);\nUsing an inverse normal distribution, convert the P value of each KO into a Z value ($Z_{koi}$), the formula:\n“Upgrade” KO to pathway:$Z_{koi}$, calculate the Z value of the pathway, the formula:\n$$ Z_{pathway}=\\frac{1}{\\sqrt{k}}\\sum Z_{koi} $$\nwhere k means A total of k KOs were annotated to the corresponding pathway;\nEvaluate the degree of significance: permutation (permutation) 1000 times, get the random distribution of$Z_{pathway}$, the formula: $$ Z_{adjustedpathway}=(Z_{pathway}-\\mu _k)/\\sigma _k $$ $μ_k$ is The mean of the random distribution,$σ_k$ is the standard deviation of the random distribution.\ndirected Instead, “directed” mode is a derived version of “mixed”, referenced from https://github.com/wangpeng407/ReporterScore. This approach is based on the same assumption of many differential analysis methods: the expression of most genes has no significant change.\nUse the Wilcoxon rank sum test to obtain the P value of the significance of each KO difference between the two groups (ie$P_{koi}$, i represents a certain KO), and then divide the P value by 2, that is, the range of (0,1] becomes (0,0.5],$P_{koi}=P_{koi}/2$;\nUsing an inverse normal distribution, convert the P value of each KO into a Z value ($Z_{koi}$), the formula:\n$$ Z_{koi}=\\theta ^{-1}(1-P_{koi}) $$\nsince the above P value is less than 0.5, all Z values will be greater than 0;\nConsidering whether each KO is up-regulated or down-regulated, calculate$\\Delta KO_i$, $$ \\Delta KO_i=\\overline {KO_{i_{g1}}}-\\overline {KO_{i_{g2}}} $$ $\\overline {KO_{i_{g1}}}$ is average abundance of$KO_i$ in group1,$\\overline {KO_{i_{g2}}}$ is average abundance of$KO_i$ in group2. Then,\n$$ Z_{koi} = \\begin{cases} -Z_{koi}, \u0026amp; (\\Delta KO_i\u0026lt;0) \\ Z_{koi}, \u0026amp; (\\Delta KO_i \\ge 0) \\end{cases} $$\nso$Z_{koi}$ is greater than 0 Up-regulation,$Z_{koi}$ less than 0 is down-regulation;\n“Upgrade” KO to pathway:$Z_{koi}$, calculate the Z value of the pathway, $$ Z_{pathway}=\\frac{1}{\\sqrt{k}}\\sum Z_{koi} $$\nwhere k means a total of k KOs were annotated to the corresponding pathway;\nEvaluate the degree of significance: permutation (permutation) 1000 times, get the random distribution of$Z_{pathway}$, the formula: $$ Z_{adjustedpathway}=(Z_{pathway}-\\mu _k)/\\sigma _k $$ $μ_k$ is The mean of the random distribution,$σ_k$ is the standard deviation of the random distribution.\nThe finally obtained$Z_{adjustedpathway}$ is the Reporter score value enriched for each pathway.\nIn this mode, the Reporter score is directed, and a larger positive value represents a significant up-regulation enrichment, and a smaller negative values represent significant down-regulation enrichment.\nHowever, the disadvantage of this mode is that when a pathway contains about the same number of significantly up-regulates KOs and significantly down-regulates KOs, the final absolute value of Reporter score may approach 0, becoming a pathway that has not been significantly enriched.\nReference 1. K. R. Patil, J. Nielsen, Uncovering transcriptional regulation of metabolism by using metabolic network topology. Proceedings of the National Academy of Sciences of the United States of America. 102, 2685–2689 (2005).\n2. L. Liu, R. Zhu, D. Wu, Misuse of reporter score in microbial enrichment analysis. iMeta. n/a, e95.\n","date":"2023-07-04T00:00:00Z","permalink":"/p/reporterscore-package/","title":"ReporterScore package"},{"content":"pctax pctax provides a comprehensive suite of tools for analyzing microbiome data.\nInstall 1 2 3 4 install.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#39;Asa12138/pcutils\u0026#39;,dependencies=T) devtools::install_github(\u0026#39;Asa12138/pctax\u0026#39;,dependencies=T) Usage It includes functionalities for α-diversity analysis, β-diversity analysis, differential analysis, community assembly, visualization of phylogenetic tree and functional enrichment analysis\u0026hellip;\nLook at the test data:\n1 2 3 4 5 6 library(pctax) library(pcutils) data(otutab,package = \u0026#34;pcutils\u0026#34;) #help(otutab) head(otutab) 1 2 3 4 5 6 7 8 9 10 11 12 13 ## NS1 NS2 NS3 NS4 NS5 NS6 WS1 WS2 WS3 WS4 ## s__un_f__Thermomonosporaceae 1092 1920 810 1354 1064 1070 1252 1597 1330 941 ## s__Pelomonas_puraquae 1962 1234 2362 2236 2903 1829 644 495 1230 1284 ## s__Rhizobacter_bergeniae 588 458 889 901 1226 853 604 470 1070 1028 ## s__Flavobacterium_terrae 244 234 1810 673 1445 491 318 1926 1493 995 ## s__un_g__Rhizobacter 1432 412 533 759 1289 506 503 590 445 620 ## WS5 WS6 CS1 CS2 CS3 CS4 CS5 CS6 ## s__un_f__Thermomonosporaceae 1233 1011 2313 2518 1709 1975 1431 1527 ## s__Pelomonas_puraquae 953 635 1305 1516 844 1128 1483 1174 ## s__Rhizobacter_bergeniae 846 670 1029 1802 1002 1200 1194 762 ## s__Flavobacterium_terrae 577 359 1080 1218 754 423 1032 1412 ## s__un_g__Rhizobacter 657 429 1132 1447 550 583 1105 903 ## [ reached \u0026#39;max\u0026#39; / getOption(\u0026#34;max.print\u0026#34;) -- omitted 1 rows ] 1 head(metadata) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## Id Group env1 env2 env3 env4 env5 env6 ## NS1 NS1 NS 3.057248 10.23571 5.554576 8.084997 25.00795 -1.15456682 ## NS2 NS2 NS 4.830219 11.13453 5.613455 8.556829 16.67690 0.81168745 ## NS3 NS3 NS 3.753133 10.06232 5.582916 10.226572 21.68926 1.40733211 ## NS4 NS4 NS 4.262264 10.84401 5.258419 9.002256 24.81046 1.47805320 ## NS5 NS5 NS 2.476135 7.52584 6.255314 9.357587 19.70553 0.05813095 ## NS6 NS6 NS 5.131004 10.82761 5.180966 8.141506 18.39021 -1.70032569 ## lat long ## NS1 38.72412 118.2493 ## NS2 38.31086 115.2322 ## NS3 36.82439 118.1361 ## NS4 37.59774 117.1563 ## NS5 35.94188 118.9504 ## NS6 37.68713 116.2984 1 head(taxonomy) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 ## Kingdom Phylum ## s__un_f__Thermomonosporaceae k__Bacteria p__Actinobacteria ## s__Pelomonas_puraquae k__Bacteria p__Proteobacteria ## s__Rhizobacter_bergeniae k__Bacteria p__Proteobacteria ## s__Flavobacterium_terrae k__Bacteria p__Bacteroidetes ## s__un_g__Rhizobacter k__Bacteria p__Proteobacteria ## s__un_o__Burkholderiales k__Bacteria p__Proteobacteria ## Class Order ## s__un_f__Thermomonosporaceae c__Actinobacteria o__Actinomycetales ## s__Pelomonas_puraquae c__Betaproteobacteria o__Burkholderiales ## s__Rhizobacter_bergeniae c__Gammaproteobacteria o__Pseudomonadales ## s__Flavobacterium_terrae c__Flavobacteriia o__Flavobacteriales ## s__un_g__Rhizobacter c__Gammaproteobacteria o__Pseudomonadales ## s__un_o__Burkholderiales c__Betaproteobacteria o__Burkholderiales ## Family ## s__un_f__Thermomonosporaceae f__Thermomonosporaceae ## s__Pelomonas_puraquae f__Comamonadaceae ## s__Rhizobacter_bergeniae f__Pseudomonadaceae ## s__Flavobacterium_terrae f__Flavobacteriaceae ## s__un_g__Rhizobacter f__Pseudomonadaceae ## s__un_o__Burkholderiales f__un_o__Burkholderiales ## Genus ## s__un_f__Thermomonosporaceae g__un_f__Thermomonosporaceae ## s__Pelomonas_puraquae g__Pelomonas ## s__Rhizobacter_bergeniae g__Rhizobacter ## s__Flavobacterium_terrae g__Flavobacterium ## s__un_g__Rhizobacter g__Rhizobacter ## s__un_o__Burkholderiales g__un_o__Burkholderiales ## Species ## s__un_f__Thermomonosporaceae s__un_f__Thermomonosporaceae ## s__Pelomonas_puraquae s__Pelomonas_puraquae ## s__Rhizobacter_bergeniae s__Rhizobacter_bergeniae ## s__Flavobacterium_terrae s__Flavobacterium_terrae ## s__un_g__Rhizobacter s__un_g__Rhizobacter ## s__un_o__Burkholderiales s__un_o__Burkholderiales α-diversity analysis Calculate a_diversity of otutab then link to experiment group or environment variable.\n1 2 a_diversity(otutab)-\u0026gt;a_res plot(a_res,metadata,\u0026#34;Group\u0026#34;) Figure 1: α-diversity\n1 plot(a_res,metadata,\u0026#34;env1\u0026#34;) Figure 2: α-diversity\nβ-diversity analysis There are a range of dimensionality reduction methods available for analysis, including Constrained and non-Constrained.\nLike PCA, PCoA, NMDS, RDA, CCA\u0026hellip; For example:\nPCA:\n1 2 b_analyse(otutab,method = \u0026#34;pca\u0026#34;)-\u0026gt;b_res plot(b_res,\u0026#34;Group\u0026#34;,metadata,bi = T,rate=0.5) 1 ## $PCA Figure 3: PCA for β-diversity\n1 plot(b_res,\u0026#34;Group\u0026#34;,metadata,mode = 3) 1 ## $PCA Figure 4: PCA for β-diversity\nRDA:\n1 2 3 env=metadata[,6:10] #RDA myRDA(otutab,env)-\u0026gt;phy.rda 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ## ## Call: ## vegan::decorana(veg = dat.h) ## ## Detrended correspondence analysis with 26 segments. ## Rescaling of axes with 4 iterations. ## Total inertia (scaled Chi-square): 0.3207 ## ## DCA1 DCA2 DCA3 DCA4 ## Eigenvalues 0.03127 0.02265 0.01916 0.01729 ## Additive Eigenvalues 0.03127 0.02265 0.01917 0.01727 ## Decorana values 0.03150 0.02146 0.01701 0.01035 ## Axis lengths 0.74268 0.74498 0.57253 0.52361 ## ## DCA analysis, select the sorting analysis model according to the first value of the Axis lengths row ## Axis Lengths \u0026gt;4.0-CCA (based on unimodal model, canonical correspondence analysis); ## If it is between 3.0-4.0 - both RDA/CCA; ## If less than 3.0-RDA (based on linear model, redundancy analysis) ## [1] \u0026#34;===============Initial Model================\u0026#34; ## [1] \u0026#34;Initial cca, vif\u0026gt;20 indicates serious collinearity:\u0026#34; ## env4 env5 env6 lat long ## 2.671744 3.148860 1.161378 1.417139 1.120735 ## Initial Model R-square: 0.05504835 ## [1] \u0026#34;=============Statistics===========\u0026#34; ## 0.3329753 Constrained indicates the degree to which environmental factors explain differences in community structure ## 0.6670247 unconstrained means that the environmental factors cannot explain the part of the community structure 1 RDA_plot(phy.rda,\u0026#34;Group\u0026#34;,metadata) Figure 5: RDA for β-diversity associated environmental variables\nDifferential analysis There are also lots of statistic methods for differential analysis: ALDEX, ANCOM2, randomForest, t.test, wilcox.test\u0026hellip; or deseq2, limma\u0026hellip;(Commonly used in transcriptome)\n1 2 diff_da(otutab,metadata[\u0026#34;Group\u0026#34;])-\u0026gt;res volcano_p(res) Figure 6: Volcano plot of differential analysis\n1 volcano_p(res,mode=2) Figure 7: Volcano plot of differential analysis\nCommunity assembly Community assembly in microbiome refers to the processes that shape the composition, diversity, and structure of microbial communities in a particular environment or host. Microbiome consist of diverse microbial populations that interact with each other and their surroundings, and understanding how these communities assemble is crucial for comprehending their ecological dynamics and functional implications.\n1 2 ncm(otutab)-\u0026gt;ncm_res plot(ncm_res) Figure 8: NCM model\nPhylogenetic tree 1 2 ann_tree(taxonomy,otutab)-\u0026gt;tree easy_tree(tree,add_abundance=FALSE) Figure 9: Phylogenetic tree\nCite Please cite:\nChen P (2023). pctax: Professional Comprehensive Microbiome Data Analysis. R package, https://github.com/Asa12138/pctax.\n","date":"2023-06-20T00:00:00Z","image":"/p/r-pctax/index.en_files/figure-html/rda-1_hu97483387ecb0ed5e4c4cad6758b9b089_143717_120x120_fill_box_smart1_3.png","permalink":"/p/r-pctax/","title":"R微生物组分析流程（pctax包）"},{"content":" 我自己在用R做各种分析时有不少需要反复用到的基础功能，比如一些简单的统计呀，画一些简单的图如柱形图，箱形图等等，虽说具体实现的代码也不麻烦，但有一些细节如给箱形图加上显著性分组，将柱形图按照metadata合并或分面等事情还是不太想每次用的时候去找之前的代码。\n索性将常用的各种函数整成了一个包：pcutils， 网址：https://github.com/Asa12138/pcutils\n最近也成功将pcutils提交到CRAN了。将包提交到CRAN或Bioconductor也是一件有意思的事（有机会下次讲讲）。\n但目前还是建议从github安装，包含的功能会多一些:\n1 2 install.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#39;Asa12138/pcutils\u0026#39;,dependencies=T) 当然，今天主要要来讲的是柱形图以及其各种变换形式图形的绘制。\nIntroduction 柱形图（Bar chart）是一种常用的数据可视化图表，用于展示离散类别数据的分布和比较不同类别之间的数量或频率关系。柱形图通过在水平或垂直轴上绘制矩形柱来表示数据。\n在柱形图中，每个类别对应一个矩形柱，其高度表示该类别的数据量或频率。类别通常显示在水平轴上，而数量或频率则显示在垂直轴上。矩形柱的宽度通常是固定的，相互之间没有间隔，这样便于比较不同类别之间的差异。\n柱形图常用于以下情况：\n展示不同类别的数量或频率关系。 比较多个类别之间的数据。 强调特定类别的重要性或异常值。 之前强力推荐过的绘图教程网站：https://r-graph-gallery.com/ ，上面也提供了很多柱形图从基础到不断美化的画法：\nFrom data to Viz 示例柱形图 From data to Viz 示例柱形图2 再次建议大家在上面看看具体实现的代码。\nPlot Stackplot 因为常做的是微生物组等组学数据的分析，所以一般会用到两个表，一个是abundance table（行为基因名/物种名，列为样本名），另一个是metadata（行为样本名，列为各种实验分组或表型数据）， 要做的柱形图也经常是堆积柱形图，用来看composition的情况。\n我们都知道要用ggplot画图，一般都要把我们的表格从宽格式变为长格式， 具体的转换方法也有很多，我常用的是reshape2的melt， 但是柱形图太常用了，我便把格式转换和分组映射包含在了函数stackplot中，方便自己的每一次画图。\n1 2 3 4 5 6 7 8 library(pcutils) library(ggplot2) #?stackplot data(otutab) #将species level表上升到class level class=hebing(otutab,taxonomy$Class,1,\u0026#34;sum\u0026#34;) head(class) ## NS1 NS2 NS3 NS4 NS5 NS6 WS1 WS2 WS3 WS4 WS5 WS6 CS1 CS2 ## c__Acidobacteria_Gp10 0 10 2 12 0 0 15 0 27 0 36 11 30 0 ## c__Acidobacteria_Gp11 9 0 15 0 7 8 21 0 2 0 5 10 1 12 ## c__Acidobacteria_Gp17 8 12 3 4 7 0 11 4 0 12 9 9 11 14 ## c__Acidobacteria_Gp25 11 18 2 0 3 0 0 0 2 5 9 5 3 0 ## c__Acidobacteria_Gp3 13 0 10 0 12 18 16 14 14 0 20 12 8 5 ## CS3 CS4 CS5 CS6 ## c__Acidobacteria_Gp10 20 13 3 19 ## c__Acidobacteria_Gp11 9 0 0 0 ## c__Acidobacteria_Gp17 3 16 0 14 ## c__Acidobacteria_Gp25 23 0 3 6 ## c__Acidobacteria_Gp3 8 0 10 9 ## [ reached 'max' / getOption(\u0026quot;max.print\u0026quot;) -- omitted 1 rows ] 1 head(metadata) ## Id Group env1 env2 env3 env4 env5 env6 ## NS1 NS1 NS 3.057248 10.23571 5.554576 8.084997 25.00795 -1.15456682 ## NS2 NS2 NS 4.830219 11.13453 5.613455 8.556829 16.67690 0.81168745 ## NS3 NS3 NS 3.753133 10.06232 5.582916 10.226572 21.68926 1.40733211 ## NS4 NS4 NS 4.262264 10.84401 5.258419 9.002256 24.81046 1.47805320 ## NS5 NS5 NS 2.476135 7.52584 6.255314 9.357587 19.70553 0.05813095 ## NS6 NS6 NS 5.131004 10.82761 5.180966 8.141506 18.39021 -1.70032569 ## lat long ## NS1 38.72412 118.2493 ## NS2 38.31086 115.2322 ## NS3 36.82439 118.1361 ## NS4 37.59774 117.1563 ## NS5 35.94188 118.9504 ## NS6 37.68713 116.2984 1 2 #基础绘制，无分组信息，单纯展示每个样本Top7的相对丰度,Top7以外的自动归为Other stackplot(class,legend_title =\u0026#34;Class\u0026#34;)+scale_fill_manual(values =get_cols(10)) 1 2 3 #按分组合并丰度值，展示合并后的相对丰度变化 stackplot(class, metadata, group = \u0026#34;Group\u0026#34;,legend_title =\u0026#34;Class\u0026#34;)+ scale_fill_manual(name=\u0026#34;Class\u0026#34;,values =get_cols(10)) 1 2 3 4 #geom_bar可指定的参数可以传入bar_params中 stackplot(class, metadata, group = \u0026#34;Group\u0026#34;,bar_params = list(position=position_dodge()), legend_title =\u0026#34;Class\u0026#34;)+ scale_fill_manual(values =get_cols(10)) 1 2 3 #展示每一个部分的数字？number = T stackplot(class, metadata, group = \u0026#34;Group\u0026#34;,number = T)+ scale_fill_manual(name=\u0026#34;Class\u0026#34;,values =get_cols(10)) 1 2 3 #如果想把分组中的单个样本展示出来的话，可以设置style = \u0026#34;sample\u0026#34; stackplot(class, metadata, group = \u0026#34;Group\u0026#34;,style = \u0026#34;sample\u0026#34;)+ scale_fill_manual(name=\u0026#34;Class\u0026#34;,values =get_cols(10)) 1 2 3 #如果想看的是绝对量而不是相对丰度：relative = F stackplot(class, metadata, group = \u0026#34;Group\u0026#34;,style = \u0026#34;sample\u0026#34;,relative = F)+ scale_fill_manual(name=\u0026#34;Class\u0026#34;,values =get_cols(10)) 1 2 3 #我们有时候可以希望按照某个物种/基因的丰度排序，设置group_order： stackplot(class, metadata, group = \u0026#34;Id\u0026#34;,group_order = \u0026#34;c__Actinobacteria\u0026#34;)+ scale_fill_manual(name=\u0026#34;Class\u0026#34;,values =get_cols(10)) 1 2 3 #或者是直接更改每一个fill的上下顺序，设置stack_order： stackplot(class, metadata, group = \u0026#34;Id\u0026#34;,stack_order = \u0026#34;Other\u0026#34;)+ scale_fill_manual(name=\u0026#34;Class\u0026#34;,values =get_cols(10)) 1 2 3 #或是做出流动的效果，时间数据最佳：flow=T stackplot(class, metadata, group = \u0026#34;Id\u0026#34;,group_order = \u0026#34;c__Actinobacteria\u0026#34;,flow = T)+ scale_fill_manual(name=\u0026#34;Class\u0026#34;,values =get_cols(10)) Polar coordinate ggplot2没有提供直接绘制饼图的geom对象，但柱形图可以通过极坐标变换生成饼图或环形图等图形，使用coord_polar()函数可以实现极坐标变换。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 library(ggplot2) # 创建示例数据 data \u0026lt;- data.frame( category = c(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;), value = c(30, 20, 15, 35) ) # 绘制饼图 p \u0026lt;- ggplot(data, aes(x = \u0026#34;\u0026#34;, y = value, fill = category)) + geom_bar(width = 1, stat = \u0026#34;identity\u0026#34;) + theme_void() # 显示图形 p 1 p+coord_polar(theta = \u0026#34;y\u0026#34;) 为了方便自己日常使用，我将这一类通过柱形图极坐标变换得到的图形整合进了gghuan函数中：\n1 2 3 4 #help(gghuan) #输入数据格式很简单第一列是类别，第二列是数字（对应的某个值）即可。 a=data.frame(type = letters[1:6], num = c(1, 3, 3, 4, 5, 10)) gghuan(a) + scale_fill_manual(values = get_cols(6, \u0026#34;col3\u0026#34;)) 1 2 3 4 #大胆调节每一个设置 gghuan(a,bar_params=list(col=\u0026#34;black\u0026#34;), text_params=list(col=\u0026#34;#b15928\u0026#34;,size=3), text_params2=list(col=\u0026#34;#006d2c\u0026#34;,size=5))+scale_fill_manual(values = get_cols(6, \u0026#34;col3\u0026#34;)) 1 2 3 #改变绘图风格：mode=1~3 #mode=3就是饼图 gghuan(a,mode=3) + scale_fill_manual(values = get_cols(6, \u0026#34;col3\u0026#34;)) 1 2 #mode=2就是环状柱形图，相当于coord_polar(theta = \u0026#34;x\u0026#34;)的变换 gghuan(a,mode=2) + scale_fill_manual(values = get_cols(6, \u0026#34;col3\u0026#34;)) 多层的环形图一般用来画具有某种层次关系的表格，比如物种分类信息。\n下面使用函数gghuan2来完成多层环形图绘制:\n1 2 3 #提供一个具有一定层级关系的表格（除最后一列外有多少列就有多少个环，最后一列为丰度） cbind(taxonomy,num=rowSums(otutab))[1:10,c(1:3,8)]%\u0026gt;%dplyr::arrange_all()-\u0026gt;test head(test) ## Kingdom Phylum ## s__un_g__Actinoplanes k__Bacteria p__Actinobacteria ## s__Lentzea_flaviverrucosa k__Bacteria p__Actinobacteria ## s__un_g__Streptomyces k__Bacteria p__Actinobacteria ## s__un_f__Thermomonosporaceae k__Bacteria p__Actinobacteria ## s__Flavobacterium_terrae k__Bacteria p__Bacteroidetes ## s__un_g__Rhizobium k__Bacteria p__Proteobacteria ## Class num ## s__un_g__Actinoplanes c__Actinobacteria 8518 ## s__Lentzea_flaviverrucosa c__Actinobacteria 9508 ## s__un_g__Streptomyces c__Actinobacteria 10813 ## s__un_f__Thermomonosporaceae c__Actinobacteria 26147 ## s__Flavobacterium_terrae c__Flavobacteriia 16484 ## s__un_g__Rhizobium c__Alphaproteobacteria 7789 1 gghuan2(test,percentage = F)+scale_fill_manual(values = get_cols(10)) 这个多层环形图长得跟旭日图也比较像了,不过想画旭日图的话也可以用my_sunburst函数， 输入的数据格式仍然相同：\n1 my_sunburst(test) Streamgraph 我们前面已经用stackplot设置flow=T实现了在柱形图的分割间加上了连接的线， 那我们进一步想想,如果这个连接的线扩展到整个柱子，而柱子越来越细时，就成了一个面积图（areaplot）:\n1 areaplot(class,legend_title =\u0026#34;Class\u0026#34;)+scale_fill_manual(values =get_cols(10)) 更有意思的是，如果连接的线是曲线，并且流动的曲线扩展到整个柱子，就能画出流动图（河流图，Streamgraph）了。\n这里用streamgraph包的例子\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #Install #devtools::install_github(\u0026#34;hrbrmstr/streamgraph\u0026#34;) library(streamgraph) # Create data: data \u0026lt;- data.frame( year=rep(seq(1990,2016) , each=10), name=rep(letters[1:10] , 27), value=sample( seq(0,1,0.0001) , 270) ) # Basic stream graph: just give the 3 arguments pp \u0026lt;- streamgraph(data, key=\u0026#34;name\u0026#34;, value=\u0026#34;value\u0026#34;, date=\u0026#34;year\u0026#34;, height=\u0026#34;300px\u0026#34;, width=\u0026#34;1000px\u0026#34;) pp Sankey 如果我们的数据是每个柱子间有联系的（层级关系或其他包含关系均可），用柱子间的面作为这种通量的量化， 我们就得到了一个这样的图形，\n1 my_sankey(test,mode = \u0026#34;gg\u0026#34;) 如果柱子之间再拆开一点的话，这便是我们熟悉的桑基图了：\n1 my_sankey(test,mode = \u0026#34;gg\u0026#34;,space = 1e4) 总之，一个简单的柱形图也可以变化出各式各样的衍生图形。\n但所有的可视化方法都是为展示数据服务的， 我整合这些函数也是希望可以更关注数据本身，花更少的精力在调节图形上，先快速对我们的数据有整体的把握。\npcutils的初衷还是迎合我自己的编程与数据分析习惯的，所以可能并不适合所有人，大家也可以直接fork并修改我的源码，欢迎大家提出建议与意见。\n","date":"2023-06-19T00:00:00Z","image":"/p/r-bar-chart/images/barchart_hue6695c576ffcb589442c4c147a726928_158594_120x120_fill_box_smart1_3.png","permalink":"/p/r-bar-chart/","title":"R绘制柱形图及其变换（bar chart）"},{"content":"分享一下R进阶使用的小技巧，相当于是之前写的R语言学习的实践和总结了。\n点击Presentation查看slide，或者点击Download下载slide和相关的各种test文件。\n关键词：\n","date":"2023-06-19T00:00:00Z","permalink":"/p/r-tips/","title":"R进阶使用小技巧"},{"content":"Introduction 机器学习是一种人工智能的分支，旨在使计算机系统能够通过数据和经验自动学习和改进，而无需明确的编程指令。它的目标是通过构建和训练模型来解决各种复杂的问题和任务，使计算机能够自动从数据中学习规律、做出预测或进行决策。\n机器学习的主要方法包括监督学习、无监督学习和强化学习：\n监督学习（Supervised Learning）：在监督学习中，模型通过输入数据和对应的标签进行训练。目标是学习一个函数，将输入映射到正确的输出。常见的监督学习算法包括线性回归、逻辑回归、决策树、随机森林和支持向量机等。\n无监督学习（Unsupervised Learning）：在无监督学习中，模型仅使用输入数据进行训练，没有标签信息。目标是发现数据中的结构、模式和关系。无监督学习算法包括聚类、降维和关联规则挖掘等。\n强化学习（Reinforcement Learning）：强化学习通过模型与环境进行交互，根据环境给出的奖励和反馈来学习最优的行为策略。它主要用于智能体在复杂环境中做出决策和学习行为。\n在生物信息学领域，机器学习发挥着重要的作用，可以应用于各种生物数据的分析和解释。以下是一些机器学习在生物信息学上的应用示例：\n基因组学：机器学习可以用于基因组数据的分类、预测和注释，包括基因表达数据的聚类分析、基因功能预测和基因调控网络的构建等。\n蛋白质结构预测：机器学习算法可以通过分析已知的蛋白质结构和序列信息，预测未知蛋白质的结构和功能。\n药物设计：机器学习可用于药物发现和设计过程中，例如通过对已知药物和分子数据库的分析，预测药物相互作用和药物分子的性质。\n微生物组学：机器学习可以帮助解析微生物组的功能和组成，如通过分析微生物组数据来预测微生物的菌株、代谢功能\n机器学习在人类疾病研究中有许多具体应用。下面是一些常见的机器学习在人类疾病研究中的应用示例：\n疾病预测和诊断：机器学习可以通过分析患者的临床数据、基因组数据、影像数据等，帮助进行疾病预测和诊断。例如，可以利用机器学习算法构建模型来预测疾病的风险，比如癌症、心血管疾病等，或者通过图像识别技术进行疾病的早期检测和诊断。\n药物研发和个体化治疗：机器学习可以用于药物研发过程中，例如通过分析大量的分子数据和药物效应数据，预测药物的活性、副作用和目标蛋白等。此外，机器学习还可以根据患者的个体特征和基因组信息，进行个体化治疗方案的设计和优化。\n疾病预后和生存分析：机器学习可以通过分析患者的临床数据、基因组数据和生存数据，预测患者的预后和生存情况。这对于制定治疗方案、评估治疗效果和指导临床决策具有重要意义。\n疾病子类型和分类：机器学习可以通过对大规模的生物标志物数据进行聚类和分类，帮助确定不同疾病的亚型和分类。这有助于深入了解疾病的异质性，为个体化治疗和精准医学提供基础。\n健康管理和预防：机器学习可以利用个人的健康数据和生活方式信息，建立模型来预测患病风险、制定个性化的健康管理计划，并提供个性化的健康建议和预防措施。\n算法 以下是一些常用算法的简要介绍：\nLASSO（Least Absolute Shrinkage and Selection Operator）：LASSO是一种用于回归和特征选择的线性模型。它通过对模型的参数加上L1正则化项，促使模型中的某些特征权重变为零，从而实现特征选择和模型简化。\nLightGBM：LightGBM是一种梯度提升框架，用于解决分类和回归问题。它采用基于梯度的决策树算法，具有快速训练速度和高效的内存使用。LightGBM还支持并行训练和大规模数据处理。\nCatBoost：CatBoost是一种梯度提升框架，专门用于处理具有类别特征的数据。它能够自动处理类别特征的编码，并具有较强的泛化能力和鲁棒性。\nXGBoost（eXtreme Gradient Boosting）：XGBoost也是一种梯度提升框架，用于解决分类和回归问题。它使用了正则化技术、并行化处理和自定义损失函数等特性，具有较强的准确性和可解释性。\nRF（Random Forest）：随机森林是一种集成学习方法，通过构建多个决策树并进行投票或平均来进行分类和回归。它具有良好的鲁棒性、准确性和抗过拟合能力。\nLR（Logistic Regression）：逻辑回归是一种用于分类问题的线性模型。它使用逻辑函数（如sigmoid函数）来估计样本属于某个类别的概率，并进行分类决策。\nSVM（Support Vector Machines）：支持向量机是一种广泛应用于分类和回归问题的监督学习算法。它通过构建最优的超平面或者间隔最大化来进行分类。\nCNN（Convolutional Neural Network）：卷积神经网络是一种在计算机视觉和图像处理中广泛应用的深度学习算法。它通过卷积层、池化层和全连接层等结构，实现对图像和空间数据的特征提取和分类。\n示例 LASSO（最小绝对收缩和选择算子） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 导入所需的库 library(glmnet) # 加载iris数据集 data(iris) # 划分数据集为训练集和测试集 set.seed(123) trainIndex \u0026lt;- createDataPartition(iris$Species, p = 0.8, list = FALSE) trainData \u0026lt;- iris[trainIndex, ] testData \u0026lt;- iris[-trainIndex, ] # 定义自变量和因变量 x \u0026lt;- as.matrix(trainData[, -5]) y \u0026lt;- as.factor(trainData$Species) # 使用LASSO进行特征选择和分类预测 lassoModel \u0026lt;- cv.glmnet(x, y, family = \u0026#34;multinomial\u0026#34;) lassoPredictions \u0026lt;- predict(lassoModel, newx = as.matrix(testData[, -5]), s = \u0026#34;lambda.min\u0026#34;, type = \u0026#34;class\u0026#34;) # 评估预测结果 confusionMatrix(lassoPredictions, testData$Species) LightGBM 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # 导入所需的库 library(lightgbm) # 加载iris数据集 data(iris) # 划分数据集为训练集和测试集 set.seed(123) trainIndex \u0026lt;- createDataPartition(iris$Species, p = 0.8, list = FALSE) trainData \u0026lt;- iris[trainIndex, ] testData \u0026lt;- iris[-trainIndex, ] # 定义训练参数 params \u0026lt;- list(objective = \u0026#34;multiclass\u0026#34;, metric = \u0026#34;multi_logloss\u0026#34;, num_class = 3) # 使用LightGBM进行分类预测 lightgbmModel \u0026lt;- lgb.train(params, data = lgb.Dataset(as.matrix(trainData[, -5]), label = trainData$Species), nrounds = 100) lightgbmPredictions \u0026lt;- predict(lightgbmModel, as.matrix(testData[, -5])) # 转换为类别标签 lightgbmPredictions \u0026lt;- colnames(lightgbmPredictions)[apply(lightgbmPredictions, 1, which.max)] # 评估预测结果 confusionMatrix(lightgbmPredictions, testData$Species) CatBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # 导入所需的库 library(catboost) # 加载iris数据集 data(iris) # 划分数据集为训练集和测试集 set.seed(123) trainIndex \u0026lt;- createDataPartition(iris$Species, p = 0.8, list = FALSE) trainData \u0026lt;- iris[trainIndex, ] testData \u0026lt;- iris[-trainIndex, ] # 转换为CatBoost适用的数据格式 catFeatures \u0026lt;- c(5) # 指定类别特征的列索引 trainPool \u0026lt;- catboost.load_pool(data = as.matrix(trainData[, -5]), label = trainData$Species, cat_features = catFeatures) testPool \u0026lt;- catboost.load_pool(data = as.matrix(testData[, -5]), label = testData$Species, cat_features = catFeatures) # 定义训练参数 params \u0026lt;- list(loss_function = \u0026#34;MultiClass\u0026#34;, eval_metric = \u0026#34;MultiClass\u0026#34;, iterations = 100) # 使用CatBoost进行分类预测 catboostModel \u0026lt;- catboost.train(trainPool, params) catboostPredictions \u0026lt;- catboost.predict(catboostModel, testPool) # 评估预测结果 confusionMatrix(catboostPredictions, testData$Species) XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # 导入所需的库 library(xgboost) # 加载iris数据集 data(iris) # 划分数据集为训练集和测试集 set.seed(123) trainIndex \u0026lt;- createDataPartition(iris$Species, p = 0.8, list = FALSE) trainData \u0026lt;- iris[trainIndex, ] testData \u0026lt;- iris[-trainIndex, ] # 转换为XGBoost适用的数据格式 xgbMatrix \u0026lt;- xgb.DMatrix(as.matrix(trainData[, -5]), label = trainData$Species) # 定义训练参数 params \u0026lt;- list(objective = \u0026#34;multi:softmax\u0026#34;, eval_metric = \u0026#34;mlogloss\u0026#34;, num_class = 3) # 使用XGBoost进行分类预测 xgbModel \u0026lt;- xgb.train(params, xgbMatrix, nrounds = 100) xgbPredictions \u0026lt;- predict(xgbModel, as.matrix(testData[, -5])) # 转换为类别标签 xgbPredictions \u0026lt;- colnames(xgbPredictions)[apply(xgbPredictions, 1, which.max)] # 评估预测结果 confusionMatrix(xgbPredictions, testData$Species) RF (随机森林） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 加载randomForest库 library(randomForest) # 加载iris数据集 data(iris) # 划分数据集为训练集和测试集 set.seed(123) trainIndex \u0026lt;- createDataPartition(iris$Species, p = 0.8, list = FALSE) trainData \u0026lt;- iris[trainIndex, ] testData \u0026lt;- iris[-trainIndex, ] # 使用随机森林进行分类预测 rfModel \u0026lt;- randomForest(Species ~ ., data = trainData, ntree = 100) rfPredictions \u0026lt;- predict(rfModel, newdata = testData) # 评估预测结果 confusionMatrix(rfPredictions, testData$Species) LR (逻辑回归) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 加载glm库 library(glm) # 加载iris数据集 data(iris) # 划分数据集为训练集和测试集 set.seed(123) trainIndex \u0026lt;- createDataPartition(iris$Species, p = 0.8, list = FALSE) trainData \u0026lt;- iris[trainIndex, ] testData \u0026lt;- iris[-trainIndex, ] # 使用逻辑回归进行分类预测 lrModel \u0026lt;- glm(Species ~ ., data = trainData, family = \u0026#34;binomial\u0026#34;) lrProbabilities \u0026lt;- predict(lrModel, newdata = testData, type = \u0026#34;response\u0026#34;) lrPredictions \u0026lt;- ifelse(lrProbabilities \u0026gt; 0.5, \u0026#34;versicolor\u0026#34;, \u0026#34;setosa\u0026#34;) # 评估预测结果 confusionMatrix(lrPredictions, testData$Species) SVM（支持向量机） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 加载e1071库 library(e1071) # 加载iris数据集 data(iris) # 划分数据集为训练集和测试集 set.seed(123) trainIndex \u0026lt;- createDataPartition(iris$Species, p = 0.8, list = FALSE) trainData \u0026lt;- iris[trainIndex, ] testData \u0026lt;- iris[-trainIndex, ] # 使用支持向量机进行分类预测 svmModel \u0026lt;- svm(Species ~ ., data = trainData, kernel = \u0026#34;linear\u0026#34;) svmPredictions \u0026lt;- predict(svmModel, newdata = testData) # 评估预测结果 confusionMatrix(svmPredictions, testData$Species) ","date":"2023-05-28T00:00:00Z","permalink":"/p/r-machine-learning/","title":"R语言机器学习（基础）"},{"content":"Introduction 2023.5.20-5.22，本人有幸参加了2023肠道大会，“CHINAGUT 2023系列活动”，这是我博士期间参加的第一个学术会议，怀着开阔眼界，见识科学前沿，学习新方法新技术的心态，听完了三天的报告，认识了一些大佬，收获颇丰。\n大会持续秉承”爱肠道、爱学习、爱整合”的永久主题，拟设立超过30个学术会场，深度研讨微生物组、医药转化、营养与食品、动物肠道、免疫、 代谢、神经、生理与发育、肿瘤、炎症性肠病、整合肠病学、特殊人群、技术方法、同一健康等科学问题。\n在此记录一些我印象较深，和我研究方向相关的报告。\n组学实验技术 临床低生物量微生物组 香港大学黄适博士报告\n近年来,微生物组不仅在粪便、口腔、皮肤等位点，而且在多种人体组织样品中被发现，揭示其在慢性疾病发生过程中发挥的关键作用。但是，这些样品的低生物量或者高宿主DNA污染的特征使得在株水平分析人体微生物组变得更加具有挑战性、甚至难以实现。\n在当前微生物组研究中，解析微生物群落的物种构成主要依赖于两种高通量研究技术：\n扩增子测序（16S/18S/ITS）存在扩增偏好性、脱靶扩增、分辨率低等问题，且无法同时检测细菌、真菌、古菌。\n鸟枪宏基因组测序（WMS），虽然鸟枪宏基因组测序可以有效避免上述问题，但该方法对样本DNA质量要求高，面对大规模临床样本容易受到成本限制，且不适合于检测含有大量宿主DNA的样本，如石蜡切片、肿瘤切片、肿瘤组织等样品检测。\n2bRDA 实验与分析流程 他们开发了一种简化宏基因组方法（2bRAD）(1)，主要利用IIB型内切酶实现捕获约1%的基因组序列进行DNA测序，因而增加了每个微生物基因组覆盖度，从而实现高精度分析低生物量微生物组的目标。\n配套生信分析流程（Strain2bFunc）用于株水平鉴定和丰度估计分析。\n壁报：Strain-resolved Taxonomic Profiling And Functional Prediction Of Human Microbiota Using Strain2bfunc-2023肠道大会 (chinagut.cn)\n2bRDA 准确度评估 目前感觉缺点是类似16S，只关心物种组成，没有考虑真正的功能信息（鸟枪法能够做到），但是临床低生物量的应用还是非常不错。\nMetaproteomics 李乐园老师报告：深度宏蛋白质组揭示人肠道微生物群蛋白质水平功能冗余度 (chinagut.cn)\n宏蛋白组（metaproteomics）是一种研究微生物群落中蛋白质组的方法。与宏基因组学类似，宏蛋白组学关注的是微生物群落中所有微生物的蛋白质组成。它通过分析和识别微生物群落中的蛋白质，旨在揭示微生物群落的功能和代谢活动。\n宏蛋白组学涉及从样品中提取蛋白质，对蛋白质进行消化（例如使用胰蛋白酶将蛋白质分解为肽段），然后使用质谱技术（如液相色谱-质谱联用，LC-MS/MS）对这些肽段进行分析和鉴定。通过与数据库中的蛋白质序列进行比对和注释，可以确定微生物群落中存在的蛋白质，并进一步了解其功能、代谢通路和生物学特征。\n之前有一篇Nature Ecology \u0026amp; Evolution 文章中提到 (2)：\n群落构建似乎是基于功能基因而不是物种进行的\n功能冗余度是生态系统的关键性质。在生态系统中，不同物种通过表达冗余的功能发挥类似的生态作用。宏基因组数据已被用于人体微生物组功能潜力的冗余度（即基因组水平冗余度FR_g）定量。\n在这次会上，李老师提出了量化微生物组的蛋白质水平功能冗余度FR_p的方法，功能冗余度是一个高度综合性的指标，由功能是否表达、功能蛋白质丰度、功能的分类学来源多因素综合影响。在检测群落对环境因素响应方面，与目前广泛采用的多样性指标相比更为灵敏（目前好像还没找到发表的资料）。\n单细胞微生物组 一篇cell综述总结展望了单细胞微生物组技术 (3)： 总体分析步骤是样品单细胞分离-\u0026gt;培养/功能表征与筛选/测序/荧光\n微生物单细胞技术挑战：\n细胞小 环境：好氧性、竞争、污染 细胞聚集、结块 核酸量少 mRNA无ployA结构 单细胞分离培养技术（主要还是微流控/孔板法）：\n拉曼组 又听到了一个新概念和新技术。\n徐健老师团队提出，“拉曼组”（ramanome）作为一种单细胞精度代谢表型组，有三大内涵：\n每个拉曼组是在特定条件和时间点下一个细胞群体中采集的N个单细胞拉曼光谱的集合，体现的是该细胞群体的”单细胞代谢功能集体照”；\n每张单细胞拉曼光谱上超过1500个拉曼谱峰中，因每个峰或峰的组合均可能代表一个代谢表型（phenotype），故揭示了该细胞在该状态下的代谢表型组；\n一个拉曼组内部的不同单细胞拉曼光谱之间具有与生俱来的异质性，而且这种细胞间差异具有重要的生物学意义。\n已经证明，拉曼组能在单细胞精度，定量检测细胞利用含氢、含碳等底物的代谢速率、测定各种拉曼敏感产物（色素、甘油三酯、淀粉、蛋白等）之多样性及其含量、表征细胞的环境应激性（如微生物药敏、微生物药物应激机制、肿瘤药敏性与药物应激机制等）、检测细胞之间的代谢互作、重建细胞内代谢物相互转化网络（拉曼组内关联分析；IRCA）、也可区分不同的物种。拉曼组能够测量的代谢表型范围仍在不断拓展中。\n针对微生物组样品，运用整合了RAGE技术的RACS-Seq仪器，也实现了临床实际样品（包括尿液、胃粘膜等）或复杂环境（土壤、海水等）菌群中精确到一个细菌细胞的原位代谢功能识别及其对应高质量单细胞基因组的解析。\n单细胞测序 报告：高通量单细胞微生物基因组学 — Microbe-seq (chinagut.cn)\n单细胞测序的挑战：\n微生物群落组成复杂：将单个细胞分离 单菌起始DNA仅1个拷贝：全基因组扩增 背景噪音大：提高信噪比，例如缩小反应体积 群落细胞和物种多样：高通量分析 细胞壁裂解和其他流程不兼容：多步法反应 郑文山老师介绍的Micro-seq技术 (4)：一种基于液滴微流控的高通量单细胞微生物基因组测序技术，将该技术应用于人体肠道微生物样本中。通过对超过2万个单微生物进行测序，组装出了76个物种的基因组，并从10个物种中分辨出多个菌株并对其菌株水平基因组进行组装。在此基础上，构建了人体肠道微生物水平基因转移（HGT）网络并观察到92个不同物种间的HGT。\n非常酷的技术，也肯定是未来微生物组的研究趋势。\n微生物单细胞测序应用与发展：\n通过对28份水样，12715个微生物细胞进行单细胞测序：\n发现细胞与细胞间的基因变异非常常见，指出这对传统的宏基因组分箱分析方法来带巨大挑战。 阐述了C/N代谢、次级代谢合成途径在不可培养微生物重的进化。 构建了海洋浅表层丰富的微生物基因组数据库（GORG-Tropics）。 P. Ma, H. M. Amemiya, et.al, Bacterial droplet-based single-cell RNA-seq reveals antibiotic-associated heterogeneous cellular states. Cell. 186, 877-891.e14 (2023)\n开发了BacDrop，一种高度可扩展的细菌单细胞 RNA 测序技术，它克服了许多阻碍细菌 scRNA-seq 发展的挑战。\n通过对肺炎克雷伯菌在有/无抗生素扰动时的群落内异质性研究:\n发现种群内异质性主要是由促进抗生素耐药性的移动遗传原件的表达驱动。 细菌耐药性的进化可能来源由于高表达MGE基因的亚群 生信分析软件\u0026amp;方法 KMCP 这是沈伟/任红团队开发兼顾原核生物和病毒的宏基因组物种组成分析软件，文章发表在bioinformatics上 (5)，taxonkit这个好用的工具也是他们做的。\n壁报：KMCP: accurate metagenomic profiling of both prokaryotic and viral populations by pseudo-mapping-2023肠道大会 (chinagut.cn)\nKMCP 算法与流程 算法方面我不是特别懂：\nKMCP先将每个微生物基因组分割成n（默认10）等份，每份称为基因组区块（genome chunk）。每个基因组区块的所有k-mer用一个布隆过滤器（Bloom filter）保存，所有Bloom filter保存在一个改进的COBS索引中。\n在reads搜索的时候，将查询序列的所有k-mer去和数据库中所有的Bloom filter比较，返回具有一定共有k-mer数量的基因组区块。\n最终可以知道查询序列与哪些基因组的哪个基因组区块有较高相似度，这个过程称为pseudo-mapping。\n搜索完成后，匹配的参考基因组经过三轮过滤筛选（过滤条件包括k-mer相似性信息与基因组覆盖信息），最后再使用Expectation-Maximization（EM）算法估算匹配的参考基因组的相对丰度。\nKMCP 性能评估 优点：\n看起来建库非常方便（基因组序列库和分类库）。 当参考基因组由来自GTDB的原核生物和来自NCBI的真菌和病毒组成时，还可以合并GTDB和NCBI分类学数据。 KMCP搜索速度与基因组区块的数量线性相关，而且所有基因组区块的索引数据相互独立，用户可以将参考基因组分为多个较小的部分，每个分布分别创建数据库并用来搜索，时间可能长一些，但是内存占用少，时间换空间的策略。 KMCP 准确度评估 在种水平，KMCP优于其它工具：KMCP的Completeness（Recall,召回率，0.915）比Bracken（0.944）略低，比其他都高；而KMCP的Purity（Precision，精确度，0.830）比mOTUs2（0.887）和DUDes（0.871）低。在相对丰度上，KMCP的L1 norm error（越低越好）在属水平排在第二低，而在种水平最低，优于其它软件。\n目前我用的比较多的都是kraken，运行速度快，但是需要将整个数据库导入内存（还好我们的集群目前够大），未来很可能用上，先码住。\nFlex Meta-Storms 壁报：Flex Meta-Storms elucidates the microbiome local beta-diversity under specific phenotypes-2023肠道大会 (chinagut.cn)\nBeta多样性能够量化微生物群落之间的差异，从而揭示微生物组组成与环境特性或宿主表型之间的关联。Beta多样性分析所依赖的群落间的距离，目前均采用所有微生物特征来计算。\n然而，在某些情况下，群落中只有一小部分成员发挥着关键作用。这么小的比例不足以影响和改变整体距离。另一方面，当只关注其中一小部分微生物时，由于数据稀疏性，Beta多样性模式也会受到干扰。\n开发了Flex Meta-Storms (FMS) 距离算法，该算法首次实现了微生物组的”局部对齐”。 采用一种基于微生物加权系统发育和功能关系的特征”弹性提取”策略，FMS能够计算复杂微生物组之间，所关注的群落成员之间标准化的系统发育距离。我们展示了FMS 在使用人工和真实数据集检测不同状态下微生物组的细微变化方面的优势，而这些变化会被常规距离度量所忽略。因此，FMS能够以更高的灵敏度和灵活性有效区分不同状态下的微生物组，从而有助于深入理解微生物与宿主的相互作用，并促进微生物组数据在疾病筛查和预测等方面的利用。\n地址：qdu-bioinfo/flex-meta-storms: Flex Meta-Storms (FMS) can mine potential relationships generated by a small subset of microbes in community samples. (github.com)\n我之前以为感兴趣的直接提出来再做降维分析即可😂，看来想的太简单了\nDKI 刘洋彧老师现任哈佛大学医学院副教授，在美国东北大学物理系和复杂网络研究中心先后担任博士后，他实验室 (https://yangyuliu.bwh.harvard.edu) 目前的研究重点是从群落生态学，网络科学，控制论，和机器学习等多个角度研究复杂微生物群落，尤其关注人类微生物组的一系列根本性问题以及人类微生物组在疾病治疗和精准营养上的应用目前的研究重点是从群落生态学，网络科学，控制论，和机器学习等多个角度研究复杂微生物群落，尤其关注人类微生物组的一系列根本性问题以及人类微生物组在疾病治疗和精准营养上的应用。\n报告：在微生物群落中寻找基石物种 (chinagut.cn)\n微生物群落中含有基石物种，去除这些基石物种会导致微生物组结构和功能发生巨大变化。然而，我们仍然缺乏一种有效的方法来系统地识别微生物群落中的基石物种。\n我们之前经常在网络分析中见到这个概念 (6)，通过计算生态网络中每个物种的拓扑指标反映keystone。\n而这里刘老师提到，网络分析中的keystone有些问题：\nA recent numerical study claimed that those highly connected species (i.e., “hubs”) in the microbial correlation network are keystone species of microbial communities. We think this claim is problematic for at least two reasons.\n首先，微生物相关网络中的边并不代表直接的生态相互作用，而只是具有统计学意义的物种共现或相互排斥。 其次，物种迁移的影响自然取决于群落。这强调了基石物种识别的根本挑战——群落特异性，基于微生物相关网络中的枢纽（由一组微生物组样本构建）或生态网络中的任何拓扑指数（从实验数据推断出有针对性的种间相互作用）来识别关键物种完全忽略了这种群落特异性。\n刘洋彧老师提出一种基于深度学习和数据驱动的基石物种识别框架（DKI，Data-driven Keystone species Identification）。核心想法是用从某特定环境中收集的大量的微生物组样本来训练一个深度学习模型，从而隐式地学习该环境中微生物群落的组装规则。训练好的深度学习模型可以帮我们通过进行物种清除的假想实验来量化来自该环境中的任何微生物组样本中任何物种的基石性（keystoneness）。\n目前文章还是preprint (7)，主要的工作流程如下：\nDKI 工作原理 考虑一个特定的栖息地（或宏群落），包含不同微生物物种。假设我们从这个栖息地收集了大量微生物组样本。每个微生物组样本可以被视为栖息地的本地群落。样本的物种组合可以用二元向量表示，如果物种在某个样本存在（或不存在）。而该样本的微生物组成或分类学特征由组成向量表征，其中每个条目表示物种的相对丰度。假设收集的样本大致代表当地社区的稳定状态，以便它们可用于学习这些群落的构建规则。\n在第一阶段（图 1a），我们使用深度学习方法以训练数据隐式学习该栖息地微生物群落的组装规则。\n在第二阶段（图 1b），为了量化某一个物种i在当地群落或微生物组样本中的群落特异性基石性。我们可以这样考虑，一个是保留物种i的二元数据，使用cNODE2计算最终的群落组成，再删除i重新计算相对丰度得到零模型的群落组成。另一个则是直接删除i的二元数据，使用cNODE2计算最终的群落组成。\n然后便可以比较两者来量化移除物种i造成的影响：通过组成变化和功能变化来量化。具体的算法和公式可以阅读原文章了解。\nDKI vs co-network 这个DKI模型也在广义 Lotka-Volterra的模拟群落中进行了验证。发现DKI框架计算的keystoneness和真实的keystoneness相关性很高（左a~f），但是传统网络指标与真实的keystoneness相关性不强（右a~f）。\n随后作者在其他人体或环境栖息地如肠道，口腔，土壤宏基因组数据中运用DKI计算评估了微生物的keystoneness。\nMicrobiome SV 丁涛老师的报告是”从微生物组特征研究肠道肿瘤：更加精确的视角和方法“，但是他提到了基因组的结构变异对于细菌有重要的功能影响 (8)：\n目前看到的应用于宏基因组的结构变异分析还比较少，丁涛老师团队也开发了一个名为MicroSVsProfiler 的框架，但是目前暂未发表，找不到更多资料。\n已有的可以识别细菌基因组SVs的软件有SGVFinder：https://github.com/segalab/SGVFinder。\n有一篇Nature用二代测序专门在人体肠道微生物组做了这个 (9)，利用的是已有的基因组数据作为参考基因组，里面提到了 SVs 富含 CRISPR 相关和产生抗生素的功能的基因，而管家基因比较少，表明它们在微生物适应中发挥作用。\n然后又有一篇NC 结合二三代测序做了SV检测 (10)，通过利用 Oxford Nanopore Technology (ONT) 提供的长读长优势，鉴定肠道微生物组中结构变异 (SV) 的精细遗传变异，这里他们用的是MAG作为参考基因组。也提到了在 SV 中，前噬菌体和高度可变的 CRISPR 元件占很大比例。\n我觉得现在测序数据那么多，还是很值得进一步从基因组结构，进化角度进一步挖掘的。\n土壤与环境 在肠道大会听土壤介绍(有点神奇😂)，但有个老师说得好：土壤就像地球的肠道一样，集中了很多的微生物与功能基因，是一个资源宝库，所以也合理。\n这次大会上基本是跟农田相关的报告，而且大多是实验假设驱动的微生物研究，如退化农田，植物多样性，施肥，堆肥等实验条件对土壤表层微生物组或根际微生物组的影响。\n分析方法也较为传统：多样性、群落演替、群落稳定性（生态网络）等等，功能基因关注ARG，固氮潜能等，当然一般会做一些实验验证。\n有一个壁报倒是关注元素循环多一些：\nMetagenomic insights into the sustainability of the rice-crayfish co-culture model: from the perspective of microbial biogeochemical cycling-2023肠道大会 (chinagut.cn)\n研究的是稻虾共生模型（RCFP，rice-crayfish co-culture model），对水体、土壤、虾肠道都进行了微生物组测序，主要结果有：\n碳 (C) 循环 RCFP 在生境环境中具有更强的碳水化合物代谢能力，但在小龙虾肠道中没有。 氮 (N) 循环 RCFP 在N 同化和N 污染去除方面具有优势，而非RCFP 具有更高的N 分解和反硝化能力。 RCFP 具有较高的N同化和N污染物（如NO、N、O）去除能力，这与其独特的环境有关。 硫 (S) 循环 三个S 途径：同化硫酸盐还原(ASR)、(ASR)、异化硫酸盐还原(DSR) 和SOX 复合物对硫代硫酸盐的氧化。 S-cycling 基因分布在水产养殖模型中差异很大。RCFP 含有较少的 S 相关污染物（例如 H.S）。 在人类相关微生物组倒很少看到关注这些元素循环的，说不定也会有重要作用。\n总结 精力有限，这里先记录下这些。 除了精彩的报告，还有很多亮眼的posters和有趣的产业，出去参加一次大型学术会议对现在的我来说，不仅能够学到不少东西，还能开阔眼界，帮助想到一些新的ideas，看到这么多优秀的同行，自己还是要继续加油呀💪！\nReference 1. Z. Sun, S. Huang, P. Zhu, L. Tzehau, H. Zhao, J. Lv, R. Zhang, L. Zhou, Q. Niu, X. Wang, M. Zhang, G. Jing, Z. Bao, J. Liu, S. Wang, J. Xu, Species-resolved sequencing of low-biomass or degraded microbiomes using 2bRAD-M. Genome Biology. 23, 36 (2022).\n2. S. Louca, M. F. Polz, F. Mazel, M. B. N. Albright, J. A. Huber, M. I. O’Connor, M. Ackermann, A. S. Hahn, D. S. Srivastava, S. A. Crowe, M. Doebeli, L. W. Parfrey, Function and functional redundancy in microbial systems. Nature Ecology \u0026amp; Evolution. 2, 936–943 (2018).\n3. V. Lloréns-Rico, J. A. Simcock, G. R. B. Huys, J. Raes, Single-cell approaches in human microbiome research. Cell. 185, 2725–2738 (2022).\n4. W. Zheng, S. Zhao, Y. Yin, H. Zhang, D. M. Needham, E. D. Evans, C. L. Dai, P. J. Lu, E. J. Alm, D. A. Weitz, High-throughput, single-microbe genomics with strain resolution, applied to a human gut microbiome. Science. 376, eabm1483 (2022).\n5. W. Shen, H. Xiang, T. Huang, H. Tang, M. Peng, D. Cai, P. Hu, H. Ren, KMCP: Accurate metagenomic profiling of both prokaryotic and viral populations by pseudo-mapping. Bioinformatics. 39, btac845 (2023).\n6. S. Liu, H. Yu, Y. Yu, J. Huang, Z. Zhou, J. Zeng, P. Chen, F. Xiao, Z. He, Q. Yan, Ecological stability of microbial communities in Lake Donghu regulated by keystone taxa. Ecological Indicators. 136, 108695 (2022).\n7. X.-W. Wang, Z. Sun, H. Jia, S. Michel-Mata, M. T. Angulo, L. Dai, X. He, S. T. Weiss, Y.-Y. Liu, Identifying keystone species in microbial communities using deep learning (2023), p. 2023.03.15.532858.\n8. M. G. Durrant, A. S. Bhatt, Microbiome genome structure drives function. Nature Microbiology. 4, 912–913 (2019).\n9. D. Zeevi, T. Korem, A. Godneva, N. Bar, A. Kurilshikov, M. Lotan-Pompan, A. Weinberger, J. Fu, C. Wijmenga, A. Zhernakova, E. Segal, Structural variation in the gut microbiome associates with host health. Nature. 568, 43–48 (2019).\n10. L. Chen, N. Zhao, J. Cao, X. Liu, J. Xu, Y. Ma, Y. Yu, X. Zhang, W. Zhang, X. Guan, X. Yu, Z. Liu, Y. Fan, Y. Wang, F. Liang, D. Wang, L. Zhao, M. Song, J. Wang, Short- and long-read metagenomics expand individualized structural variations in gut microbiomes. Nature Communications. 13, 3175 (2022).\n","date":"2023-05-25T00:00:00Z","image":"/p/gut-2023/images/gut2023_hu66373a927f81782480fa2c0c3afe4d16_1071135_120x120_fill_q75_box_smart1.jpeg","permalink":"/p/gut-2023/","title":"gut 2023 肠道大会个人总结"},{"content":"","date":"2023-05-19T00:00:00Z","permalink":"/p/c-cycling/","title":"功能基因-元素循环（碳 C-cycling）"},{"content":" Introduction 抗生素 抗生素是一类药物，用于治疗或预防由细菌引起的感染。它们通过抑制细菌的生长或杀死细菌，帮助人体抵抗细菌感染。\nAn antibiotic is a type of antimicrobial substance active against bacteria.\n图1: Mechanisms of Antibiotics (YouTube (Antibiotics - Lecture 3)) 抗生素可以通过不同的机制对细菌产生作用。以下是几种常见的抗生素类型及其作用机制（图1）：\n青霉素类抗生素：作用于细菌细胞壁的合成，干扰其结构，导致细菌细胞壁破裂，细菌死亡。\n大环内酯类抗生素：通过阻断细菌蛋白质的合成过程，阻碍细菌的生长和复制。\n氨基糖苷类抗生素：抑制细菌蛋白质的合成，通过与细菌的核糖体结合，阻碍蛋白质合成的进行。\n四环素类抗生素：抑制细菌的蛋白质合成，通过与细菌的核糖体结合，阻碍蛋白质链的延伸。\n氟喹诺酮类抗生素：抑制细菌DNA的复制和转录过程，阻断细菌的生长。\n抗生素抗性 图2: Mechanisms of Antibiotic Resistance (Courtesy of E. Wistrand-Yuen.) 抗生素抗性是指细菌或其他微生物对抗生素的治疗效果降低或完全失效的现象。当细菌暴露在抗生素的作用下，一些细菌可能具有抗药性，能够存活和继续繁殖，导致感染的持续存在或进一步扩散。\nAntibiotic resistance is the ability of a microorganism to withstand the effects of antibiotic(s).\n抗生素抗性可以是天然的，即细菌天生具有抗药性。然而，更为严重的是通过基因突变或水平基因转移等机制，细菌可以获得抗药性。\n同样，为了对付抗生素的作用机制有多种耐药机制，CARD数据库 (1) 将其分为7类：\n导致抗生素耐药性的抗生素靶标的突变改变或酶促修饰。\n抗生素作用靶点的更换或替代，该过程会导致抗生素耐药性。\n保护抗生素作用靶点免受抗生素结合，该过程将导致抗生素耐药性。\n抗生素的酶促灭活赋予耐药性。\n通过将抗生素转运出细胞而产生的抗生素耐药性。\n通常通过减少孔蛋白的产生来降低对抗生素的渗透性，可以提供耐药性。\n基因缺失（通常是孔蛋白）赋予抗生素耐药性的机制。\n抗生素抗性基因（Antibiotics resistance genes，ARGs） 是指存在于细菌或其他微生物基因组中的基因，使它们具有对抗生素的抵抗能力。 这些基因编码了一系列的蛋白质或其他分子机制，使细菌能够对抗生素产生耐药性。\nARG被视为是一种新出现的生物污染物，由ARG引起的抗生素耐药性被认为是当代医学最重要的挑战之一，也是严重的公共卫生问题。抗生素耐药性阻碍了临床上对细菌感染的有效治疗，并成为流行病威胁和高死亡率的原因。\n根据获得的欧盟和欧洲经济活动国家的数据，抗生素抗性细菌（ARB）在2015年造成671,689人感染，每年导致超过33,000人死亡 (2)。\nARGs在环境中的广泛传播提高了其作为污染物的危害性。 抗生素在人类、兽医和农业用途中的大量使用导致它们不断释放到环境中，同时，ARG也被引入各种环境中，如临床（医院、诊所）和兽医环境、人体（胃肠道微生物组）以及废水处理厂和自然环境（土壤、水、空气）等。\n移动遗传元件（MGE）如质粒、转座子、整合子被证明与ARG的转移传播具有很强的相关性 (3)，将 ARG 与移动遗传元件联系起来是十分重要的。\nMethods \u0026amp; Tools 生物信息学在ARG研究上的主要目标是开发分析流程，用于准确检测抗性组（抗性基因的补充延伸）并随后根据基因组和宏基因组数据准确预测抗菌谱（AMR 的表型范围和易感性）以及ARG宿主等信息。\n目前已开发出很多的ARG相关的生物信息学软件工具（表1)和数据库（表2），注释管理不一致，每个工具和数据库都有不同的重点领域和不同的预测范围。\nTools 图3: 宏基因组测序数据ARG分析的工作流程 目前使用宏基因组测序数据对存在于微生物群落中的ARG进行识别鉴定的计算工作流程主要有两种：基于组装的contigs的分析和基于原始reads的比对分析（图3）。\n且大多数方法从宏基因组数据中获得的序列(原始reads或contigs)通常不能明确地确定其属于物种起源，分析的范围通常仅限于基于同源性就能识别的ARG家族，而缺乏高度依赖环境的抗性决定因素，如点突变的分析。\nTable 1: 用于鉴定、预测ARGs的生信软件 软件名 描述 ARG-ANNOT 使用 BLAST 针对抗菌素耐药性 (AMR) 参考序列和 SNP 的精选数据库检测细菌基因组中现有的和假定的新抗生素耐药性基因。 ARGs-OAP 用于从宏基因组序列中快速注释和分类抗生素抗性基因样序列（使用 BLASTX 针对 SARG 数据库）的在线管道。 BacAnt 允许同时注释 ARG、整合子和转座子的，可用于比较基因组分析的应用程序 DeepARG 通过深度学习方法，分别为短读序列和全基因长度序列构建了两个深度学习模型 DeepARG-SS 和 DeepARG-LS，提供了准确的抗菌素耐药性注释。 Mykrobe predictor 快速筛选金黄色葡萄球菌和结核分枝杆菌原始 FASTQ 测序结果中的 AMR 基因和 SNP，并对 12 种抗菌药物进行抗生素谱预测。 ResFinder 通过 BLAST 针对 AMR 参考序列的精选数据库，在全基因组数据中识别水平获得的 AMR 基因。 RGI 根据综合抗生素耐药性数据库 (CARD) 中的同源性和 SNP 模型，从蛋白质或核苷酸数据中预测耐药性组。 SEAR 使用 ARG-ANNOT 数据库检测基因组或宏基因组测序数据中水平获得的 AMR 基因。 SRST 一种快速测序读取映射工具，用于快速准确地检测 MLST、毒力、AMR 或其他标记。包括 ResFinder 和 ARG-ANNOT 参考数据库。 AMRFinder NCBI开发的一种使用高质量精选 AMR 基因参考数据库识别 AMR 基因的工具。 Database 近年来随着ARG数据的快速增长，数据管理、分析和访问需要更完善的数据库来承载。\nARG命名本身就是一个难题，同义词经常使用，名称冲突，基因名称有时基于核苷酸序列有时基于蛋白质序列，一些基因家族使用通用名称，而另一些则命名每个等位基因，加上各种ARG数据库非正式地交换有关 AMR 管理、命名和分类的信息，导致管理较为混乱。\n当然，没有一个ARG数据库是完整的，ARG在各种病原体间移动与突变会增加大量数据，突变数据的管理是最具挑战性的任务。\nTable 2: ARGs综合信息数据库 数据库 描述 ARDB 一个手动管理的数据库，其中每个基因和抗性类型都用抗性概况、作用机制、本体论和序列和蛋白质数据库的外部链接进行注释。自 2009 年以来未更新，所有数据均已整理到 CARD 中。 ARG-ANNOT AMR 参考序列和 SNP 的精选数据库。 CARD 手动管理的抗性基因和突变、其产物和相关表型数据库，涵盖 AMR 的所有机制。由抗生素耐药性本体论 (ARO) 组织管理。 CBMAR 提供对 β-内酰胺酶家族的分子和生化表征有用的信息。 MvirDB 通过整合来自多个来源的数据，专注于对生物防御应用至关重要的毒素、毒力因子和抗生素抗性基因。 NCBI BioProject PRJNA313047 以抗性为重点的 AMR 基因序列整理。 PATRIC 细菌感染性疾病信息系统，以ARDB和CARD为基础，辅以AMR元数据，对完整的病原体基因组进行注释。 Resfams 蛋白质家族和相关配置文件隐藏马尔可夫模型 (HMM) 的精选数据库，确认具有抗生素抗性功能并按本体组织。 ResFinder 水平获得的 AMR 基因数据库。 SARG 抗生素抗性基因、亚型和参考序列，整合来自ARDB和CARD的信息。 很多ARG数据库已经很久没有管理升级了，目前CARD是注释比较全面，管理完善，很多ARG条目经过实验验证的数据库。\nCARD提供参考 DNA 和蛋白质序列、检测模型和基于细菌抗菌素耐药性(AMR)分子基础的生物信息学工具，设计了抗生素抗性本体论 (ARO)。\nARO 分为三个主要分支：抗生素耐药性决定因素(ARO:3000000)、抗生素分子(ARO:1000003)和抗生素耐药机制(ARO:1000002)，最新的CARD现在已经有6000多个本体术语，这些还得到了很多研究论文的支持。\nCARD 开发了抗性基因标识符（RGI) 软件（v5.0）。\nRGI软件利用四种CARD模型类型来预测抵抗组：蛋白质同源模型（使用BLASTP或DIAMOND检测AMR基因的功能同源物）、蛋白质变异模型（用于准确区分易感内在基因和获得赋予AMR的突变的内在基因，基CARD的精选SNP矩阵）、rRNA突变模型（用于检测抗药性rRNA靶序列）和蛋白质过度表达模型（检测与AMR相关的外排亚基，但也突出显示存在时赋予过度表达的突变）。\n所以我一般用的也是RGI+CARD来做宏基因组的ARG鉴定。当然鉴定后的下游分析就跟其他的功能基因类似，有很多可以做的，最好跟实验设计紧紧联系说明问题。\nRGI RGI（Resistance Gene Identifier）是一个用于检测和注释细菌基因组中抗生素抗性基因的工具。\n地址：https://github.com/arpcard/rgi\nRGI有网页版应用https://card.mcmaster.ca/analyze/rgi，可以选择两种输入，右侧调节参数：\nEnter a GenBank accession(s): Upload FASTA sequence file(s): GenBank: JN420336.1，试试这个 Klebsiella pneumoniae plasmid pNDM-MAR, complete sequence，肺炎克雷伯菌质粒 pNDM-MAR，完整序列， 267242 bp。\n结果会返回一个表，多个旭日图： 当然我们要做宏基因组分析的话肯定要用命令行版本的rgi软件：\n安装方法：\n最简单的方式就是conda新建环境后直接安装：\n1 2 3 4 5 6 7 8 # searches rgi package and show available versions $ conda search --channel bioconda rgi # install rgi package $ conda install --channel bioconda rgi # install rgi specific version $ conda install --channel bioconda rgi=3.1.1 # remove rgi package $ conda remove --channel bioconda rgi 如果上述方法有问题的话（比如我当时的环境），可以考虑源码编译：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 # 克隆仓库， git clone https://github.com/arpcard/rgi # 在rgi文件夹内新建环境 conda env create -f conda_env.yml conda activate rgi # 在rgi文件夹内build python setup.py build python setup.py test python setup.py install #test cd tests pytest -v -rxs #这里可能会有几个问题，需要把card.json，和由card_database_v3.1.4.fasta创建的card_reference.fasta放在合适位置 #查看安装情况 rgi main -h #成功的话会输出帮助文件 usage: rgi main [-h] -i INPUT_SEQUENCE -o OUTPUT_FILE [-t {contig,protein}] [-a {DIAMOND,BLAST}] [-n THREADS] [--include_loose] [--include_nudge] [--local] [--clean] [--keep] [--debug] [--low_quality] [-d {wgs,plasmid,chromosome,NA}] [-v] [-g {PRODIGAL,PYRODIGAL}] [--split_prodigal_jobs] Resistance Gene Identifier - 6.0.2 - Main #下载card数据库 wget https://card.mcmaster.ca/latest/data tar -xvf data ./card.json #在本地或工作目录中加载card数据库 rgi load --card_json /path/to/card.json --local #查看本地数据库版本 rgi database --version --local #注意不加--local的话数据库会加载到全局环境中，在任意目录都可以运行 安装成功后，就可以来对我们的序列进行鉴定了，常用的有两种模式：\n基因组或组装序列（DNA/蛋白质） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 rgi main --input_sequence /path/to/nucleotide_input.fasta --output_file /path/to/output_file --local --clean 部分可选参数： -h, --help 显示此帮助信息并退出 -i 输入序列, --input_sequence 输入序列 输入文件必须在 FASTA（重叠群和蛋白质）或 gzip 格式！例如 myFile.fasta -o 输出文件, --output_file 输出文件，输出文件夹和文件名 -t {contig,protein}, --input_type {contig,protein} 指定数据输入类型（默认 = contig） -a {DIAMOND,BLAST}, --alignment_tool {DIAMOND,BLAST} 指定比对工具（默认 = BLAST） -n 线程，--num_threads 线程 BLAST 搜索中使用的线程数 (CPU)（默认值=16） --include_loose 除了严格和完美之外还包括宽松的命中点击（默认值：False） --include_nudge 包括从宽松到严格命中的命中（默认值：False） --local 使用本地数据库（默认：使用数据库可执行目录） --clean 删除临时文件（默认值：False） 结果表格每列的内容如下：\nField Content ORF_ID 开放阅读框架标识符（RGI 内部） Contig 源序列 Start ORF起始坐标 Stop ORF的结束坐标 Orientation ORF链 Cut_Off RGI 检测范式（完美、严格、松散） Pass_Bitscore 严格检测模型 bitscore 截断 Best_Hit_Bitscore 与 CARD 中的最高命中匹配的 Bitscore 值 Best_Hit_ARO CARD 中的最高命中匹配的 ARO 术语 Best_Identities 与 CARD 中的最高命中匹配的同一性百分比 ARO ARO 匹配到 CARD 中的最高命中的Accession号 Model_type CARD检测模型类型 SNPs_in_Best_Hit_ARO 在 CARD 中最高命中的 ARO 术语中观察到的突变 Other_SNPs 以模型 ID 指示的其他匹配项的 ARO 术语观察到的突变 Drug Class ARO分类 Resistance Mechanism ARO分类 AMR Gene Family ARO分类 Predicted_DNA ORF预测核苷酸序列 Predicted_Protein ORF预测的蛋白质序列 CARD_Protein_Sequence CARD中top hit的蛋白质序列 Percentage Length of Reference Sequence ORF蛋白长度/CARD参考蛋白长度 ID HSP 标识符（RGI 内部） Model_id CARD检测型号id Nudged TRUE = 命中从松散微调到严格 Note 其他注意事项的原因 宏基因组reads，基因组reads 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 rgi bwt --read_one /path/to/fastq/R1.fastq.gz --read_two /path/to/fastq/R2.fastq.gz --output_file output_prefix --local 部分可选参数： -h, --help 显示此帮助信息并退出 -1 READ_ONE, --read_one READ_ONE -2 READ_TWO，--read_two READ_TWO -a {kma,bowtie2,bwa}, --aligner {kma,bowtie2,bwa} 选择读取对齐器（默认=kma） -n 线程，--threads 线程 要使用的线程 (CPU) 数（默认值=16） -o 输出文件, --output_file 输出文件 输出文件名的名称 --debug 调试模式（默认=False） --clean 删除临时文件（默认=False） --local 使用本地数据库（默认：使用可执行目录中的数据库） --include_wildcard 包含wild数据库（默认=False） 最终结果有5个文件：\nFile Contents output_prefix.allele_mapping_data.txt RGI bwt read mapping results at allele level output_prefix.gene_mapping_data.txt RGI bwt read mapping results at gene level output_prefix.artifacts_mapping_stats.txt Statistics for read mapping artifacts output_prefix.overall_mapping_stats.txt Statistics for overall read mapping results output_prefix.reference_mapping_stats.txt Statistics for reference matches 常用的进一步分析的是 at allele level，这个表格每列的内容如下：\nField Contents Reference Sequence read映射到的参考等位基因 ARO Term ARO Term ARO Accession ARO Accession Reference Model Type CARD 检测模型类型 Reference DB 参考等位基因来自 CARD 或 WildCARD Reference Allele Source See below Resistomes \u0026amp; Variants: Observed in Genome(s) 这个等位基因序列是否在 CARD 患病率基因组序列中被观察到？ Resistomes \u0026amp; Variants: Observed in Plasmid(s) 是否已在 CARD Prevalence 质粒序列中观察到该等位基因序列？ Resistomes \u0026amp; Variants: Observed Pathogen(s) CARD 携带此等位基因序列的流行病原体。 如果 Reference DB 是 CARD，将显示在 CARD 检测模型中用作参考的病原体。 使用 k-mers 验证病原体来源。 Completely Mapped Reads 完全映射到等位基因的read数量 Mapped Reads with Flanking Sequence 未完全映射到等位基因的read数量 All Mapped Reads 前两列的总和 Percent Coverage 读数覆盖的参考等位基因百分比 Length Coverage (bp) Base pairs of reference allele covered by reads Average MAPQ (Completely Mapped Reads) 平均 MAPQ 值，映射质量得分量化了读取错位的可能性。 Heng Li 和 Richard Durbin 在他们描述 MAQ 的论文中介绍了它们，并且通常以 Phred 量表进行报告。 Mate Pair Linkage 对于配对双端测序，如果姐妹读取映射到不同的 AMR 基因，则会列出 Reference Length 参考等位基因的长度 (bp) AMR Gene Family ARO分类 Drug Class ARO分类 Resistance Mechanism ARO分类 Depth 覆盖深度（仅在使用 kma 时报告） SNPs 从映射读取中观察到的单核苷酸多态性（仅在使用 kma 时报告） Consensus Sequence DNA 使用映射读取的核苷酸一致序列（仅在使用 kma 时报告） Consensus Sequence Protein 从 DNA 翻译的蛋白质共有序列（仅在使用 kma 时报告） Reference 1. B. P. Alcock, A. R. Raphenya, T. T. Y. Lau, K. K. Tsang, et al., CARD 2020: Antibiotic resistome surveillance with the comprehensive antibiotic resistance database. Nucleic Acids Research. 48, D517–D525 (2020).\n2. A. Cassini, L. D. Högberg, D. Plachouras, A. Quattrocchi, et al., Attributable deaths and disability-adjusted life-years caused by infections with antibiotic-resistant bacteria in the EU and the European Economic Area in 2015: A population-level modelling analysis. The Lancet. Infectious Diseases. 19, 56–66 (2019).\n3. J. Wang, J. Gu, X. Wang, Z. Song, et al., Enhanced removal of antibiotic resistance genes and mobile genetic elements during swine manure composting inoculated with mature compost. Journal of Hazardous Materials. 411, 125135 (2021).\n","date":"2023-05-16T00:00:00Z","image":"/p/antibiotics-resistance/images/ARG_mechanism_hu3d756b490213c73e905feda3b49bca32_266021_120x120_fill_box_smart1_3.png","permalink":"/p/antibiotics-resistance/","title":"Antibiotics resistance gene 抗生素抗性基因（ARG）"},{"content":" Introduction 功能富集分析是一种用于分析基因集合或基因组数据中功能模式富集程度的计算方法。它可以帮助揭示在特定生物学背景下，哪些功能模块、代谢通路、基因家族等在统计上富集或显著过表示。\n功能富集分析通常包括以下步骤：\n数据预处理：根据研究问题和数据类型，选择适当的基因集合或基因组数据，例如基因表达数据、基因注释数据或基因列表。\n注释和功能分类：将基因集合与已知的功能注释数据库进行比较，例如基因本体论（Gene Ontology）、KEGG（Kyoto Encyclopedia of Genes and Genomes）通路数据库等。这一步将基因与特定的功能或生物过程相关联。\n统计分析：使用合适的统计方法，如超几何分布、Fisher’s exact test、GSEA（基因集富集分析）等，评估每个功能的富集程度。这些方法会计算一个得分或P值，用于判断功能是否在给定基因集合中富集。\n结果解释和可视化：根据统计分析的结果，识别在给定条件下显著富集的功能模块，并将结果进行解释和可视化。这可以帮助研究人员理解基因集合或基因组数据中的生物学特征和功能。\n功能富集分析可应用于多个研究领域，如基因表达分析、蛋白质组学、微生物组学等。它可以帮助研究人员理解基因集合的生物学意义，从而揭示生物过程、代谢通路、细胞组分等在特定条件下的调控机制，并为进一步的实验设计和研究提供有价值的指导。\n$$ P=1-\\sum_{i=0}^{m-1}\\frac{C_M^iC_{N-M}^{n-i}}{C_N^n} $$\n**P：**某pathway的富集显著性；**N：**注释上KEGG的所有基因的数量；**n：**所有显著差异的基因数量；**M：**所有基因中注释到某pathway的基因数量；**m：**所有差异基因中注释到某pathway的基因数量\nR函数phyper:\n1 2 1-phyper(k-1,m, N-m, n,) phyper(k-1,M, N-M, n, lower.tail=F) Table 1: Methods for microbial enrichment analysis. Method Type Notes Hypergeometric test / Fisher’ exact test algorithm The most common method used in enrichment analysis. There are many enrichment analysis platforms or software developed based on it, including DAVID, clusterprofile, etc. Gene set enrichment analysis (GSEA) software Gene Set Enrichment Analysis (GSEA) is a computational method that determines whether an a priori defined set of genes shows statistically significant, concordant differences between two biological states (e.g. phenotypes). Clusterprofiler R package ClusterProfiler automates the process of biological-term classification and the enrichment analysis of gene clusters, which calculates enrichment test for GO terms and KEGG pathways based on hypergeometric distribution. Reporter score algorithm The plus or minus sign of reporter score does not represent regulation direction. Reporter feature analysis algorithm Reporter feature can achieve enrichment ananlysis for non-directional, mixed-directional up/down-regulation, and distinct-directional up/down-regulation classes. Piano R package Piano is a R package that implements the Reporter Features algorithm. Piano performs gene set analysis using various statistical methods, from different gene level statistics and a wide range of gene-set collections. Furthermore, the Piano package contains functions for combining the results of multiple runs of gene set analyses. Reporter score是一种改良的微生物富集分析的新方法，此方法最初是为了揭示代谢网络中的转录调控模式而开发的，目前已被引入微生物研究中进行功能富集分析。\nMethod Reporter score算法最初由Patil和Nielsen于2005年开发，用于识别代谢调节热点的代谢物 (1)。\n应用于宏基因组分析，则是基于基因的KO（KEGG orthology，同源基因）注释，获得KO的差异信息，再”上升”至KEGG pathway的功能层面。主要步骤如下：\n使用Wilcoxon秩和检验获得两分组间每个KO差异显著性的P值（即$P_{koi}$，i代表某个KO）；\n采用逆正态分布，将每个KO的P值转化为Z值（$Z_{koi}$）,公式：$Z_{koi}=\\theta ^{-1}(1-P_{koi})$；\n将KO”上升”为pathway：$Z_{koi}$，计算通路的Z值,$Z_{pathway}=\\frac{1}{\\sqrt{k}}\\sum Z_{koi}$，其中k表示对应通路共注释到k个KO；\n评估显著程度：置换（permutation）1000次，获得$Z_{pathway}$的随机分布，公式：$Z_{adjustedpathway}=(Z_{pathway}-\\mu _k)/\\sigma _k$，$μ_k$为随机分布的均值，$σ_k$为随机分布的标准差。\n最终获得的$Z_{adjustedpathway}$，即为每条代谢通路富集的Reporter score值，Reporter score是非方向性的，Reporter score越大代表富集越显著，但不能指示通路的上下调信息。\nMisuse 最近有一篇文章就讨论了reporter-score的正负号误用问题 (2)：\nhttps://mp.weixin.qq.com/s?__biz=MzUzMjA4Njc1MA==\u0026mid=2247507105\u0026idx=1\u0026sn=d5a0f0aaf176e245de7976f0a48f87a8#rd\n主要结论是 reporter score算法（上述）是一种忽略通路中KOs上/下调节信息的富集方法，直接将reporter score的符号视为通路的调节方向是不正确的。\n但是我们应该可以将其改为能够考虑通路内KO上下调的方式，我称为directed 模式, 参考自https://github.com/wangpeng407/ReporterScore。\n具体步骤如下:\n使用Wilcoxon秩和检验或者t.test获得两分组间每个KO差异显著性的P值（即$P_{koi}$，i代表某个KO），再将P值除以2，即将(0,1]的范围变为(0,0.5]，$P_{koi}=P_{koi}/2$；\n采用逆正态分布，将每个KO的P值转化为Z值（$Z_{koi}$）,公式：$Z_{koi}=\\theta ^{-1}(1-P_{koi})$，由于上述P值小于0.5，则Z值将全部大于0；\n考虑每个KO是上调还是下调，计算$\\Delta KO_i$，\n$$ \\Delta KO_i=\\overline {KO_{i_{g1}}}-\\overline {KO_{i_{g2}}} $$\n其中，$\\overline {KO_{i_{g1}}}$ 是组1的$KO_i$ 的平均丰度,$\\overline {KO_{i_{g2}}}$ 是组2的$KO_i$ 的平均丰度，然后：\n$$ Z_{koi} = \\begin{cases} -Z_{koi}, \u0026amp; (\\Delta KO_i\u0026lt;0) \\ Z_{koi}, \u0026amp; (\\Delta KO_i \\ge 0) \\end{cases} $$\n这样的话$Z_{koi}$大于0为上调，$Z_{koi}$小于0为下调。\n将KO”上升”为pathway：$Z_{koi}$，计算通路的Z值,$Z_{pathway}=\\frac{1}{\\sqrt{k}}\\sum Z_{koi}$，其中k表示对应通路共注释到k个KO；\n评估显著程度：置换（permutation）1000次，获得$Z_{pathway}$的随机分布，公式：$Z_{adjustedpathway}=(Z_{pathway}-\\mu _k)/\\sigma _k$，$μ_k$为随机分布的均值，$σ_k$为随机分布的标准差。\n最终获得的$Z_{adjustedpathway}$，即为每条代谢通路富集的Reporter score值，在这种模式下，Reporter score是方向性的，更大的正值代表显著上调富集，更小的负值代表显著下调富集。\n但是这种方法的缺点是当一条通路显著上调KO和显著下调KO差不多时，最终的Reporter score绝对值可能会趋近0，成为没有被显著富集的通路。\nRpackage 因为我看目前没有现成的工具完成Reporter Score分析（除了一些云平台，但可能不太方便），所以我参考https://github.com/wangpeng407/ReporterScore 写了一个R包帮助分析（虽然也不是特别复杂）\n地址：https://github.com/Asa12138/ReporterScore\n安装方法：\n1 2 install.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#39;Asa12138/ReporterScore\u0026#39;,dependencies=T) 使用方法：\n1 2 3 4 5 6 7 library(ReporterScore) library(dplyr) library(ggplot2) #准备KO丰度表和实验metadata data(KO_test) head(KO_abundance) ## CG1 CG2 CG3 CG4 CG5 CG6 ## K03169 0.002653545 0.005096380 0.002033923 0.000722349 0.003468322 0.001483028 ## K07133 0.000308237 0.000280458 0.000596527 0.000859854 0.000308719 0.000878098 ## K03088 0.002147068 0.002030742 0.003797459 0.004161979 0.002076596 0.003091182 ## CG7 CG8 CG9 CG10 CG11 CG12 ## K03169 0.002261685 0.004114644 0.002494258 0.002793671 0.004053729 0.002437170 ## K07133 0.000525566 0.000356138 0.000445409 0.000268306 0.000293546 0.000465780 ## K03088 0.003098506 0.002558730 0.002896506 0.002618472 0.002367986 0.002082786 ## CG13 CG14 CG15 EG1 EG2 EG3 ## K03169 0.002187500 0.001988374 0.002304885 0.003317368 0.001150671 0.002610814 ## K07133 0.000507992 0.000409447 0.000327910 0.002916018 0.004820742 0.001973789 ## K03088 0.002680792 0.003066870 0.002975895 0.002257401 0.002889640 0.001997586 ## EG4 EG5 EG6 EG7 EG8 EG9 ## K03169 0.000900673 0.001545374 0.001640295 0.001445024 0.001096728 0.001026556 ## K07133 0.003359927 0.001913932 0.001384079 0.001321643 0.002376473 0.004391014 ## K03088 0.002613441 0.002803388 0.002251835 0.002981244 0.002944061 0.003113215 ## EG10 EG11 EG12 EG13 EG14 EG15 ## K03169 0.001513195 0.001812732 0.003256782 0.006723067 0.001769819 0.001233307 ## K07133 0.002479040 0.003484868 0.000790457 0.000127818 0.000634529 0.004746572 ## K03088 0.003177522 0.002790092 0.001607913 0.002574928 0.001662157 0.002614489 ## [ reached 'max' / getOption(\u0026quot;max.print\u0026quot;) -- omitted 3 rows ] 1 head(Group_tab) ## Group ## CG1 CG ## CG2 CG ## CG3 CG ## CG4 CG ## CG5 CG ## CG6 CG 分组检验获得P值,threads多线程可加速 1 ko_pvalue=ko_test(KO_abundance,\u0026#34;Group\u0026#34;,Group_tab,threads = 1,verbose = F) ## Compared groups: CG and EG ## Total KO number: 4535 ## Time use: 1.348 1 head(ko_pvalue) ## KO_id avg_CG sd_CG avg_EG sd_EG diff_mean ## 1 K03169 0.0026728975 0.0011094132 0.0020694937 0.0014922004 0.00060340387 ## 2 K07133 0.0004554658 0.0001951678 0.0024480601 0.0014916566 -0.00199259427 ## 3 K03088 0.0027767713 0.0006253559 0.0025519275 0.0004966801 0.00022484380 ## 4 K03530 0.0005779169 0.0008952163 0.0005197504 0.0001435245 0.00005816647 ## 5 K06147 0.0020807307 0.0007731661 0.0014321838 0.0004273716 0.00064854693 ## 6 K05349 0.0021064422 0.0005243558 0.0017419317 0.0005382770 0.00036451047 ## p.value ## 1 0.03671754164 ## 2 0.00002654761 ## 3 0.48636476395 ## 4 0.01125600770 ## 5 0.00987482265 ## 6 0.12614740102 2.将P值矫正并转为Z-Score，这里提供两种方法（mixed就是经典的方法，另一种是directed方法）\n1 ko_stat=pvalue2zs(ko_pvalue,mode=\u0026#34;directed\u0026#34;) ## ================================================================================ ## ## Chi-squared test for given probabilities ## ## data: up_down_ratio ## X-squared = 21.823, df = 1, p-value = 0.000002991 1 head(ko_stat) ## KO_id avg_CG sd_CG avg_EG sd_EG diff_mean ## 1 K03169 0.0026728975 0.0011094132 0.0020694937 0.0014922004 0.00060340387 ## 2 K07133 0.0004554658 0.0001951678 0.0024480601 0.0014916566 -0.00199259427 ## 3 K03088 0.0027767713 0.0006253559 0.0025519275 0.0004966801 0.00022484380 ## 4 K03530 0.0005779169 0.0008952163 0.0005197504 0.0001435245 0.00005816647 ## 5 K06147 0.0020807307 0.0007731661 0.0014321838 0.0004273716 0.00064854693 ## 6 K05349 0.0021064422 0.0005243558 0.0017419317 0.0005382770 0.00036451047 ## p.value sign type q.value Z_score ## 1 0.01835877082 1 Enriched 0.0669268695 1.4990767 ## 2 0.00001327381 -1 Depleted 0.0004777517 -3.3033110 ## 3 0.24318238198 1 Enriched 0.3058325297 0.5076981 ## 4 0.00562800385 1 Enriched 0.0304570375 1.8741186 ## 5 0.00493741133 1 Enriched 0.0277461715 1.9150013 ## 6 0.06307370051 1 Enriched 0.1607865272 0.9912305 3.将KO”上升”为pathway，计算ReporterScore：\n1 reporter_s=get_reporter_score(ko_stat) ## =================================Checking file================================= ## ==================================load KOlist================================== ## ===================KOlist download time: 2023-05-12 00:07:41=================== ## ============================Calculating each pathway============================ ## ID number: 479 ## Time use: 4.641 1 head(reporter_s) ## ID ReporterScore Description K_num ## 1 map00010 -1.6255358 Glycolysis / Gluconeogenesis 106 ## 2 map00020 -1.8906022 Citrate cycle (TCA cycle) 67 ## 3 map00030 -0.8261448 Pentose phosphate pathway 88 ## 4 map00040 -0.6685755 Pentose and glucuronate interconversions 89 ## 5 map00051 -2.2494558 Fructose and mannose metabolism 112 ## 6 map00052 -0.4196662 Galactose metabolism 78 结果进行绘图 1 2 plot_report(reporter_s,rs_threshold=c(2,-7),y_text_size=10,str_width=40)+ labs(title = \u0026#34;CG vs EG\u0026#34;) 1 2 plot_report(reporter_s,rs_threshold=c(2,-7),mode = 2,y_text_size=10,str_width=40)+ labs(title = \u0026#34;CG vs EG\u0026#34;) 5.挑选一条通路进行绘制\n1 plot_KOs_in_pathway(map_id = \u0026#34;map00780\u0026#34;,ko_stat = ko_stat) Reference 1. K. R. Patil, J. Nielsen, Uncovering transcriptional regulation of metabolism by using metabolic network topology. Proceedings of the National Academy of Sciences of the United States of America. 102, 2685–2689 (2005).\n2. L. Liu, R. Zhu, D. Wu, Misuse of reporter score in microbial enrichment analysis. iMeta. n/a, e95.\n","date":"2023-05-10T00:00:00Z","image":"/p/reporter-score/images/misuse2_hu0533df8c600b6ebf5e775c57536fef00_337235_120x120_fill_q75_box_smart1.jpeg","permalink":"/p/reporter-score/","title":"Reporter Score 微生物功能富集分析"},{"content":"上次已经介绍过了使用R绘制系统发育树的基本用法，也埋下一个小坑：复现文章里好看的树，现在过来填坑了，哈哈哈。\n我准备了一些文章里自己觉得很不错的树（当然尽可能风格不同），然后自己生成随机的树和一些随机的无科学意义的注释（仅供画图参考！！！），主要是用ggtree和ggtreeExtra进行代码复现，争取把树的主体都用代码完成，当然一些小细节就还是不得不使用AI等pdf编辑软件进行添加了。\nPreparation 首先还是要把我们要用的一些包都安装好并导入进来。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #tree if (!requireNamespace(\u0026#34;BiocManager\u0026#34;, quietly=TRUE)) install.packages(\u0026#34;BiocManager\u0026#34;) BiocManager::install(\u0026#34;YuLab-SMU/treedataverse\u0026#34;) BiocManager::install(\u0026#34;ggtree\u0026#34;) BiocManager::install(\u0026#34;ggtreeExtra\u0026#34;) library(treedataverse) library(ggtree) library(ggtreeExtra) library(treeio) #plot library(dplyr) library(ggplot2) library(ggnewscale) library(reshape2) library(ggrepel) 然后需要准备一些函数：\n1 2 3 4 5 6 7 #写个函数类似child获取子节点，但是可以指定层数 child2=function(tree,node,depth=1){ if(depth==1){return(child(tree,node))} else { return(child2(tree,child(tree,node)%\u0026gt;%unlist()%\u0026gt;%unname(),depth-1)%\u0026gt;%unlist()%\u0026gt;%unname()) } } Example1 第一个例子来自Nature Communication的一篇文章 (1)，这是一个相对简单的树。\n按照ggplot搭积木的逻辑，我们看看有哪些需要画的：\n树的主体，圆形布局，并打开一个小角度，方便展示注释信息的x轴label 外圈注释1，热图形式（tile），颜色代表每一个tip的Phylum，透明度代表相对丰度 外圈注释2，柱形图形式（col或bar），颜色代表每一个tip的Phylum，高度代表SVM系数 相应的我们生成数据：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #生成一个2000tip的树 ntips=2000 set.seed(123) tree=rtree(ntips,rooted = T) #变成tbl_tree对象，方便操作 ggtree::fortify(tree)-\u0026gt;tree_df #生成一些假的taxonomy信息 phylums=c(\u0026#34;Arthropoda\u0026#34;,\u0026#34;Streptophyta\u0026#34;,\u0026#34;Cyanobacteria\u0026#34;,\u0026#34;Acidobacteriota\u0026#34;,\u0026#34;Bacteroidetes\u0026#34;,\u0026#34;Firmicutes\u0026#34;,\u0026#34;Actinobacteria\u0026#34;,\u0026#34;Proteobacteria\u0026#34;,\u0026#34;Others\u0026#34;) phy_nodes=child2(tree,ntips+1,depth = 4) set.seed(123) phy_nodes=setNames(phy_nodes,sample(phylums,length(phy_nodes),replace = T)) tree_df=groupClade(tree_df,phy_nodes,group_name = \u0026#34;Phylum\u0026#34;) anno=filter(tree_df,isTip==\u0026#34;TRUE\u0026#34;)%\u0026gt;%select(label,Phylum) #生成随机数作为丰度和SVM anno=cbind(anno,sapply(1:4, \\(i)rexp(ntips,3))%\u0026gt;%vegan::decostand(.,\u0026#34;total\u0026#34;,2)) anno=cbind(anno,rexp(ntips)/10) colnames(anno)=c(\u0026#34;node\u0026#34;,\u0026#34;Phylum\u0026#34;,\u0026#34;Ice/Snow\u0026#34;,\u0026#34;Terrestrial\u0026#34;,\u0026#34;Marine\u0026#34;,\u0026#34;Freshwater\u0026#34;,\u0026#34;SVM\u0026#34;) head(anno) ## node Phylum Ice/Snow Terrestrial Marine Freshwater ## 1 t339 Cyanobacteria 0.0007693819 0.000448952905 0.00003100616 0.00034017986 ## 2 t1180 Cyanobacteria 0.0002356377 0.000483643521 0.00124672220 0.00009722752 ## 3 t1807 Cyanobacteria 0.0002908480 0.000035084895 0.00074445757 0.00011096549 ## 4 t572 Cyanobacteria 0.0019889166 0.000108407239 0.00098076293 0.00014436096 ## 5 t1739 Cyanobacteria 0.0004149838 0.000004413762 0.00006021236 0.00006270886 ## 6 t1245 Cyanobacteria 0.0004753852 0.000004451763 0.00015540813 0.00011400371 ## SVM ## 1 0.0001332960 ## 2 0.0474528998 ## 3 0.0222924708 ## 4 0.0001208082 ## 5 0.0169753369 ## 6 0.0277805382 有了树和注释数据，我们开始绘图：\n1 2 3 4 # 1. 树的主体,树枝太多把size调小，圆形布局，并打开一个小角度 p=ggtree(tree,size=0.1,layout = \u0026#34;fan\u0026#34;,open.angle = 10) #(p=ggtree(tree_df,aes(color=Phylum),size=0.1,layout = \u0026#34;fan\u0026#34;,open.angle = 20)) p 1 2 3 # 2. 外圈注释1，热图形式（tile），颜色代表每一个tip的Phylum，透明度代表相对丰度 anno1=melt(anno[,1:6],id.vars =1:2,variable.name = \u0026#34;Env\u0026#34;,value.name = \u0026#34;Abundance\u0026#34;) head(anno1) ## node Phylum Env Abundance ## 1 t339 Cyanobacteria Ice/Snow 0.0007693819 ## 2 t1180 Cyanobacteria Ice/Snow 0.0002356377 ## 3 t1807 Cyanobacteria Ice/Snow 0.0002908480 ## 4 t572 Cyanobacteria Ice/Snow 0.0019889166 ## 5 t1739 Cyanobacteria Ice/Snow 0.0004149838 ## 6 t1245 Cyanobacteria Ice/Snow 0.0004753852 1 2 3 4 5 6 7 8 9 p1=p+geom_fruit( data=anno1, geom = geom_tile, mapping = aes(y=node,x=Env,fill=Phylum,alpha=Abundance), pwidth = 0.2, axis.params=list(axis=\u0026#34;x\u0026#34;,text.size = 2,text.angle=270) )+scale_alpha(range = c(0,1),guide=guide_none())+ ggsci::scale_fill_npg() p1 1 2 3 4 5 6 7 8 9 10 # 3. 外圈注释2，柱形图形式（col或bar），颜色代表每一个tip的Phylum，高度代表SVM系数 p2=p1+geom_fruit( data=anno, geom = geom_col, mapping = aes(y=node,x=SVM,fill=Phylum), pwidth = 0.3, axis.params=list(axis=\u0026#34;x\u0026#34;,text.size = 2), grid.params = list() )+theme(legend.position = c(0,0.3)) p2 Example2 第二个例子来自Nature Microbiology的一篇文章 (2)。\n我们看看有哪些需要画的：\n树的主体，比较特别的布局（equal_angle），并且树枝要加上一些Form的分类颜色信息,再加上一个scale标尺 外圈注释1，标签，在每类分支附近，背景颜色是Form的分类 外圈注释2，点和文字，应该是手动挑选的一些节点，在树枝顶端加上了灰点以及黑色文字 相应的我们生成数据：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #生成一个400tip的树 ntips=400 set.seed(123) tree=rtree(ntips,rooted = T) #变成tbl_tree对象，方便操作 ggtree::fortify(tree,layout =\u0026#34;equal_angle\u0026#34; )-\u0026gt;tree_df #生成一些假的Form信息 forms=paste0(\u0026#34;Form \u0026#34;,c(\u0026#34;I\u0026#34;,\u0026#34;II\u0026#34;,\u0026#34;II/III\u0026#34;,\u0026#34;III-a\u0026#34;,\u0026#34;III-b\u0026#34;,\u0026#34;III-c\u0026#34;,\u0026#34;III-like\u0026#34;,\u0026#34;IV\u0026#34;)) phy_nodes=child2(tree,ntips+1,depth = 3) set.seed(123) phy_nodes=setNames(phy_nodes,sample(forms,length(phy_nodes),replace = F)) tree_df=groupClade(tree_df,phy_nodes,group_name = \u0026#34;Form\u0026#34;) #指定颜色 colors=c(\u0026#34;#1F77B4FF\u0026#34;,\u0026#34;#FF7F0EFF\u0026#34;,\u0026#34;#2CA02CFF\u0026#34;,\u0026#34;#D62728FF\u0026#34;,\u0026#34;#9467BDFF\u0026#34;,\u0026#34;#8C564BFF\u0026#34;,\u0026#34;#E377C2FF\u0026#34;,\u0026#34;#BCBD22FF\u0026#34;,\u0026#34;#17BECFFF\u0026#34;) #挑选一些nodes set.seed(123) label_node=sample(seq_len(ntips),20) 有了树和注释数据，我们开始绘图：\n1 2 3 4 5 6 # 1. 树的主体，比较特别的布局（equal_angle），并且树枝要加上一些Form的分类颜色信息 p=ggtree(tree_df,aes(color=Form),layout = \u0026#34;equal_angle\u0026#34;)+ geom_treescale(-5,7,fontsize=3, linesize=0.5,width=1)+ scale_color_manual(values = c(\u0026#34;black\u0026#34;,colors))+ coord_flip()+theme(legend.position = \u0026#34;none\u0026#34;) p 1 2 3 4 5 # 2. 外圈注释1，标签，在每类分支附近，背景颜色是Form的分类 p1=p+geom_label_repel(data = subset(tree_df,node%in%phy_nodes), mapping = aes(x=x,y=y,label=Form,fill=Form),color=\u0026#34;black\u0026#34;,alpha=0.7)+ scale_fill_manual(values = colors) p1 1 2 3 4 5 6 # 3. 外圈注释2，点和文字，应该是手动挑选的一些节点，在树枝顶端加上了灰点以及黑色文字 p2=p1+geom_point(data = subset(tree_df,node%in%label_node), mapping = aes(x=x*1.03,y=y*1.03),color=\u0026#34;grey50\u0026#34;)+ geom_text_repel(data = subset(tree_df,node%in%label_node), mapping = aes(x=x*1.05,y=y*1.05,label=label),color=\u0026#34;black\u0026#34;) p2 当然文字和标签的位置有点不太好，需要导出pdf再稍微调整一下。\nExample3 第三个例子来自Nature Biotechnology的一篇文章 (3) 。\n我们看看有哪些需要画的：\n树的主体，层级树的感觉（把branch.length忽略了，所有的tip在一个位置），打开角度为180，灰色树枝 内圈注释，给部分clade加上不同Phylum的背景颜色 外圈注释1，3圈热图，用的是有无数据 外圈注释2，2圈柱形图，Size和GC含量 相应的我们生成数据：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #生成一个1000tip的树 ntips=1000 set.seed(123) tree=rtree(ntips,rooted = T) #变成tbl_tree对象，方便操作 ggtree::fortify(tree)-\u0026gt;tree_df #生成一些假的taxonomy信息 phylums=rev(c(\u0026#34;Arthropoda\u0026#34;,\u0026#34;Streptophyta\u0026#34;,\u0026#34;Cyanobacteria\u0026#34;,\u0026#34;Acidobacteriota\u0026#34;,\u0026#34;Bacteroidetes\u0026#34;,\u0026#34;Firmicutes\u0026#34;,\u0026#34;Actinobacteria\u0026#34;,\u0026#34;Proteobacteria\u0026#34;)) #把8个phylum赋给最多tip的几个clade，其他的是others phy_nodes=child2(tree,ntips+1,depth = 4) phy_nodes_tips=sapply(phy_nodes, \\(i)nrow(offspring(tree_df,i))) names(phy_nodes)=rep(\u0026#34;Others\u0026#34;,length(phy_nodes)) names(phy_nodes)[which(phy_nodes_tips%in%tail(sort(phy_nodes_tips),8))]=phylums tree_df=groupClade(tree_df,phy_nodes,group_name = \u0026#34;Phylum1\u0026#34;) anno=filter(tree_df,isTip==\u0026#34;TRUE\u0026#34;)%\u0026gt;%select(label,Phylum1) #添加每个phylum的个数和百分比 anno%\u0026gt;%count(Phylum1)%\u0026gt;%mutate(per=100*n/sum(n))%\u0026gt;% mutate(Phylum=paste0(Phylum1,\u0026#34; (\u0026#34;,n,\u0026#34;, \u0026#34;,per,\u0026#34;%)\u0026#34;))%\u0026gt;%select(Phylum1,Phylum)-\u0026gt;in_anno in_anno=right_join(in_anno,data.frame(node=phy_nodes,Phylum1=names(phy_nodes))) set.seed(123) #生成随机变量作为Source,16S rRNA presence,Newly identified anno$Source=sample(c(\u0026#34;Cultivated\u0026#34;,\u0026#34;MAG\u0026#34;),ntips,T,c(0.1,0.9)) anno$`16S rRNA presence`=sample(c(\u0026#34;Yes\u0026#34;,\u0026#34;No\u0026#34;),ntips,T,c(0.3,0.7)) anno$`Newly identified`=sample(c(\u0026#34;Yes\u0026#34;,\u0026#34;No\u0026#34;),ntips,T,c(0.9,0.1)) #生成随机数作为Size和GC含量 anno$`Size (Mb)`=10-rpois(ntips,2) anno$`GC (%)`=runif(ntips,30,80) colnames(anno)[1]=\u0026#34;node\u0026#34; head(anno) ## # A tibble: 6 × 7 ## node Phylum1 Source `16S rRNA presence` Newly identifie…¹ Size …² GC (%…³ ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 t876 Others MAG No Yes 9 45.2 ## 2 t896 Others MAG No Yes 6 71.6 ## 3 t437 Others MAG No Yes 9 59.7 ## 4 t750 Others MAG Yes Yes 8 70.4 ## 5 t270 Others Cultivated Yes Yes 9 44.7 ## 6 t412 Others MAG No Yes 8 37.1 ## # … with abbreviated variable names ¹​`Newly identified`, ²​`Size (Mb)`, ## # ³​`GC (%)` 1 2 3 # 1. 树的主体，层级树的感觉（把branch.length忽略了，所有的tip在一个位置），打开角度为180，灰色树枝 p=ggtree(tree,layout = \u0026#34;fan\u0026#34;,open.angle = 180,branch.length = \u0026#34;none\u0026#34;,size=0.2,color=\u0026#34;grey\u0026#34;) p 1 2 3 4 5 # 2. 内圈注释，给部分clade加上不同Phylum的背景颜色 p1=p+geom_highlight(data = in_anno, mapping = aes(node=node,fill=Phylum),to.bottom = T,alpha=1)+ ggsci::scale_fill_rickandmorty(guide=guide_legend(ncol = 2,title.position = \u0026#34;top\u0026#34;,title = \u0026#34;Clade: Phylum\u0026#34;,order = 4)) p1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # 3. 外圈注释1，3圈热图，用的是有无数据 p2=p1+ggnewscale::new_scale_fill()+ geom_fruit( data=anno, geom = geom_tile, mapping = aes(y=node,fill=`Newly identified`), pwidth = 0.05,offset = 0.05,color=NA )+scale_fill_manual(values = c(\u0026#34;white\u0026#34;,\u0026#34;red\u0026#34;), guide=guide_legend(ncol = 2,title.position = \u0026#34;top\u0026#34;,override.aes = list(color=\u0026#34;red\u0026#34;,size=3),order = 3))+ ggnewscale::new_scale_fill()+ geom_fruit( data=anno, geom = geom_tile, mapping = aes(y=node,fill=`16S rRNA presence`), pwidth = 0.05,offset = 0.1,color=NA )+scale_fill_manual(values = c(\u0026#34;white\u0026#34;,\u0026#34;blue4\u0026#34;), guide=guide_legend(ncol = 2,title.position = \u0026#34;top\u0026#34;,override.aes = list(color=\u0026#34;blue4\u0026#34;,size=3),order = 2))+ ggnewscale::new_scale_fill()+ geom_fruit( data=anno, geom = geom_tile, mapping = aes(y=node,fill=Source), pwidth = 0.05,offset = 0.1,color=NA )+scale_fill_manual(values = c(\u0026#34;green4\u0026#34;,\u0026#34;white\u0026#34;), guide=guide_legend(ncol = 2,title.position = \u0026#34;top\u0026#34;,override.aes = list(color=\u0026#34;green4\u0026#34;,size=3),order = 1)) p2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 4. 外圈注释2，2圈柱形图，Size和GC含量 p3=p2+geom_fruit( data=anno, geom = geom_col, mapping = aes(y=node,x=`Size (Mb)`),fill=\u0026#34;purple3\u0026#34;, pwidth = 0.15,offset = 0.1, axis.params=list(axis=\u0026#34;x\u0026#34;,text.size = 2,text.angle=90,hjust=1,nbreak=3,line.color=\u0026#34;black\u0026#34;), grid.params = NULL )+geom_fruit( data=anno, geom = geom_col, mapping = aes(y=node,x=`GC (%)`),fill=\u0026#34;#F7C194\u0026#34;, pwidth = 0.2,offset = 0.05, axis.params=list(axis=\u0026#34;x\u0026#34;,text.size = 2,text.angle=90,hjust=1,nbreak=2,line.color=\u0026#34;black\u0026#34;), grid.params = NULL )+theme(legend.position = c(0.5,0.3),legend.box = \u0026#34;horizontal\u0026#34;, legend.text = element_text(size=8)) #最后再加上几个标签 p3+geom_text(data = data.frame(x=c(20,22,24,27,31),y=c(10), label=c(\u0026#34;Source \u0026#34;,\u0026#34;16S rRNA presence \u0026#34;,\u0026#34;Newly identified \u0026#34;,\u0026#34;Size (Mb) \u0026#34;,\u0026#34;GC (%) \u0026#34;)), aes(x,y,label=label),angle=90,hjust=1,size=3) Example4 第四个例子来自Nature的一篇文章 (4)。这个图是用iTOL做的，因为iTOL支持直接画tip到圆等半径的空间颜色填充。但是我觉得用R还是一样能画。\n我们看看有哪些需要画的：\n树的主体，很正常，打开小角度，开口在左上角 内圈注释，给部分clade加上不同Phylum的颜色，但是这个色块是加在tip到圆等半径的空间（这个很有意思，还没有看到过别人用R实现过） 外圈注释1，方块代表phage，颜色代表family 外圈注释2，灰色五角星代表Genome contains Thoeris 外圈注释3，绿色菱形加上文字 相应的我们生成数据：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 #生成一个200tip的树 ntips=200 set.seed(123) tree=rtree(ntips,rooted = T) #变成tbl_tree对象，方便操作 ggtree::fortify(tree)-\u0026gt;tree_df #生成一些假的taxonomy信息 phylums=rev(c(\u0026#34;Arthropoda\u0026#34;,\u0026#34;Streptophyta\u0026#34;,\u0026#34;Cyanobacteria\u0026#34;,\u0026#34;Acidobacteriota\u0026#34;,\u0026#34;Bacteroidetes\u0026#34;,\u0026#34;Firmicutes\u0026#34;,\u0026#34;Actinobacteria\u0026#34;,\u0026#34;Proteobacteria\u0026#34;)) #把8个phylum赋给最多tip的几个clade，其他的是others phy_nodes=child2(tree,ntips+1,depth = 4) phy_nodes_tips=sapply(phy_nodes, \\(i)nrow(offspring(tree_df,i))) names(phy_nodes)=rep(\u0026#34;Unknown\u0026#34;,length(phy_nodes)) names(phy_nodes)[which(phy_nodes_tips%in%tail(sort(phy_nodes_tips),8))]=phylums tree_df=groupClade(tree_df,phy_nodes,group_name = \u0026#34;Phylum\u0026#34;) anno=filter(tree_df,isTip==\u0026#34;TRUE\u0026#34;)%\u0026gt;%select(label,Phylum)%\u0026gt;%rename(node=\u0026#34;label\u0026#34;) colors=c(\u0026#34;Firmicutes\u0026#34;=\u0026#34;#d3edeb\u0026#34;,\u0026#34;Actinobacteria\u0026#34;=\u0026#34;#019a99\u0026#34;,\u0026#34;Bacteroidetes\u0026#34;=\u0026#34;#0077b0\u0026#34;, \u0026#34;Proteobacteria\u0026#34;=\u0026#34;#ffba4d\u0026#34;,\u0026#34;Acidobacteriota\u0026#34;=\u0026#34;#282152\u0026#34;,\u0026#34;Cyanobacteria\u0026#34;=\u0026#34;#caa59a\u0026#34;, \u0026#34;Streptophyta\u0026#34;=\u0026#34;#ff7880\u0026#34;,\u0026#34;Arthropoda\u0026#34;=\u0026#34;#aac8eb\u0026#34;,Unknown=\u0026#34;white\u0026#34;) set.seed(123) #生成随机变量作为Source,16S rRNA presence,Newly identified anno$`Phage family`=anno$`Thoeris`=anno$`bac`=NA anno$`Phage family`[sample(seq_len(ntips),30)]=sample(c(\u0026#34;Myoviridae\u0026#34;,\u0026#34;Podoviridae\u0026#34;,\u0026#34;Siphoviridae\u0026#34;),30,replace = T,prob = c(0.2,0.1,0.7)) anno$`Thoeris`[sample(seq_len(ntips),5)]=\u0026#34;Genome contains Thoeris\u0026#34; anno$bac[sample(seq_len(ntips),10)]=\u0026#34;Bac\u0026#34; 1 2 3 4 5 6 7 # 1. 树的主体，很正常，打开小角度，开口在左上角 p=ggtree(tree,layout = \u0026#34;fan\u0026#34;,open.angle = 5) # 2. 内圈注释，给部分clade加上不同Phylum的颜色，但是这个色块是加在tip到圆等半径的空间（这个很有意思，还没有看到过别人用R实现过） p1=p+geom_tiplab(data=tree_df,mapping = aes(color=Phylum),align = T,linetype = 1,linesize = 3.5,size=0)+ scale_color_manual(values = colors,guide=guide_legend(title = \u0026#34;Host phylum\u0026#34;,nrow = 3))+geom_tree(layout = \u0026#34;fan\u0026#34;) p1 1 2 3 4 5 6 7 8 9 10 11 12 # 3. 外圈注释1，方块代表phage，颜色代表family library(ggstar) p2=p1+geom_fruit( geom = geom_star, data = anno, mapping = aes( y=node,fill=`Phage family` ), starshape=13, starstroke=0,size=4 )+scale_fill_manual(values = c(\u0026#34;#de255c\u0026#34;,\u0026#34;#496db6\u0026#34;,\u0026#34;#c4c64f\u0026#34;),na.translate=FALSE,guide=guide_legend(ncol = 1)) p2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # 4. 外圈注释2，灰色五角星代表Genome contains Thoeris p3=p2+new_scale_fill()+ geom_fruit( geom = geom_star, data = anno, mapping = aes( y=node,fill=`Thoeris` ), starshape=1,offset = 0, starstroke=0,size=5 )+scale_fill_manual(name=\u0026#34;\u0026#34;,values = c(\u0026#34;grey\u0026#34;),na.translate=FALSE) # 5. 外圈注释3，绿色菱形加上文字 p4=p3+new_scale_fill()+ geom_fruit( geom = geom_star, data = anno, mapping = aes( y=node,fill=`bac` ), starshape=12,offset = 0.1, starstroke=0,size=3 )+scale_fill_manual(values = c(\u0026#34;#74cfd2\u0026#34;),na.translate=FALSE,guide=guide_none())+ geom_tiplab(data = tree_df%\u0026gt;%filter(label%in%(anno$node[which(!is.na(anno$bac))])), mapping = aes(label=label),angle=0,align = T,linetype = 0,offset = 2.5) p4+theme(legend.position = \u0026#34;bottom\u0026#34;) 呼～，暂时先做这几个图吧，再次强调，这是生成随机的树和一些随机的无科学意义的注释（仅供画图参考！！！）。\n如果你有好看的图需要复现或者有什么绘图上的问题，欢迎联系。\nReference 1. M. Bourquin, S. B. Busi, S. Fodelianakis, H. Peter, A. Washburne, T. J. Kohler, L. Ezzat, G. Michoud, P. Wilmes, T. J. Battin, The microbiome of cryospheric ecosystems. Nature Communications. 13, 3087 (2022).\n2. M. Royo-Llonch, P. Sánchez, C. Ruiz-González, G. Salazar, C. Pedrós-Alió, M. Sebastián, K. Labadie, L. Paoli, F. M. Ibarbalz, L. Zinger, B. Churcheward, S. Chaffron, D. Eveillard, E. Karsenti, S. Sunagawa, P. Wincker, L. Karp-Boss, C. Bowler, S. G. Acinas, Compendium of 530 metagenome-assembled bacterial and archaeal genomes from the polar Arctic Ocean. Nature Microbiology. 6, 1561–1574 (2021).\n3. Y. Liu, M. Ji, T. Yu, J. Zaugg, A. M. Anesio, Z. Zhang, S. Hu, P. Hugenholtz, K. Liu, P. Liu, Y. Chen, Y. Luo, T. Yao, A genome and gene catalog of glacier microbiomes. Nature Biotechnology. 40, 1341–1348 (2022).\n4. A. Leavitt, E. Yirmiya, G. Amitai, A. Lu, J. Garb, E. Herbst, B. R. Morehouse, S. J. Hobbs, S. P. Antine, Z.-Y. J. Sun, P. J. Kranzusch, R. Sorek, Viruses inhibit TIR gcADPR signalling to overcome bacterial defence. Nature. 611, 326–331 (2022).\n","date":"2023-05-05T00:00:00Z","image":"/p/r/images/example3_hudb155f8e92790c1df7e82ecec90ce7ca_1845460_120x120_fill_box_smart1_3.png","permalink":"/p/r/","title":"R绘制优美的进化树（进阶）"},{"content":"Introduction 批次效应（Batch effect）是指实验中的不同批次之间存在的系统性偏差。在组学研究中，批次效应通常由于样本的采集、处理或测量方法不同而产生。\n例如，如果将不同的样本在不同的实验批次中处理，可能会导致批次效应。这可能会对数据产生显著的影响，因为批次效应可能会掩盖或引入与所研究的变量无关的噪声。\n从最早的微阵列试验中就可以观察到批次效应,它由许多因素引起,包括试验分析时间、平台、环境等。后来的RNA-seq，微生物组等组学也会受到其影响，尤其是蛋白组和代谢组（质谱数据好像技术误差很大，批次效应比较明显，需要前期上足够的QC配合后期的数据处理来去除批次效应）。\n在组学研究中，批次效应可能会导致假阳性或假阴性结果，从而影响研究的准确性和可靠性。因此，研究人员通常会采取各种方法来检测和校正批次效应，例如使用正交化批次校正法（ComBat）或其他标准化方法来调整数据。这些方法可以帮助消除批次效应，提高数据质量，并确保结果的可靠性和可重复性。\nMethods 用于去除基因表达量批次效应的主要方法有ComBat方法、替代变量分析法、距离加权判别法和基于比值的方法等。\nhttp://html.rhhz.net/njnydxxb/201903001.htm\nhttps://www.chinagut.cn/papers/read/1098488862\n","date":"2023-05-03T00:00:00Z","permalink":"/p/batch-effect/","title":"组学数据去除批次效应（batch effect）"},{"content":"https://mp.weixin.qq.com/s/NEhoOlAau_jyxHPTf7H3ug 这篇推文讲的很详细。\nIntroduction 结构方程模型（Structural Equation Modeling，简称SEM）是一种结合多元统计方法和数学模型的分析技术。它能够帮助研究者探究多个变量之间的关系和影响，包括直接和间接的影响。SEM 可以同时估计多个方程（即多元回归模型），并且允许变量间相互作用，同时还能考虑隐变量（latent variable）和测量误差等因素。它的应用范围非常广泛，例如社会科学、心理学、教育学、医学、生物学等领域。\n在生物学领域，结构方程模型（SEM）被广泛用于探索和测试生物学系统的复杂关系。以下是一些具体的应用示例：\n模拟生态系统：SEM可用于建立生态系统的复杂模型，并确定影响生态系统的不同因素之间的关系。例如，可以将SEM应用于研究气候变化对生态系统的影响，或者研究生物多样性和生态系统功能之间的关系（当然包括了微生物生态研究）。\n遗传学：SEM可以用于研究遗传因素如何影响特定性状，例如疾病易感性、身高或体重等。通过SEM，可以确定哪些遗传因素是最重要的，以及这些遗传因素之间的相互作用。\n病理学：SEM可用于研究疾病的发生机制。通过SEM，可以确定哪些因素对疾病的发生和发展具有关键作用，并识别可以用于治疗疾病的目标因素。\n行为科学：SEM可用于研究生物行为的复杂性。例如，可以使用SEM来研究基因、环境和行为之间的相互作用，以确定影响人类和动物行为的主要因素。\n生态毒理学：SEM可以用于研究污染物对生态系统的影响。通过SEM，可以确定哪些污染物是最有害的，以及它们对生态系统的影响方式。\nExample 在宏基因组微生物生态学中，研究者通常会测量样品中多个微生物群落的组成和它们所处的环境变量（如温度、pH值等）之间的关系。然而，这些微生物群落之间可能存在相互作用，如竞争、合作等，这些作用可能会影响到它们与环境之间的关系。在这种情况下，SEM可以用来建立微生物群落与环境变量之间的关系网络，同时考虑微生物群落之间的相互作用。\n例如，一项研究调查了植物根际微生物群落与土壤性质之间的关系。研究者测量了植物根际中多个微生物群落的组成，以及土壤中的一些物理化学性质，如有机质含量、pH值等。他们使用SEM来建立微生物群落与土壤性质之间的关系网络，并同时考虑微生物群落之间的相互作用。他们发现，微生物群落之间存在着复杂的相互作用，而这些相互作用会影响到微生物群落与土壤性质之间的关系，提高了对微生物群落与环境之间关系的理解。\n下面给出一个用R生成模拟数据并实现SEM的例子，假设我们研究一些微生物和它们生长环境之间的关系。其中，环境因子包括pH、温度、盐度，微生物包括细菌、真菌和古菌。\n首先，我们需要生成一些符合正态分布的随机数据作为我们的变量。具体代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 library(mvtnorm) set.seed(123) # 生成环境因子数据 n \u0026lt;- 1000 mu_env \u0026lt;- c(pH = 7, temp = 25, salinity = 3) sigma_env \u0026lt;- matrix(c(1, 0.8, 0.5, 0.8, 1, 0.3, 0.5, 0.3, 1), ncol = 3) env \u0026lt;- rmvnorm(n, mean = mu_env, sigma = sigma_env)%\u0026gt;%as.data.frame() # 生成微生物数据 mu_microbe \u0026lt;- c(bacteria = 20, fungi = 10, archaea = 5) sigma_microbe \u0026lt;- matrix(c(1, 0.5, 0.3, 0.5, 1, 0.2, 0.3, 0.2, 1), ncol = 3) microbe \u0026lt;- rmvnorm(n, mean = mu_microbe, sigma = sigma_microbe)%\u0026gt;%as.data.frame() # 将生成的数据合并为一个数据框 df \u0026lt;- data.frame(env, microbe) 接下来，我们使用sem函数实现SEM。具体代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 library(lavaan) model \u0026lt;- \u0026#39; # 定义测量模型 # bacteria =~ bacteria # fungi =~ fungi # archaea =~ archaea # 定义结构模型 bacteria ~ pH + temp fungi ~ temp + salinity archaea ~ pH + salinity # 直接效应 bacteria ~~ fungi bacteria ~~ archaea fungi ~~ archaea \u0026#39; # 运行SEM fit \u0026lt;- sem(model, data = df) # 查看SEM结果 summary(fit) ## lavaan 0.6.15 ended normally after 22 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 12 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 3.024 ## Degrees of freedom 3 ## P-value (Chi-square) 0.388 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## bacteria ~ ## pH -0.022 0.047 -0.463 0.643 ## temp 0.074 0.048 1.541 0.123 ## fungi ~ ## temp 0.019 0.033 0.574 0.566 ## salinity 0.024 0.029 0.828 0.408 ## archaea ~ ## pH 0.017 0.036 0.481 0.631 ## salinity 0.028 0.034 0.822 0.411 ## ## Covariances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## .bacteria ~~ ## .fungi 0.467 0.035 13.496 0.000 ## .archaea 0.295 0.032 9.221 0.000 ## .fungi ~~ ## .archaea 0.183 0.031 5.869 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## .bacteria 0.990 0.044 22.361 0.000 ## .fungi 0.988 0.044 22.361 0.000 ## .archaea 0.948 0.042 22.361 0.000 最后，我们可以使用semPlot函数绘制SEM图形，以便更好地理解SEM模型的结构。具体代码如下：\n1 2 3 library(semPlot) semPaths(fit, what = \u0026#34;std\u0026#34;, nCharNodes = 10, sizeMan = 8, edge.label.cex = 1.1, curvePivot = TRUE, fade = FALSE) 运行上述代码后，我们就得到了一个可视化的SEM模型图，该模型描述了微生物和环境因子之间的关系。 Pro ","date":"2023-04-19T00:00:00Z","permalink":"/p/sem/","title":"结构方程模型（SEM）学习"},{"content":"这篇文章主要是介绍绘制进化树用的ggtree以及一些拓展包的基础用法，因为gg系列绘图的逻辑基本是搭积木，这种介绍性的文章能够帮助快速的找到想要的“积木”以及拼接的方法，剩下的就是随心所欲设计想要的图了。所以这篇出的图可能不是很“漂亮”，下次一定搞一篇提供复现各种文章美图的代码的推文💪。\nIntroduction 在数学中，“树”是一种特殊的无向图，它没有任何循环，并且任意两个顶点之间都只有一条简单路径。树通常用于建立层次关系，比如在计算机科学中，用树来表示文件系统、程序的调用关系、HTML文档的DOM结构等等。树也是算法和数据结构中非常重要的概念，许多问题可以用树来解决，比如搜索、排序、编码等等。\n系统发育树（Phylogenetic tree） 是一种用于描述不同物种或生物体系之间进化关系的树形结构。它是一种分支图，用于表示生命的进化历史，从共同祖先开始，沿着时间轴向后展开。系统发育树通常是基于各种生物学数据的分析所建立的，如形态特征、基因序列、蛋白质序列、生化反应等，以便识别物种之间的遗传变异和演化趋势。\n系统发育树的树枝代表演化分支，节点表示各个物种的最近共同祖先，树枝的长度表示演化距离或进化时间。系统发育树的形状和结构可以帮助我们理解不同物种之间的亲缘关系，了解生物进化和演化过程。在生物分类学、生态学、生物地理学和生物技术等领域中，系统发育树都具有重要的应用价值。\nVisualization 有很多软件平台可以用来可视化Phylogenetic tree，下面列举一些比较常用的：\niTOL (Interactive Tree Of Life)：这是一个在线工具，可以方便地创建、注释和可视化Phylogenetic tree。它提供了多种样式和注释选项，可以根据需求自定义显示效果。网址为：https://itol.embl.de/（不付费的话无法保存中间步骤，注释方法不太友好[Doge]）\nFigTree：这是一个免费的软件，可以在Mac和Windows系统上运行。它可以可视化Nexus、Newick和Nexml等格式的Phylogenetic tree，并提供了多种样式和注释选项。网址为：http://tree.bio.ed.ac.uk/software/figtree/\nPhylo.io：这也是一个在线工具，可以可视化Phylogenetic tree并进行自定义注释。它还提供了交互式功能，可以探索和比较不同部分之间的关系。网址为：https://phylo.io/\nDendroscope：这是一个基于Java的软件，可以用来可视化Phylogenetic tree并进行比较和注释。它支持多种文件格式，包括Nexus、Newick、Phylip等。网址为：http://dendroscope.org/\nMEGA (Molecular Evolutionary Genetics Analysis)：这是一个免费的软件，可以用于分子进化分析和可视化。它可以创建、注释和可视化Phylogenetic tree，并提供了多种样式和注释选项。网址为：https://www.megasoftware.net/\n当然，在线平台和带GUI界面的软件有一定弊端，操纵不是那么自由。在R语言中，用于可视化Phylogenetic tree的包也有很多：\nggtree (1) +ggtreeExtra (2) ：Y叔的神包，基于ggplot2语法，学起来比较流畅，提供高度可定制化的绘图，支持各种不同类型的进化树。是我们要讲的重点，画树掌握这两个包基本就够了。官方英文教程；现在Y叔的中文教程也在热卖中。\nape：提供了用于绘制基本的进化树和一些简单的树的可视化函数，基础函数还是非常有用的，phylo对象也是一直要用的。\nphytools：提供了多种用于绘制进化树的函数，包括变形进化树的函数。\nggphylo：基于ggplot2语法，提供了一些用于绘制进化树的函数，支持多种不同的树形态。\n我们可以先安装载入一些包。\n1 2 3 4 5 6 7 8 9 10 11 12 if (!requireNamespace(\u0026#34;BiocManager\u0026#34;, quietly=TRUE)) install.packages(\u0026#34;BiocManager\u0026#34;) BiocManager::install(\u0026#34;YuLab-SMU/treedataverse\u0026#34;) BiocManager::install(\u0026#34;ggtree\u0026#34;) BiocManager::install(\u0026#34;ggtreeExtra\u0026#34;) library(dplyr) library(ggplot2) library(treedataverse) library(ggtree) library(ggtreeExtra) format 首先我们还是介绍一下存储树的一些文件格式和读取方法（毕竟读入数据一直是可视化的第一步）。\nNewick格式 (.newick 或 .nwk)：Newick格式是最常见的用于存储和传输系统发育树的格式之一。它是一种文本格式，可以使用文本编辑器打开和编辑。在Newick格式中，每个节点由一个括号表示，其后面跟着一个表示该节点的标签和一个冒号，然后是一个表示该节点的分支长度的数字，最后跟着逗号和下一个节点的括号。叶节点是没有子节点的节点。\nNexus格式 (.nex 或 .nexus)：Nexus格式是一种广泛使用的用于存储分子系统发育树和其他分子生物学数据的格式。它是一种文本格式，可以使用文本编辑器打开和编辑。Nexus格式允许存储多个数据集和多个树，并提供了一些其他的扩展功能，如注释、区分字符等。\nPhyloXML格式 (.xml)：PhyloXML格式是一种基于XML的格式，专门用于存储和传输系统发育树。它提供了许多其他格式所不具备的灵活性和扩展性，可以存储大量的信息，如节点注释、支持值、分支长度、支持率等。\nNeXML格式 (.xml)：NeXML格式是一种基于XML的格式，用于存储和传输生物多样性和系统发育树数据。它提供了PhyloXML格式的所有功能，并添加了一些额外的功能，如存储生物多样性元数据、事件、传输网格数据等。\n其他还有Phylip 格式 (.phy 或 .phylip)，PAUP 格式 (.paup)，IQ-TREE 格式 (.tree)，RAxML 格式 (.raxml)，MEGA 格式 (.meg)等等。\n在R中，可以使用不同的包和函数来读取不同格式的系统发育树文件。如 ape 包中的 read.tree() 函数可以读取Newick格式的系统发育树文件、 phangorn 包中的 read.phyDat() 和 read.phylo() 函数读取包括Nexus、Newick、Phylip等格式。\n当然，Y叔团队也贴心地准备好了一个包 treeio 用于读取、处理各种系统发育树文件格式：\n1 2 3 4 5 6 7 library(treeio) # 从 Newick 格式读取系统发育树数据 tree \u0026lt;- read.tree(\u0026#34;tree.nwk\u0026#34;) # 从 Nexus 格式读取系统发育树数据 tree \u0026lt;- read.nexus(\u0026#34;tree.nexus\u0026#34;) # 从 Phylip 格式读取系统发育树数据 tree \u0026lt;- read.phylo(\u0026#34;tree.phy\u0026#34;) Nexus、Newick、Phylip这几种格式读入后会变成常见的phylo S3对象，其他格式读入后一般会变成treedata S4对象（更高级全面的一个对象，当然亦可以和phylo相互转换），这两种对象都可以直接使用ggtree画图的，我们只要读入就好。\nbasic plot 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 set.seed(1234) #生成示例树 tree=rtree(50) (main_p=ggtree(tree, mapping = NULL, layout = \u0026#34;rectangular\u0026#34;,#常用circular，daylight open.angle = 0,#部分支持，如fan mrsd = NULL,#时间轴 as.Date = FALSE, yscale = \u0026#34;none\u0026#34;, yscale_mapping = NULL, ladderize = TRUE,#阶梯状排列树 right = FALSE, branch.length = \u0026#34;branch.length\u0026#34;,#\u0026#34;none\u0026#34;就会让branch末端都对齐 root.position = 0, xlim = NULL, #线段风格 color=\u0026#34;black\u0026#34;, size=0.5, linetype=1)) layout 整棵树的布局方式有很多’rectangular’, ‘dendrogram’, ‘slanted’, ‘ellipse’, ‘roundrect’, ‘fan’, ‘circular’, ‘inward_circular’, ‘radial’, ‘equal_angle’, ‘daylight’ or ‘ape’，也可以用其它一些参数调节：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 ggtree(tree) ggtree(tree, layout=\u0026#34;roundrect\u0026#34;) ggtree(tree, layout=\u0026#34;slanted\u0026#34;) ggtree(tree, layout=\u0026#34;ellipse\u0026#34;) ggtree(tree, layout=\u0026#34;circular\u0026#34;) ggtree(tree, layout=\u0026#34;fan\u0026#34;, open.angle=120) ggtree(tree, layout=\u0026#34;equal_angle\u0026#34;) ggtree(tree, layout=\u0026#34;daylight\u0026#34;) ggtree(tree, branch.length=\u0026#39;none\u0026#39;) ggtree(tree, layout=\u0026#34;ellipse\u0026#34;, branch.length=\u0026#34;none\u0026#34;) ggtree(tree, layout=\u0026#39;circular\u0026#39;, branch.length=\u0026#39;none\u0026#39;) ggtree(tree, layout=\u0026#34;daylight\u0026#34;, branch.length = \u0026#39;none\u0026#39;) 1 2 3 4 5 6 7 8 9 10 11 ggtree(tree) + scale_x_reverse()#左右颠倒 ggtree(tree) + coord_flip()#上下颠倒 ggtree(tree) + layout_dendrogram()#只看骨架 ggplotify::as.ggplot(ggtree(tree), angle=-30, scale=.9)#旋转特定角度和缩放 ggtree(tree, layout=\u0026#39;slanted\u0026#39;) + coord_flip() ggtree(tree, layout=\u0026#39;slanted\u0026#39;, branch.length=\u0026#39;none\u0026#39;) + layout_dendrogram() ggtree(tree, layout=\u0026#39;circular\u0026#39;) + xlim(-10, NA)#扩大树中间的空白区域 ggtree(tree) + layout_inward_circular()#内外颠倒 ggtree(tree) + layout_inward_circular(xlim=15) label 1 2 #对齐加虚线 main_p+geom_tiplab(align = T,linetype = 3,linesize = 0.5,size=2,color=\u0026#34;red\u0026#34;) 1 2 #对齐不加虚线 main_p+geom_tiplab(align = F,size=4,color=\u0026#34;blue\u0026#34;,as_ylab = T) 1 2 #加上内部节点名称 ggtree(tree)+geom_nodelab(mapping = aes(label=node)) 1 2 #比例尺 main_p+geom_treescale(fontsize=6, linesize=1, offset=1, width=1, color=\u0026#39;red\u0026#39;) manipulate 我们可以使用tidytree::as_tibble或者ggtree::fortify把整个树（phylo对象）转为一个易于阅读的dataframe。\nparent和node两列展示了整个树的连接关系,branch.length是每个枝（连接parent和node）的枝长，label是node的label，一般每个tip（端点）都会有一个label，内部的点可能没有label（比如这个示例），我们也可以手动加上。\n1 tidytree::as_tibble(tree)%\u0026gt;%head() ## # A tibble: 6 × 4 ## parent node branch.length label ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 58 1 0.318 t2 ## 2 59 2 0.634 t47 ## 3 59 3 0.127 t40 ## 4 57 4 0.423 t20 ## 5 56 5 0.914 t44 ## 6 55 6 0.468 t3 1 2 tree_df=ggtree::fortify(tree) head(tree_df) ## # A tibble: 6 × 9 ## parent node branch.length label isTip x y branch angle ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 58 1 0.318 t2 TRUE 4.28 48 4.12 346. ## 2 59 2 0.634 t47 TRUE 5.55 49 5.24 353. ## 3 59 3 0.127 t40 TRUE 5.05 50 4.98 360 ## 4 57 4 0.423 t20 TRUE 3.88 47 3.67 338. ## 5 56 5 0.914 t44 TRUE 3.87 46 3.42 331. ## 6 55 6 0.468 t3 TRUE 3.17 45 2.94 324 我们可以用以下方法访问树的节点（node）的关系节点，可以通过tree或者tree生成的dataframe查询：\n1 2 library(tidytree) child(tree, 53)#子节点,一层 ## [1] 54 63 1 parent(tree, 1)#父节点，一层 ## [1] 58 1 offspring(tree, 53)#所有的子节点，包括子节点的子节点 ## [1] 54 63 55 60 56 6 57 5 58 4 1 59 2 3 7 61 62 10 8 9 64 67 65 66 11 ## [26] 12 13 14 15 16 1 ancestor(tree, 1)#所有的父节点，包括父节点的父节点 ## [1] 58 57 56 55 54 53 52 51 1 sibling(tree, 1)#兄弟节点 ## [1] 59 1 MRCA(tree, 1, 2)#最近祖先 ## [1] 58 节点分组\ngroupOTU() 和 groupClade() 方法旨在将分类群分组信息添加到输入树对象，这种分组信息可以直接用在 ggtree 的树可视化中。\ngroupOTU() 输入的是要给分组下的每个node。\ngroupClade() 输入给每个分组的最高级node即可，会自动分配该node下所有offspring node同一个分组。\n1 groupOTU(tree, .node = c(52,53),group_name = \u0026#34;OTU\u0026#34;)%\u0026gt;%as_tibble()%\u0026gt;%head ## # A tibble: 6 × 5 ## parent node branch.length label OTU ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; ## 1 58 1 0.318 t2 0 ## 2 59 2 0.634 t47 0 ## 3 59 3 0.127 t40 0 ## 4 57 4 0.423 t20 0 ## 5 56 5 0.914 t44 0 ## 6 55 6 0.468 t3 0 1 groupClade(tree, .node = c(52,53),group_name = \u0026#34;Clade\u0026#34;)%\u0026gt;%as_tibble()%\u0026gt;%head ## # A tibble: 6 × 5 ## parent node branch.length label Clade ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; ## 1 58 1 0.318 t2 2 ## 2 59 2 0.634 t47 2 ## 3 59 3 0.127 t40 2 ## 4 57 4 0.423 t20 2 ## 5 56 5 0.914 t44 2 ## 6 55 6 0.468 t3 2 取树的子集\n1 2 3 4 5 6 7 clade \u0026lt;- tree_subset(tree, node=53, levels_back=0) clade2 \u0026lt;- tree_subset(tree, node=53, levels_back=1) p1 \u0026lt;- ggtree(clade) + geom_tiplab() + xlim(0, 5) p2 \u0026lt;- ggtree(clade2, aes(color=group)) + geom_tiplab() + xlim(0, 9) + scale_color_manual(values=c(\u0026#34;black\u0026#34;, \u0026#34;red\u0026#34;)) p1+p2 其他一些操作:\n1 2 3 4 5 6 7 8 #重设树根,指定外类群 trda2 \u0026lt;- root(tree, outgroup = 104, edgelabel = TRUE) #缩放树枝 rescale_tree(tree, \u0026#39;dN\u0026#39;) #删除tip drop.tip(tree,1:5) annotation tip annotation 这类注释方法使用的注释数据集基本是一列是tip的node号或label，其他列为注释数据（分类数据/连续数值数据），每一行是一个tip。\nggtree本身自带一些实现tip注释（添加热图等对象）的功能，基于facet以及数据对齐tip：\n1 2 3 4 5 6 #生成测试注释文件 anno=data.frame(row.names = tree$tip.label,node=tree$tip.label, Group=sample(LETTERS[1:3],50,replace = T), Type=sample(letters[1:3],50,replace = T), value1=abs(rnorm(50)),high=abs(rnorm(50,5))) head(anno) ## node Group Type value1 high ## t2 t2 B a 1.41126240 5.287710 ## t47 t47 B c 0.83758243 4.340230 ## t40 t40 B a 1.12376279 7.919140 ## t20 t20 A a 3.04376589 5.677416 ## t44 t44 A b 0.23502131 4.315680 ## t3 t3 C a 0.03325861 5.186492 1 2 3 p=ggtree(tree)+geom_tiplab(align = T) gheatmap(p, anno[\u0026#34;Group\u0026#34;], offset=0, width=0.2) 或者使用aplot包进行会自动对齐tip的组图（用cowplot无法实现），但缺点就是组图时候的一些细节不好调节，且要考虑清楚各个分面insert的关系和宽度。\n1 2 3 4 5 6 library(aplot) ap1=ggplot(anno,aes(x=node,y=\u0026#34;Group\u0026#34;,fill=Group))+geom_tile()+ coord_flip() + theme_tree2() + theme(legend.position=\u0026#39;none\u0026#39;) ap2=ggplot(anno,aes(x=node,y=high,fill=Group))+geom_col()+ coord_flip() + theme_tree2() + theme(legend.position=\u0026#39;none\u0026#39;) ap1%\u0026gt;% insert_left(p,width = 4) %\u0026gt;% insert_right(ap2, width=2) 目前感觉最好用的tip注释方法就是 ggtreeExtra，有统一的注释逻辑geom_fruit,按照图层一个一个添加，比较易学和调节。具体能用来注释的图层有’geom_bar’, ‘geom_col’, ‘geom_boxplot’, ‘geom_violin’，‘ggstar’,’ggimage’等。\n每一个geom图层就像一个积木块一样，我们要做的就是挑选合适的积木，用同样的方法搭上去即可（也是整个ggplot的绘图逻辑，优雅），对每个geom图层，我们同样可以使用ggplot的各种scale方法调节颜色、大小、形状等。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 p=ggtree(tree,layout = \u0026#34;fan\u0026#34;) #geom_star library(ggstar) p1=p + geom_fruit( data=anno, geom=geom_star, mapping=aes(y=`node`,fill=Type, size=value1, starshape=Group), position=\u0026#34;identity\u0026#34;, starstroke=0.1 )+ #调节star的大小以及其legend scale_size_continuous( range=c(1, 3), # the range of size. guide=guide_legend( keywidth=0.5, keyheight=0.5, override.aes=list(starshape=15), order=2 #legend的顺序 ) ) + #调节star的颜色以及其legend scale_fill_manual( values=c(\u0026#34;#8dd3c7\u0026#34;, \u0026#34;#ffed6f\u0026#34;, \u0026#34;#bebada\u0026#34;), guide=\u0026#34;none\u0026#34; ) + #调节star的形状以及其legend scale_starshape_manual( values=c(1, 16,15), guide=guide_legend( keywidth=0.5, keyheight=0.5, order=1 ) ) p1 要注意的是！在添加多个图层时，有时候我们要用多次的fill或color属性，但是ggplot的scale_fill_manual会对全局生效，结果就会比较奇怪，所以我们在图层之间一般要使用ggnewscale包来产生一个新的scale，之后就可以分别调节了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 library(ggnewscale) p2=p1 + ggnewscale::new_scale_fill() + geom_fruit( data=anno, geom=geom_tile, mapping=aes(y=node, fill=value1), offset=0.08, # 调节 pwidth=0.25 # width of the external layer, default is 0.2 times of x range of tree. ) + scale_fill_gradient(low = \u0026#34;white\u0026#34;,high = \u0026#34;red\u0026#34;) p2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 p2+ ggnewscale::new_scale_fill() + geom_fruit( data=anno, geom=geom_col, mapping=aes(y=node, x=high, fill=Type), pwidth=0.4,offset = 0.1, axis.params=list( axis=\u0026#34;x\u0026#34;, # 添加x轴文字 text.size=2, #文字大小 text.angle=-45, # 角度 hjust=0 # 调节 ), grid.params=list() # 添加网格线 ) + scale_fill_manual( values=c(\u0026#34;#a6bce3\u0026#34;, \u0026#34;#fb9a99\u0026#34;, \u0026#34;#fdbf6f\u0026#34;), guide=guide_legend(keywidth=0.5, keyheight=0.5, order=6) ) 当然还可以加一些有趣的比如 image注释 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 library(ggimage) library(ggtree) nwk \u0026lt;- paste0(\u0026#34;((((bufonidae, dendrobatidae), ceratophryidae),\u0026#34;, \u0026#34;(centrolenidae, leptodactylidae)), hylidae);\u0026#34;) imgdir \u0026lt;- system.file(\u0026#34;extdata/frogs\u0026#34;, package = \u0026#34;TDbook\u0026#34;) x = read.tree(text = nwk) ggtree(x)+geom_tiplab()+ geom_fruit( data = data.frame(node=x$tip.label,image=paste0(imgdir, \u0026#39;/\u0026#39;, x$tip.label, \u0026#39;.jpg\u0026#39;)), geom = geom_image, mapping = aes(y=node,image=image), offset=0.5, align=2, size=.2 ) 生物剪影，Phylopic 包含 3200 多个生物剪影，涵盖很多门类。\n1 2 3 4 5 6 7 8 9 10 11 12 13 newick \u0026lt;- paste0(\u0026#34;((Pongo_abelii,(Gorilla_gorilla_gorilla,(Pan_paniscus,\u0026#34;, \u0026#34;Pan_troglodytes)Pan,Homo_sapiens)Homininae)Hominidae,\u0026#34;, \u0026#34;Nomascus_leucogenys)Hominoidea;\u0026#34;) tree \u0026lt;- read.tree(text=newick) #通过phylopic_uid查询剪影 d \u0026lt;- ggimage::phylopic_uid(tree$tip.label) d$body_mass \u0026lt;- c(52, 114, 47, 45, 58, 6) p \u0026lt;- ggtree(tree) %\u0026lt;+% d + geom_tiplab(aes(image=uid, colour=body_mass), geom=\u0026#34;phylopic\u0026#34;, offset=2.5) + geom_tiplab(aes(label=label), offset = .2) + xlim(NA, 7) + scale_color_viridis_c() internal annotation 这类注释方法使用基本是内部的node号或label，加上额外的注释数据（分类数据/连续数值数据）\n可选的对象有cladelab，strip，highlight，taxa_link… 也能直接给一部分树枝上色。\n1 2 3 4 5 6 7 8 9 internal_anno=data.frame(node=c(53,68,79),name=paste0(\u0026#34;clade\u0026#34;,1:3),angle1=c(0,90,0)) tree2 \u0026lt;- groupClade(tree, setNames(internal_anno$node,internal_anno$name),group_name = \u0026#34;test\u0026#34;) (col_p=ggtree(tree2, aes(color=test))+ scale_color_manual(values=c(\u0026#34;black\u0026#34;, \u0026#34;red3\u0026#34;, \u0026#34;skyblue\u0026#34;,\u0026#34;yellow3\u0026#34;)) + #theme(legend.position=\u0026#39;none\u0026#39;)+ guides(color=guide_legend(override.aes = list(linewidth=3))) ) 1 2 3 #添加point注释 ggtree(tree2, layout=\u0026#34;fan\u0026#34;) + geom_point2(mapping = aes(subset=c(test!=0),color=test)) 1 2 ggtree(tree, layout=\u0026#34;fan\u0026#34;) + geom_cladelab(data = internal_anno,mapping = aes(node=node,label=name,color=name,angle=angle1)) 1 2 3 4 5 6 #strip类似cladelab，不过提供的是两个tip，可以横跨不同位置的枝条,不接受mapping ggtree(tree, layout=\u0026#34;fan\u0026#34;) + geom_tiplab() + geom_strip(\u0026#39;t10\u0026#39;, \u0026#39;t33\u0026#39;, barsize=2, color=\u0026#39;red\u0026#39;, label=\u0026#34;group1\u0026#34;, offset.text=.1) + geom_strip(\u0026#39;t12\u0026#39;, \u0026#39;t21\u0026#39;, barsize=2, color=\u0026#39;blue\u0026#39;, label = \u0026#34;group2\u0026#34;, offset.text=.1) 1 2 3 #highlight ggtree(tree, layout=\u0026#34;fan\u0026#34;) + geom_highlight(data = internal_anno,mapping = aes(node=node,label=name,fill=name)) 1 2 3 4 #balance类似highlight，不过可以提供两个有包含关系的clade,不接受mapping ggtree(tree) + geom_balance(node=55, fill=\u0026#39;steelblue\u0026#39;, alpha=0.6, extend=1) + geom_balance(node=56, fill=\u0026#39;darkgreen\u0026#39;, alpha=0.6, extend=1) 1 2 3 4 5 6 7 8 9 10 11 12 links \u0026lt;-data.frame(from=c(\u0026#34;t36\u0026#34;,\u0026#34;t7\u0026#34;,\u0026#34;t23\u0026#34;), to=c(\u0026#34;t6\u0026#34;, \u0026#34;t46\u0026#34;, \u0026#34;t29\u0026#34;), type=c(\u0026#34;l1\u0026#34;, \u0026#34;l2\u0026#34;, \u0026#34;l3\u0026#34;), s=c(2, 1, 2)) ggtree(tree, layout=\u0026#34;inward_circular\u0026#34;, xlim=c(30, 0)) + geom_taxalink(data=links, mapping=aes(taxa1=from, taxa2=to, color=type, size=s)) + geom_tiplab(hjust=1) + scale_size_continuous(range=c(1,3)) 另外一些是通过内部节点改变整个树分布的方法：\n1 2 #通过scaleClade调节某个clade占比 scaleClade(col_p, node=53, scale=.1) 1 2 #通过collapes折叠某个clade collapse(col_p, node=53,mode = c(\u0026#34;mix\u0026#34;,\u0026#34;max\u0026#34;,\u0026#34;min\u0026#34;,\u0026#34;none\u0026#34;)[1],fill=\u0026#34;red3\u0026#34;,alpha=0.4) 1 2 #通过rotate旋转两个clade ggtree::rotate(col_p,node = 52)+geom_nodelab(aes(subset=c(node==52),label=node)) 1 2 #通过flip交换两个clade ggtree::flip(col_p,53,68)+geom_nodelab(aes(label=node)) 1 2 #通过open_tree和rotate_tree调节角度 open_tree(col_p, 180)%\u0026gt;%rotate_tree(90) 掌握以上这些方法，基本上可以绘制出用在文章的树了（当然一定要自己好好找配色，调整细节！！！）\n下次进阶！\nReference 1. G. Yu, D. K. Smith, H. Zhu, Y. Guan, T. T.-Y. Lam, Ggtree: An r package for visualization and annotation of phylogenetic trees with their covariates and other associated data. Methods in Ecology and Evolution. 8, 28–36 (2017).\n2. S. Xu, Z. Dai, P. Guo, X. Fu, S. Liu, L. Zhou, W. Tang, T. Feng, M. Chen, L. Zhan, T. Wu, E. Hu, Y. Jiang, X. Bo, G. Yu, ggtreeExtra: Compact Visualization of Richly Annotated Phylogenetic Data. Molecular Biology and Evolution. 38, 4039–4042 (2021).\n","date":"2023-04-14T00:00:00Z","image":"/p/r-tree/index.cn_files/figure-html/unnamed-chunk-15-1_hu64a041d9665808fa476c64c6e2c5c164_245537_120x120_fill_box_smart1_3.png","permalink":"/p/r-tree/","title":"R绘制优美的进化树（基础）"},{"content":"【浙江大学生命科学研究院蒋超实验室】实验技术员招聘火热进行中！🔥\n🌟浙江大学生命科学研究院蒋超实验室诚邀实验技术员 1-2 名加盟！我们邀请您加入我们的课题组，参与全球大型暴露组研究项目，负责 DNA 提取和二代测序的文库构建工作。估计项目周期约为4个月(带薪水)，届时您将有机会深入系统地学习生物信息学，甚至有机会作为共同作者在顶级期刊发表文章！\n🔬我们欢迎具有分子实验基础的同学（大三或即将硕士毕业）加入我们的队伍！\n🔗实验室网站：http://lsi.zju.edu.cn/jianglab_exposome/list.htm\n👨🏻‍🎓导师简介：\n蒋超博士现任浙江大学生命科学研究院研究员，兼聘浙江大学附属第一医院，获得国家级青年人才项目。本科毕业于厦门大学生命科学学院。博士师从美国印第安纳大学的 Yves Brun 教授，从事分子进化层面解析细菌形态进化的机制的研究，于 2014 年取得博士学位。后加入美国斯坦福大学医学院遗传系精准医疗中心 Mike Snyder 教授组进行博士后研究。Mike Snyder 教授是国际著名的精准化医疗和多组学研究的先驱。博士后期间，首次全面解读人类健康研究中未知的重要组成部分-个人环境暴露组。\n于 2019 回国后主要研究围绕环境暴露组、微生物组、精准医学开展，具体方向为：环境暴露组在宏观及个体水平上的精准时空动态测量；环境暴露组与人类疾病健康的关系；微量人、环境微生物组精准时空动态测量及其与人类疾病健康和环境健康的关系；环境暴露组、微生物组以及相关组学的分子实验技术和计算统计方法的开发；个体环境暴露组可穿戴监测设备的开发和运用；微生物进化；微生物组功能原件大规模挖掘；多组学和机器学习在微生物组及精准医学的应用等。\n以第一作者和/或通讯作者（含共同）在国际知名期刊 Cell、Nature、Nature Protocols、Cell Discovery等杂志发表多篇研究论文。以共同作者在Nature Biotechnology、Circulation Research、Genome Research等杂志刊文。近期以通讯作者在 Science 上发表对新冠病毒起源时间模型分析的 eLetter 评论。已取得美国空气暴露组监测设备相关专利，申请国内相关专利若干。\n实验室研究方向 💼职位要求：\n1.具备扎实的分子实验基础，最好熟练掌握 DNA 或 RNA 提取，荧光定量 PCR 或普通 RCR；\n2.具备较强的学习能力、执行力，以及出色的团队合作意识和沟通能力；\n3.每周工作至少 4 天，周一到周日，可根据您的时间安排。大三和即将硕士毕业同学皆可；\n4.实验全程将在博士后的带领和指导下进行；\n5.提供从入门到精通的生信学习机会，包括手把手教授大数据分析和高级可视化技巧。PI 将提供从科研创新思维到项目落实的全方位\u0026quot;博士\u0026quot;级别培养和指导。\n💰待遇： 薪酬待遇为每个月5000元。实验室提供专业的科研指导和一流的科研环境。硕士毕业生待项目结束后可以考虑续聘为全职技术员。\n📧申请方式： 有意向的候选人，请发送个人简历及相关资料（包括学习、工作和研究经历，以及其他证明个人能力、水平的相关资料）至 qiong_chen@zju.edu.cn，并注明\u0026quot;姓名+应聘技术员\u0026quot;。经考核后，我们将择优录取。\n📅招聘时间： 2023年4月10日至4月30日\n🏢实验地址： 浙大紫金港生科院338\n⌚️项目计划开展时间： 约2023年4月30日至2023年8月30日，大概率会提前完成实验工作。若该阶段实验未完成，可酌情续约与否。\n走过路过不要错过，请抓住这次难得的机会，加入我们这个充满活力和创新的团队，开启一段富有挑战和收获的科研之旅！期待您的加入！\n","date":"2023-04-10T00:00:00Z","image":"/p/werving-of-jiang-lab/images/research%20field_hu4f5e226684f5961809532c452b77bd89_122405_120x120_fill_box_smart1_3.png","permalink":"/p/werving-of-jiang-lab/","title":"蒋超实验室项目技术员招聘"},{"content":"Introduction 在 R 语言中，有多个软件包可以用来绘制地图，其中最流行的是 ggplot2、sp、maptools 和 leaflet。\nsimple examples 以下是一些简单的示例：\n使用 ggplot2 包绘制世界地图： 1 2 3 4 5 6 7 library(ggplot2) library(maps) world_map \u0026lt;- map_data(\u0026#34;world\u0026#34;) ggplot(world_map, aes(x = long, y = lat, group = group)) + geom_polygon(fill = \u0026#34;lightblue\u0026#34;, color = \u0026#34;white\u0026#34;) 使用 sp 包和 maptools 包绘制美国州界地图： 1 2 3 4 5 6 7 8 library(sp) library(maptools) #sids.shp是一个内置的数据，后面会讲到 us_states \u0026lt;- readShapePoly(system.file(\u0026#34;shapes/sids.shp\u0026#34;, package=\u0026#34;maptools\u0026#34;)[1], proj4string=CRS(\u0026#34;+proj=longlat +datum=WGS84\u0026#34;)) plot(us_states) 使用 leaflet 包绘制交互式地图： 1 2 3 4 5 library(leaflet) leaflet() %\u0026gt;% addTiles() %\u0026gt;% addMarkers(lng=-74.5, lat=40, popup=\u0026#34;New York City\u0026#34;) 如果需要更高级的地图可视化，就需要一些地图相关知。例如在地图上添加标注或进行交互式可视化，也是我们接下来要讨论的地方。\nFormat 首先，在地图绘制中，常用的数据格式包括：\n.shp 文件：地理信息系统 (GIS) 中的一种数据格式，通常用于存储地理矢量数据，例如点、线、面、多边形等地理特征的坐标和属性信息。\n.kml 文件：Google 地图使用的一种数据格式，通常用于存储地理矢量数据和标记，例如点、线、面、图标等地理特征的坐标、属性信息和样式信息。\n.kmz 文件：Google 地图使用的一种数据格式，通常是 .kml 文件的压缩版本，可以方便地共享和传输。\n.gpx 文件：一种常见的 GPS 数据格式，通常用于存储路径、轨迹等 GPS 数据。\n.json 文件：一种常见的数据交换格式，可以存储各种结构化数据，包括地理矢量数据、属性信息等。\n.csv 文件：一种常见的数据格式，可以存储表格数据，例如经纬度、名称、描述等地理信息。\n除了以上列举的格式之外，还有其他一些格式，例如 .gml、.svg、.dxf 等，这些格式通常在不同的 GIS 软件和地图绘制工具之间进行数据交换和共享。需要根据具体需求和使用工具选择合适的数据格式。\nR 可以处理上述列举的地图相关的文件格式。\n对于 .shp 文件，R中的 rgdal、sp、sf 等软件包可以读取和操作该文件格式，可以进行地图绘制、分析和处理等任务。\n对于 .kml 和 .kmz 文件，R中的 plotKML 软件包可以读取和操作该文件格式，可以进行地图绘制和分析等任务。\n对于 .gpx 文件，R中的 maptools、ggmap 等软件包可以读取和操作该文件格式，可以进行路径和轨迹可视化等任务。\n对于 .json 和 .csv 文件，R内置的函数和外部软件包都可以读取和操作该文件格式，可以进行地图数据的转换、处理和可视化等任务。\n总的来说，R 中有许多内置函数和软件包可以读取、操作和可视化地图相关的数据格式，可以帮助我们进行地图制作、数据分析和可视化等任务。\nCRS 坐标参考系统（Coordinate Reference System，CRS）是一个基于地球表面形状和大小的坐标系统。它是通过经度和纬度来描述地球表面上的点和区域的。\n在地理坐标系中，经度是指地球表面上某点所在的经线和地球本初子午线（即0度经线）之间的夹角，以东经为正，以西经为负。纬度是指地球表面上某点所在的纬线和赤道之间的夹角，以北纬为正，以南纬为负。\nWGS84坐标系统是全球定位系统（GPS）使用的一种地理坐标系统。WGS84是\u0026quot;World Geodetic System 1984\u0026quot;的缩写，是由美国国防部和美国国家地球空间情报局共同研制的。WGS84坐标系统使用经度和纬度来描述地球表面上的点和区域。它使用的是大地基准面，与传统的椭球体模型有所不同。WGS84坐标系统的零点定义为通过英国格林尼治天文台的经线（0度经线），也称为本初子午线。R的sf识别\u0026quot;EPSG:4326\u0026quot; or \u0026ldquo;WGS84\u0026rdquo; 都是该坐标系。\n在R里面我们可以定义自己绘图需要的坐标系，如\u0026quot;+proj=laea +lat_0=40 +lon_0=104\u0026quot; ，它表示一个局部等面积投影（LAEA）坐标系，中心纬度为40度，中心经度为104度（后面会用到）。\n如果想将一个地理坐标系（比如经度-纬度坐标系）中的点转换为这个投影坐标系中的点，可以使用R中的proj4库来进行转换。下面是一个简单的R代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 library(proj4) # 定义源坐标系（经度-纬度坐标系） src_crs \u0026lt;- \u0026#34;+proj=longlat +datum=WGS84\u0026#34; # 定义目标坐标系（局部等面积投影坐标系） dst_crs \u0026lt;- \u0026#34;+proj=laea +lat_0=40 +lon_0=104\u0026#34; # 定义待转换的点（经度、纬度） lon \u0026lt;- 116.4074 lat \u0026lt;- 39.9042 # 转换坐标系 p \u0026lt;- project(cbind(lon, lat), dst_crs) # 输出转换后的坐标（x、y） print(p) # 或者使用sf转换 source_df=st_as_sf(data.frame(lon, lat),coords = c(\u0026#34;lon\u0026#34;, \u0026#34;lat\u0026#34;), crs = 4326) sf::st_transform(source_df,crs=\u0026#34;+proj=laea +lat_0=40 +lon_0=104\u0026#34;) China map basic map 基础的ggplot2绘图代码如下，但是最近maps包好像没有存储中国的数据了，所以我们应该自己去找自己要画的区域的地图文件。\n1 2 3 4 5 6 7 8 9 10 11 12 # 注意，无法正常运行 library(maps) library(ggplot2) # 读取中国地图数据 china_map \u0026lt;- map_data(\u0026#34;china\u0026#34;) # 绘制中国地图 ggplot() + geom_polygon(data = china_map, aes(x = long, y = lat, group = group), fill = \u0026#34;white\u0026#34;, color = \u0026#34;black\u0026#34;) + theme_void() + coord_fixed() 我常用的是geojson文件，网上能够下载到中国地图的地方很多，比如 GeoJSON.cn，yezongyang/china-geojson等等，而世界上其他区域，不同精度（洲，国家，城市，县\u0026hellip;）的地图数据就需要自己按照需求寻找一下，如GeoJSON Maps of the globe (ash.ms)，georgique/world-geojson，datasets/geo-countries等等。\n中国地图绘制主要参考这篇博客：R-ggplot2 标准中国地图制作。\n使用sf包读取地图数据：这里用的是民政局提供的一个API，如果是自己下载的数据就换成文件地址即可，st_read或者read_sf可以以处理geojson文件。\n1 2 3 4 5 6 7 8 9 10 11 library(sf) library(ggplot2) #1.下载数据 dir=getwd() china_shp=paste0(dir,\u0026#34;/china.json\u0026#34;) if(!file.exists(china_shp))download.file(\u0026#34;https://gitcode.net/mirrors/lyhmyd1211/geomapdata_cn/-/raw/master/china.json?inline=false\u0026#34;,china_shp) china \u0026lt;- sf::read_sf(china_shp) #2.民政部API link: https://mp.weixin.qq.com/s/qj1SRc6D8sgYJYaZzDux6Q API_pre = \u0026#34;http://xzqh.mca.gov.cn/data/\u0026#34; china = sf::st_read(dsn = paste0(API_pre, \u0026#34;quanguo.json\u0026#34;), stringsAsFactors=FALSE) 读取数据后变成一个sf对象，sf类型的数据本质还是一个data.frame，但是其中有些元素是MULTIPOLYGON S3对象，这是一种表示多边形几何对象的数据类型，可以用于描述多个不相交的多边形区域，每个多边形可以由一个或多个线性环组成。在R中，MULTIPOLYGON 通常作为空间数据类型的一种表示，用于表示地理区域的空间形状和位置关系。\nst_crs函数可以用来改变地理坐标系：\n1 head(china) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## Simple feature collection with 6 features and 10 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 97.1729 ymin: 34.58341 xmax: 126.0685 ymax: 53.33701 ## Geodetic CRS: WGS 84 ## # A tibble: 6 × 11 ## adcode name child…¹ level parent subFe…² adchar center centr…³ acrou…⁴ ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 110000 北京市 16 prov… \u0026#34;{ \\\u0026#34;… 0 \u0026lt;NA\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 2 120000 天津市 16 prov… \u0026#34;{ \\\u0026#34;… 1 \u0026lt;NA\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 3 130000 河北省 11 prov… \u0026#34;{ \\\u0026#34;… 2 \u0026lt;NA\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 4 140000 山西省 11 prov… \u0026#34;{ \\\u0026#34;… 3 \u0026lt;NA\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 5 150000 内蒙古自治区 12 prov… \u0026#34;{ \\\u0026#34;… 4 \u0026lt;NA\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 6 210000 辽宁省 14 prov… \u0026#34;{ \\\u0026#34;… 5 \u0026lt;NA\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## # … with 1 more variable: geometry \u0026lt;MULTIPOLYGON [°]\u0026gt;, and abbreviated variable ## # names ¹​childrenNum, ²​subFeatureIndex, ³​centroid, ⁴​acroutes 1 2 3 4 5 6 7 library(sf) library(ggplot2) library(ggspatial) # plot ggplot(china)+ geom_sf()+ labs(title=\u0026#34;Ministry of Civil of PRC\u0026#34;,x=\u0026#34;Lon\u0026#34;,y=\u0026#34;Lat\u0026#34;) 1 2 3 4 5 6 #改变crs # plot ggplot(china)+ geom_sf()+ coord_sf(crs = \u0026#34;+proj=laea +lat_0=40 +lon_0=104\u0026#34;)+ labs(title=\u0026#34;Ministry of Civil of PRC\u0026#34;,x=\u0026#34;Lon\u0026#34;,y=\u0026#34;Lat\u0026#34;) 使用ggspatial包添加比例尺和指北针：\n1 2 3 4 5 6 7 8 9 main_map=ggplot()+ geom_sf(data = china,fill=NA,size=1,color=\u0026#34;black\u0026#34;)+ coord_sf(crs = \u0026#34;+proj=laea +lat_0=40 +lon_0=104\u0026#34;)+ labs(title=\u0026#34;Ministry of Civil of PRC\u0026#34;,x=\u0026#34;Lon\u0026#34;,y=\u0026#34;Lat\u0026#34;) + annotation_scale(location = \u0026#34;bl\u0026#34;) + # spatial-aware automagic north arrow annotation_north_arrow(location = \u0026#34;tl\u0026#34;, which_north = \u0026#34;false\u0026#34;, style = north_arrow_fancy_orienteering)+theme_minimal() main_map annotation 使用ggplot常规的注释手段geom_point()或geom_text() 在使用geom_sf 绘制的地图上一般不会成功，而是应该先用st_as_sf转换我们的注释数据，然后用geom_sf() ，geom_sf_text() 或geom_sf_label()。\n不要把注释信息放在coord_sf 的后面，因为ggplot是按照图层添加元素的，coord_sf 转换坐标后加入的注释就会有位置偏差。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #生成测试数据 anno=data.frame(lon=runif(20,90,120), lat=runif(20,30,40), type=sample(letters[1:3], 20, replace = TRUE), value=runif(20,1,10)) #转换对象 anno_sf \u0026lt;- st_as_sf(anno,coords = c(\u0026#34;lon\u0026#34;, \u0026#34;lat\u0026#34;),crs = 4326) p2=ggplot()+ geom_sf(data = china,fill=NA,size=1,color=\u0026#34;black\u0026#34;)+ #添加散点注释 geom_sf(data = anno_sf,aes(fill=type,size=value),shape=21,colour=\u0026#39;black\u0026#39;,stroke=.25)+ scale_size(range = c(1,5))+ #添加text geom_sf_text(data = anno_sf,aes(label=round(value,1)),size=2,legend=\u0026#34;none\u0026#34;)+ coord_sf(crs = \u0026#34;+proj=laea +lat_0=40 +lon_0=104\u0026#34;)+ labs(title=\u0026#34;Ministry of Civil of PRC\u0026#34;,x=\u0026#34;Lon\u0026#34;,y=\u0026#34;Lat\u0026#34;) + annotation_scale(location = \u0026#34;bl\u0026#34;) + # spatial-aware automagic north arrow annotation_north_arrow(location = \u0026#34;tl\u0026#34;, which_north = \u0026#34;false\u0026#34;, style = north_arrow_fancy_orienteering)+theme_minimal() p2 如果想要添加具体坐标的注释或者使用xlim或，geom_sf_text里调整文本位置的nudge_x等功能，那也需要注意投影坐标系的变化，使用st_transform处理。\n1 2 3 4 5 6 7 8 9 #例如，这个方法是无法取出90E到120E的部分的，而是很窄的一段 p2+xlim(90,120) data.frame(lon=c(90,120),lat=c(30,30))%\u0026gt;% st_as_sf(coords = c(\u0026#34;lon\u0026#34;, \u0026#34;lat\u0026#34;), crs = 4326) %\u0026gt;% sf::st_transform(df2_sf,crs=\u0026#34;+proj=laea +lat_0=40 +lon_0=104\u0026#34;) #发现结果对应的是(-1348988,1539346),这样截取的才是90E到120E的部分。 p2+xlim(-1348988,1539346) Leaflet usage Leaflet是一个开源的JavaScript库，用于创建交互式地图。R中的Leaflet包提供了一个轻量级的接口，允许在R中创建交互式地图。Leaflet包提供了许多基本的地图操作功能，例如缩放、平移和旋转，同时还允许添加标记、文本注释、多边形、线条和点。\n在使用Leaflet包之前，需要先安装并加载它。可以通过以下代码安装和加载Leaflet包：\n1 2 install.packages(\u0026#34;leaflet\u0026#34;) library(leaflet) 在使用Leaflet包创建地图时，需要提供一个起始的中心点和缩放级别。例如，可以使用以下代码创建一个以纽约市为中心的地图，此外，可以使用addTiles()函数添加地图图层，还可以使用addMarkers()函数添加标记，使用addPolygons()函数添加多边形，使用addLines()函数添加线条，使用addPopups()函数添加弹出窗口等等。通过使用这些函数，可以在地图上添加各种各样的元素，从而创建一个自定义的交互式地图，以下是完成上述中国地图的一个完整的例子。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Prepare the text for the tooltip (HTML style): gre_text \u0026lt;- paste( \u0026#34;Type: \u0026#34;, anno$type, \u0026#34;\u0026lt;br/\u0026gt;\u0026#34;, \u0026#34;Value: \u0026#34;, round(anno$value,2), \u0026#34;\u0026lt;br/\u0026gt;\u0026#34; )%\u0026gt;% lapply(htmltools::HTML) type_col=colorFactor(palette = c(\u0026#34;red3\u0026#34;,\u0026#34;green3\u0026#34;,\u0026#34;blue3\u0026#34;),domain =anno$type) inter_p=leaflet(anno) %\u0026gt;% #添加图层 addTiles() %\u0026gt;% #确定中心点 setView(lng = 104, lat = 40, zoom = 4)%\u0026gt;% #添加散点注释 addCircleMarkers(~lon, ~lat, fillColor = ~type_col(type), fillOpacity = 0.7, color=\u0026#34;white\u0026#34;, radius=8, stroke=FALSE, label = gre_text, labelOptions = labelOptions( style = list(\u0026#34;font-weight\u0026#34; = \u0026#34;normal\u0026#34;, padding = \u0026#34;3px 8px\u0026#34;), textsize = \u0026#34;13px\u0026#34;, direction = \u0026#34;auto\u0026#34;)) %\u0026gt;% #添加图例 addLegend( pal=type_col, values=~type, opacity=0.7, title = \u0026#34;Type\u0026#34;, position = \u0026#34;bottomright\u0026#34; ) class(inter_p) inter_p add in PPT leaflet最后生成的是一个htmlwidget衍生对象，所以可以用htmlwidgets包保存为html文件并直接用浏览器打开。\n1 2 #保存结果 htmlwidgets::saveWidget(inter_p,file = \u0026#34;leaflet.html\u0026#34;) 有时候这种交互地图在presentation展示时非常好用，要是能插入到ppt就是件美事了。\n可惜的是，没有找到很好的方式完成这个功能😭（除非一开始的ppt都是用Rmarkdown做的）\nWindows上的Powerpoint可以考虑使用插件：\nwebView：PPT：如何在ppt中显示网页，并且实时操作？（但好像也挂掉了）\nLiveWeb：如何将实时网页添加到PowerPoint演示文稿 （新版office不行）\nMac上的Powerpoint更加不太行了。\n看来只能加个超链接到该文件，然后点击后跳转到浏览器了（希望未来有解决方法）。\nGeoTiff manipulate 我本人不是地科相关专业的，不太会用ArcGIS等更为专业的软件，但有时候也需要去收集一些跟地理环境相关的数据，比如气温，土壤元素等等，发现在很多数据库中这些数据以geotiff格式储存。\nGeoTIFF是一种基于TIFF格式的地理信息图像文件格式，它包含了地理空间信息和图像信息。GeoTIFF文件可以包含单一图像或多个图像，并且可以存储地球表面的各种数据，如高程数据、卫星图像、气象数据等等。GeoTIFF文件与普通的TIFF文件相比，增加了一些地理参考信息和投影信息，可以用于地图制作和GIS分析等领域。GeoTIFF文件中的地理参考信息包括地理坐标系和投影坐标系，以及每个像素所表示的地理位置和像素大小等信息。\nRasterLayer 在R语言中，可以使用raster包和gdal包读取和处理GeoTIFF文件。使用raster包中的函数可以读取GeoTIFF文件并将其转换为RasterLayer对象，以进行各种空间分析操作。\nRasterLayer是raster包中的一种对象类型，它代表了一个栅格数据集。栅格数据集是由多个栅格单元组成的二维矩阵，每个栅格单元都有一个特定的值，代表了该位置上的数据信息。在R语言中，使用raster包可以方便地读取和处理栅格数据集，例如遥感影像、地形图等。通过将栅格数据集转换为RasterLayer对象，可以进行各种空间分析操作，例如裁剪、合并、重采样、计算、绘图等。\n下面是一些基于RasterLayer对象（或者sf对象，很多函数都是通用的）的常见操作和分析的R语言代码示例：\n计算栅格数据的基本统计信息 1 2 3 4 5 6 7 8 9 10 library(raster) # 读入一个栅格数据文件 r \u0026lt;- raster(\u0026#34;path/to/raster.tif\u0026#34;) # 计算最小值、最大值、均值、方差 min_val \u0026lt;- minValue(r) max_val \u0026lt;- maxValue(r) mean_val \u0026lt;- mean(r) var_val \u0026lt;- var(r) 对栅格数据进行裁剪、合并、重采样等操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 library(raster) # 读入两个栅格数据文件 r1 \u0026lt;- raster(\u0026#34;path/to/raster1.tif\u0026#34;) r2 \u0026lt;- raster(\u0026#34;path/to/raster2.tif\u0026#34;) # 裁剪两个栅格数据文件 r1_crop \u0026lt;- crop(r1, extent(r2)) r2_crop \u0026lt;- crop(r2, extent(r1)) # 合并两个栅格数据文件 r_merge \u0026lt;- merge(r1_crop, r2_crop) # 将一个栅格数据文件的分辨率重采样为另一个栅格数据文件的分辨率 r_resample \u0026lt;- resample(r1_crop, r2_crop) 进行栅格数据的代数、逻辑运算 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 library(raster) # 读入两个栅格数据文件 r1 \u0026lt;- raster(\u0026#34;path/to/raster1.tif\u0026#34;) r2 \u0026lt;- raster(\u0026#34;path/to/raster2.tif\u0026#34;) # 对两个栅格数据文件进行加、减、乘、除运算 r_add \u0026lt;- r1 + r2 r_sub \u0026lt;- r1 - r2 r_mul \u0026lt;- r1 * r2 r_div \u0026lt;- r1 / r2 # 对一个栅格数据文件进行逻辑与、或、非运算 r_and \u0026lt;- r1 \u0026amp; r2 r_or \u0026lt;- r1 | r2 r_not \u0026lt;- !r1 进行栅格数据的可视化 1 2 3 4 5 6 7 8 9 10 11 12 13 library(raster) # 读入一个栅格数据文件 r \u0026lt;- raster(\u0026#34;path/to/raster.tif\u0026#34;) # 绘制等高线图 contour(r, add=FALSE) # 绘制等值线图 image(r, col=terrain.colors(255)) # 绘制渐变色图 plot(r, col=rainbow(255)) Example 继续我们上面的中国地图绘制，我们想要获得某个时间的中国气温图：\n首先在WorldClim网站上可以下载到全球历史天气信息：\n我们下载一个average temperature信息，精度为10minute(即10个经纬度的单位，比较粗略，35M～)，通过把全球的数据map到我们之前已经得到的sf对象china上就可以取出这部分的数据了。\n1 2 3 4 5 library(raster) avgtemp \u0026lt;- raster(\u0026#34;~/Downloads/wc2.1_10m_tavg/wc2.1_10m_tavg_01.tif\u0026#34;) #全球平均气温数据 plot(avgtemp) 1 2 3 #切片 china_tmp=crop(avgtemp,china) plot(china_tmp) 1 2 3 #掩膜 china_tmp=mask(china_tmp,china) plot(china_tmp) 最后再把经过切片掩膜处理的RasterLayer转换为sf对象，就可以根据我们之前绘制annotation的方法把气温数据全部绘制上去了。\n大功告成！这里是最后的结果：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 将RasterLayer转换为sf对象 sf_data \u0026lt;- st_as_sf(rasterToPolygons(china_tmp), crs = st_crs(china_tmp)) colnames(sf_data)[1]=\u0026#34;Average_temperature\u0026#34; ggplot()+ geom_sf(data = china,fill=NA,size=1,color=\u0026#34;black\u0026#34;)+ #添加栅格数据 geom_sf(data = sf_data,aes(fill=Average_temperature),shape=22,size=0.5,stroke=0,color=NA)+ scale_fill_gradient2(low = \u0026#34;blue\u0026#34;,mid = \u0026#34;white\u0026#34;,high = \u0026#34;red\u0026#34;)+ #使用ggnewscale产生一个新的fill映射，否则会覆盖报错。 ggnewscale::new_scale_fill()+ #添加散点注释 geom_sf(data = anno_sf,aes(fill=type,size=value),shape=21,colour=\u0026#39;black\u0026#39;,stroke=.25)+ scale_size(range = c(1,5))+ #添加text geom_sf_text(data = anno_sf,aes(label=round(value,1)),size=2,legend=\u0026#34;none\u0026#34;)+ coord_sf(crs = \u0026#34;+proj=laea +lat_0=40 +lon_0=104\u0026#34;)+ labs(title=\u0026#34;Ministry of Civil of PRC\u0026#34;,x=\u0026#34;Lon\u0026#34;,y=\u0026#34;Lat\u0026#34;) + annotation_scale(location = \u0026#34;bl\u0026#34;) + # spatial-aware automagic north arrow annotation_north_arrow(location = \u0026#34;tl\u0026#34;, which_north = \u0026#34;false\u0026#34;, style = north_arrow_fancy_orienteering)+theme_minimal() ","date":"2023-04-06T00:00:00Z","image":"/p/r-map/index_files/figure-html/unnamed-chunk-22-1_hu25d347af0f26b4f0f63f78f0a939c84a_495129_120x120_fill_box_smart1_3.png","permalink":"/p/r-map/","title":"R绘制优美的地图"},{"content":"Motivation cd-hit在数据量很大的情况下会非常慢，我问chatGPT有没有更快的：\nhttps://www.nature.com/articles/s41467-018-04964-5 是的，有一些其他的基因序列聚类软件也比较流行，其中一些也是非常快速的，例如：\nUCLUST：这是一种基于贪心算法的聚类工具，可用于聚类DNA和蛋白质序列。它是CD-HIT的一种变体，也是速度非常快的软件之一。\nVSEARCH：这是一种类似于UCLUST的工具，也是基于贪心算法，可以用于聚类DNA和蛋白质序列。它的速度比CD-HIT更快，而且还支持多线程和一些其他高级功能。\nMMSEQS2：这是一种使用图像剖析技术的聚类工具，可以用于聚类非常大的序列数据集。它比CD-HIT和UCLUST都要快，而且还支持多线程和GPU加速。\nSwarm：这是一种基于聚类半径的工具，可以用于聚类16S rRNA序列等微生物群落数据。它比CD-HIT等工具要快，而且可以处理高度冗余的序列。\n尝试使用一下mmseqs2 要使用MMseqs2聚类大量核酸序列，可以按照以下步骤操作：\n准备数据：收集所有要聚类的核酸序列，并将它们存储在一个文件中，例如FASTA格式。\n安装MMseqs2：访问MMseqs2的官方网站（https://github.com/soedinglab/MMseqs2）并下载和安装软件包。\n创建数据库：使用MMseqs2将所有核酸序列导入一个数据库中。例如，可以使用以下命令将FASTA格式的序列导入到MMseqs2数据库中： mmseqs createdb input.fasta db_name\n其中，\u0026ldquo;input.fasta\u0026quot;是你的输入文件名，\u0026ldquo;db_name\u0026quot;是你想要为数据库取的名称。\n运行聚类：使用MMseqs2运行聚类分析。例如，可以使用以下命令运行聚类分析： mmseqs easy-cluster db_name cluster_name tmp --threads 4\n其中，\u0026ldquo;db_name\u0026quot;是你的数据库名称，\u0026ldquo;cluster_name\u0026quot;是你想要为聚类结果取的名称，\u0026ldquo;tmp\u0026quot;是你想要为临时文件夹取的名称，\u0026rdquo;\u0026ndash;threads 4\u0026quot;表示你想要使用4个线程运行分析。\n解析结果：等待聚类分析完成后，可以使用MMseqs2的其他工具（如\u0026quot;mmseqs createtsv\u0026rdquo;）来解析聚类结果，并将结果导出到所需的格式中（如TSV格式）。 需要注意的是，MMseqs2支持多种聚类算法和参数设置，因此在运行分析之前，应该根据具体的应用需求进行设置和优化。例如，你可以选择使用其他的聚类算法，或调整聚类的相似性阈值等参数，以获得最佳的聚类结果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 #测试比较一下 cd ~/work/st/temp/cluster_test $ ll ../prodigal/C1.gene.fa -rw-r--r-- 1 pengchen jianglab 338M Apr 7 2022 ../prodigal/C1.gene.fa 800000条序列 #cd-hit============ ## aS覆盖度，c:identity相似度，G0局部比对？，g最优解，T多线程，M内存0不限制 cd-hit-est -i ../prodigal/C1.gene.fa \\ -o cd_hit.fa \\ -aS 0.9 -c 0.9 -g 0 -T 0 -M 0 #两小时过去了，还在处理第20000条 #mmseqs2========= ##min-seq-id:identity（相似度？），c覆盖度，rescore-mode3: global alignment #建库 input_fa=../prodigal/C1.gene.fa DB=C1.geneDB DB_clu=mmseq_out mmseqs createdb $input_fa $DB #聚类 mmseqs cluster $DB $DB_clu tmp \\ --min-seq-id 0.9 -c 0.9 --cov-mode 1 --threads 8 #输出转换 #第一列是代表序列id，第二列是成员序列id #mmseqs createtsv $DB $DB mmseq_out mmseq_out.tsv #mmseqs createseqfiledb $DB $DB_clu ${DB_clu}_seq #mmseqs result2flat $DB $DB ${DB_clu}_seq ${DB_clu}_seq.fasta #获取represent sequence mmseqs createsubdb $DB_clu $DB ${DB_clu}_rep mmseqs convert2fasta ${DB_clu}_rep ${DB_clu}_rep.fasta #上述步骤可以直接用easycluster替代。 input_fa=../prodigal/C1.gene.fa mmseqs easy-cluster $input_fa cluster_res tmp \\ --min-seq-id 0.9 -c 0.9 --cov-mode 1 --threads 8 #再测试一下easylinclust input_fa=../prodigal/C1.gene.fa mmseqs easy-linclust $input_fa lin_res tmp \\ --min-seq-id 0.9 -c 0.9 --cov-mode 1 --threads 8 #结果mmseq_out_rep.fasta：315M #这是单样本assembly结果的聚类结果，合理。 #250s,快得一批,使用10GB左右的内存。 #cluster模块可能需要大量内存。包含 N 个序列和每个序列平均 r 个比对结果的数据库的内存消耗可以估计为 #M = (6 * N * r) 字节 Cores per node: 8 CPU Utilized: 00:12:22 CPU Efficiency: 41.97% of 00:29:28 core-walltime Job Wall-clock time: 00:03:41 Memory Utilized: 9.40 GB Memory Efficiency: 60.14% of 15.62 GB 多个待聚类文件的使用方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 #cd-hit两个文件 cd-hit-est -i ../prodigal/C1.gene.fa \\ -o A.fa \\ -aS 0.9 -c 0.9 -g 0 -T 0 -M 0 # A和B基因集待聚类 # 计算B中特有的基因 cd-hit-est-2d -i A.fa -i2 B.fa -o B.uni.fa \\ -aS 0.9 -c 0.9 -g 0 \\ -T 96 -M 0 -d 0 # 合并为非冗余基因集 cat A.fa B.uni.fa \u0026gt; NR.fa #有更多文件时 #可以准备一个namelist first=`head -n1 namelist` echo handing $first cd-hit-est -i $first \\ -o NR.fa \\ -aS 0.9 -c 0.9 -g 0 -T 0 -M 0 for i in `tail -n+2 namelist` do echo handing $i cd-hit-est-2d -i NR.fa -i2 $i -o ${i}.uni.fa \\ -aS 0.9 -c 0.9 -g 0 \\ -T 96 -M 0 -d 0 cat ${i}.uni.fa \u0026gt;\u0026gt; NR.fa done #mmseqs2 #看看直接上两个文件的聚类结果 #合并文件：使用cat命令将所有核酸序列文件合并成一个大的FASTA格式文件 sed -i \u0026#34;/\u0026gt;/s/\u0026gt;/\u0026gt;C2_/\u0026#34; ../prodigal/C2.gene.fa cat ../prodigal/C1.gene.fa ../prodigal/C2.gene.fa \u0026gt; all.fasta #建库 mmseqs createdb all.fasta all DB=all DB_clu=all_clu #聚类 mmseqs cluster $DB $DB_clu tmp \\ --min-seq-id 0.9 -c 0.9 --cov-mode 1 --threads 8 --rescore-mode 3 #输出转换 #获取represent sequence mmseqs createsubdb $DB_clu $DB ${DB_clu}_rep mmseqs convert2fasta ${DB_clu}_rep ${DB_clu}_rep.fasta #mmseqs2 oldDB=C1.geneDB cluDB_old=mmseq_out newDB=all mmseqs clusterupdate $oldDB $newDB $cluDB_old ${newDB}_updated ${cluDB}_updated tmp \\ --min-seq-id 0.9 -c 0.9 --cov-mode 1 --threads 8 --rescore-mode 3 ","date":"2023-04-03T00:00:00Z","permalink":"/p/mmseqs2/","title":"Mmseqs2的基础使用"},{"content":"Theories and Methods of Species Diversity Research\n这是我在一门研究生课程上的课程任务，查阅了一些文献总结整理了物种多样性研究理论与方法，比较基础，没有对一些理论和模型深入讲解，仅做分享。\nIntroduction Biological diversity largely describes the change of life from genes to ecosystems, comprising their existences, genetic variations, their environment, populations and the ecosystem in which they are existing and other evolutionary developments that keep the system functioning, changing and adapting.\n生物多样性是度量生命的变异和变异能力的指标，又分成遗传多样性，物种多样性，生态系统多样性三个层次，我这里主要介绍物种多样性。 他又有三个尺度和三个维度，分别是α，β，γ分类多样性，谱系多样性，功能多样性，这里的每个交集都是可以研究的。\nData collection methods 我们应该如何进行物种多样性研究呢？\n首先当然是制定研究方案，进行取样， 对于自己的研究对象，可以进行随机取样，系统取样，或者其他的一些方法， 也可能是一块有代表性的永久样地，可以研究它的动态变化。\n然后是开始调查获取数据，传统的方法里，植物群落一般会进行实地调查或者使用遥感图像收集数据，动物会用到自动音视频记录，昆虫的话还会有灯光诱捕法等。\n还有对微生物进行调查的，用直接培养方法或者测序手段。\n当然这些方法不是一定专用的，一些新的方法也可以交叉使用，比如eDNA方法： eDNA是指在环境样品中所有被发现的不同生物的基因组DNA的混合，环境可以包括土壤、沉积物、排泄物、空气、水体，甚至生物个体本身。 动物在某个环境中生活，身上的各种痕迹会携带着自身DNA掉落到四周，所以用这个方法有调查各种物种的潜力，有篇文章就用空气的eDNA调查了附近的动物。\n此外还有整合的方法，比如这篇文章就提出，将遥感，田间调查和测序结合在一起，来获得完整的物种风度表。\n有了这些方法后，我们还要明确自己要获取的有哪些数据：\n我认为主要有三个方面：\n第一个就是群落的特征，包括各个物种的丰度，密度，频度，群落的生物量；如果是植物的话，还要收集盖度、树冠直径、树高、位置等等。 第二个重要的方面是我们每个样方的环境因子数据： 包括气候数据，如降水，相对湿度，温度等等，地形数据，如海拔，坡度等等，土壤数据，如土壤的粒径保水量和各种化学元素等等，有条件的话可以做微生物和人类因素的数据收集。 第三个方面是物种的特征：比如植物的生活型，生长型，功能类型等，这个对后面功能多样性的研究很关键 还有就是各个物种的系统发育位置，可以去找各种植物志，动物志或者NCBI的数据库来获得，这个对谱系多样性研究很重要。 Measure methods 获取了以上这些数据后，接下来就是使用一些指标对多样性进行度量。\nTaxa α diversity 首先是分类的a多样性指数，主要有以下几个类别：\n使用物种数指示 物种数和物种总个体数 物种数和物种总个体数和每个物种的个体数（还有一些专门用在测序数据表征多样性的，因为测序手段我们能获得的丰度表是不传统的。这种树我看到了几棵，那个动物有几只等等，而是用测序测到了多少条DNA来表征的，所以会有很多测不到的，或者只能测到几条序列的，就有Chao1和ACE指数来考虑这些非常低的物种） 用信息公示表示（常用的shannon-wiener指数就在这里） 均一性表示 Taxa β diversity beta多样性的三个常用的简单指标： 基于相异系数进行计算： 这里收集了目前会用到的各种距离的算法，比如我们常用的欧式距离，还有这个bray距离等，这样算出来就是一个距离矩阵，我们可以知道哪些样方之间差异大或者差异小 Taxa γ diversity gamma多样性，沿着长江取样来举个例子，图中每个取样点本身的微生物多样性是α-多样性（单个群落水平）；将1～9这九个样点归为一组，则这一组内各样点之间的差异，可以理解为β-多样性（局部或者区域水平）；当我们把所有取样点放在一起，站在一个更高、更广阔的尺度上时，这时候的多样性就是所谓的γ-多样性。这个指标就是α-多样性在尺度上的一个推广。\n一个例子总结三个尺度上的物种多样性：\n三个生态区九座山峰的生物多样性指数。每个符号代表不同的物种；有些物种只在一个峰上有种群，而另一些则在两个或更多峰上。每个峰上物种丰富度的变化导致每个生态区的不同 alpha、gamma 和 beta 多样性值。\n这种变化对我们如何分配有限的资源以最大限度地保护资源有影响。如果只能保护一个生态区，生态区 3可能是一个不错的选择，因为它具有高伽马（总）多样性。但是，如果只能保护一个山峰，是否应该保护生态区 1（有许多分布广泛的物种）或生态区3（有几个独特的、范围受限的物种）中的山峰？\nPhylogenetic α diversity 群落系统发育多样性PD：谱系树的总枝长度 净相关指数NRI： 计算所有物种对的平均谱系距离MPD，保持物种数量和个体数不变 最近邻体指数NTI： 计算所有物种对的平均谱系距离MNPD，保持物种数量和个体数不变 可以看上图右边的例子，3个样方发现的四种物种丰度表，我们确定了他们的系统发育关系就可以进行计算。\nPhylogenetic β diversity Beta的计算会稍微复杂一点，公式没有一个个列出，但是有相关的R包进行计算（vegan）。\nFunctional diversity 最后就是功能多样性指数，他是影响种群生存、群落和生态系统稳定的重要因素， 能更好地反映生态系统生产、养分平衡等功能。\n这个怎么计算呢，要收集好物种的功能类型，比如对于植物来说，有光和途径等等，也同样整理为功能表，功能表和物种丰度表mapping合并一下可以得到功能丰度表，然后就可以计算各种指数了。\nAnalytical methods 我们应该怎么做数据分析呢？（具体的流程和代码下次分享，这里先简单描述一下）\n首先拿到丰度表，可以对整个表进行重采样，每次采样计算一次a多样性指数，就可以做出这个稀释曲线，他可以说明我们在这个样方里的调查或者说测序手段是否是充足的。\n然后是计算每个样方的a多样性指数， 通过结合环境因子数据，我们可以做组间多重比较，比如说 对于一些数值数据，我们可以计算相关性，或者做回归分析。 这个回归又分为很多方式，比如线性回归，广义线性回归等等，这个可以帮我们发现那些因素会以怎样的方式影响a多样性，比如这里，可能海拔越高多样性越低。\n对于beta多样性，我们首先计算相异矩阵，这里就是样本与样本间的距离 我们可以做热图展示各样本间的距离， 或者做一个相似系数-地理距离的回归分析，一般来说，地理距离越远，可能beta多样性越高。\n或者具体看看组间差异，这里可以看到KO_OE组beta多样性最大， 还能做聚类分析，看看到底哪些样本更加类似。\nBeta多样性的另一种展示方法是排序方法，也叫降维分析，我们常用的方法有PCA，PcoA等等， 另外可以加上环境因子数据，做约束排序，比如RDA，CCA等方法，不仅能看到样本间的差异，还可以看到这种差异主要是由那种环境因子影响的，比如这里，在轴一上的差异主要是由env1这个变量引起的。\nTheories 物种多样性的中性理论做出了一个具有挑战性的假设，即所有个体在生态上都是相同的，并且不需要生态位差异来解释生物多样性模式。 根据中性理论，等同物种的高度多样化群落的出现是因为偶然的灭绝被物种形成所平衡。具体来说，适应性等价假设与随机或随机过程相结合，包括死亡、来自区域物种库的移民和物种形成，可以导致物种丰富的种群。（有关生态位和中性作用对多样性影响，专门有研究这种群落构建的）\nReferences ","date":"2023-04-01T00:00:00Z","image":"/p/biodiversity/images/diversity_hu497d07d6085df17c480b385e6cae73d8_591092_120x120_fill_box_smart1_3.png","permalink":"/p/biodiversity/","title":"物种多样性研究的理论和方法"},{"content":"Introduction Shiny 是一个 R 包，可轻松地直接从 R 构建交互式 Web 应用程序。还可以在网页上托管独立应用程序或将它们嵌入 R Markdown 文档或构建仪表板，还可以使用 CSS 主题、html 小部件和 JavaScript 操作扩展您的 Shiny 应用程序。\n1 2 3 4 5 6 7 8 #first app library(shiny) ui \u0026lt;- fluidPage( \u0026#34;Hello, world!\u0026#34; ) server \u0026lt;- function(input, output, session) { } shinyApp(ui, server) 整个app可分为两个部分，ui与sever，可以认为是前端与后端的关系，Rshiny提供了很多内置的小组件帮我们将前后端联系起来，非常方便好用。只需要有一点点的HTML/CSS/Javascript的知识就可以设计出更好看的界面和更多有趣的交互。\n这里是shiny的记忆手册,浓缩了大部分的操作。 Methods UI设计 Layout 最基础的布局就是sidebarLayout,可以分成sidebarPanel,mainPanel两个部分。\n1 2 3 4 5 6 7 8 ui \u0026lt;- fluidPage( titlePanel(\u0026#34;title panel\u0026#34;), sidebarLayout( sidebarPanel(\u0026#34;sidebar panel\u0026#34;), mainPanel(\u0026#34;main panel\u0026#34;) ) ) 可以通过将内容放在 *Panel 函数中来向Shiny 应用程序添加内容，shiny提供了R代码方便插入HTML (Table 1)，更多标签可以查看tag-glossary。\nTable 1: shiny function\tHTML5 equivalent\tcreates shiny function HTML5 equivalent creates p \u0026lt;p\u0026gt; A paragraph of text h1 \u0026lt;h1\u0026gt; A first level header h2 \u0026lt;h2\u0026gt; A second level header h3 \u0026lt;h3\u0026gt; A third level header h4 \u0026lt;h4\u0026gt; A fourth level header h5 \u0026lt;h5\u0026gt; A fifth level header h6 \u0026lt;h6\u0026gt; A sixth level header a \u0026lt;a\u0026gt; A hyper link br \u0026lt;br\u0026gt; A line break (e.g. a blank line) div \u0026lt;div\u0026gt; A division of text with a uniform style span \u0026lt;span\u0026gt; An in-line division of text with a uniform style pre \u0026lt;pre\u0026gt; Text ‘as is’ in a fixed width font code \u0026lt;code\u0026gt; A formatted block of code img \u0026lt;img\u0026gt; An image strong \u0026lt;strong\u0026gt; Bold text em \u0026lt;em\u0026gt; Italicized text HTML NA Directly passes a character string as HTML code Control widgets 这些是常用shiny内置的小控件，提供一种向 Shiny serve发送消息的方式。 Table 2: standard Shiny widgets function widget actionButton Action Button checkboxGroupInput A group of check boxes checkboxInput A single check box dateInput A calendar to aid date selection dateRangeInput A pair of calendars for selecting a date range fileInput A file upload control wizard helpText Help text that can be added to an input form numericInput A field to enter numbers radioButtons A set of radio buttons selectInput A box with choices to select from sliderInput A slider bar submitButton A submit button textInput A field to enter text 可以想想怎么获取更多控件，比如一个color panel等等。\n更多：\n使用 textInput() 收集少量文本，使用 passwordInput()3 收集密码，使用 textAreaInput() 收集文本段落。\n要收集数值，请使用 numericInput() 创建一个受约束的文本框或使用 sliderInput() 创建一个滑块。如果您为 sliderInput() 的默认值提供一个长度为 2 的数值向量，您将得到一个具有两端的“范围”滑块。\n使用 dateInput() 收集一天或使用 dateRangeInput() 收集两天的范围。这些提供了一个方便的日历选择器，并且诸如 datesdisabled 和 daysofweekdisabled 之类的附加参数允许您限制有效输入的集合。\n有两种不同的方法允许用户从一组预先指定的选项中进行选择：selectInput()(还可以设置 multiple = TRUE 以允许用户选择多个元素)和 radioButtons(); 可以使用checkboxGroupInput()形成多选。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ui \u0026lt;- fluidPage( textInput(\u0026#34;name\u0026#34;, \u0026#34;What\u0026#39;s your name?\u0026#34;), passwordInput(\u0026#34;password\u0026#34;, \u0026#34;What\u0026#39;s your password?\u0026#34;), textAreaInput(\u0026#34;story\u0026#34;, \u0026#34;Tell me about yourself\u0026#34;, rows = 3) ) ui \u0026lt;- fluidPage( numericInput(\u0026#34;num\u0026#34;, \u0026#34;Number one\u0026#34;, value = 0, min = 0, max = 100), sliderInput(\u0026#34;num2\u0026#34;, \u0026#34;Number two\u0026#34;, value = 50, min = 0, max = 100), sliderInput(\u0026#34;rng\u0026#34;, \u0026#34;Range\u0026#34;, value = c(10, 20), min = 0, max = 100) ) ui \u0026lt;- fluidPage( dateInput(\u0026#34;dob\u0026#34;, \u0026#34;When were you born?\u0026#34;), dateRangeInput(\u0026#34;holiday\u0026#34;, \u0026#34;When do you want to go on vacation next?\u0026#34;) ) animals \u0026lt;- c(\u0026#34;dog\u0026#34;, \u0026#34;cat\u0026#34;, \u0026#34;mouse\u0026#34;, \u0026#34;bird\u0026#34;, \u0026#34;other\u0026#34;, \u0026#34;I hate animals\u0026#34;) ui \u0026lt;- fluidPage( selectInput(\u0026#34;state\u0026#34;, \u0026#34;What\u0026#39;s your favourite state?\u0026#34;, state.name), radioButtons(\u0026#34;animal\u0026#34;, \u0026#34;What\u0026#39;s your favourite animal?\u0026#34;, animals), checkboxGroupInput(\u0026#34;animal\u0026#34;, \u0026#34;What animals do you like?\u0026#34;, animals) ) 让用户使用 actionButton() 或 actionLink() 执行操作，可以使用“btn-primary”、“btn-success”、“btn-info”、“btn-warning”或“btn-danger”之一使用类参数自定义外观，使用“btn-lg”、“btn-sm”、“btn-xs”更改大小，可以使用“btn-block”使按钮跨越它们嵌入的元素的整个宽度。\n1 2 3 4 5 6 7 8 9 ui \u0026lt;- fluidPage( fluidRow( actionButton(\u0026#34;click\u0026#34;, \u0026#34;Click me!\u0026#34;, class = \u0026#34;btn-danger\u0026#34;), actionButton(\u0026#34;drink\u0026#34;, \u0026#34;Drink me!\u0026#34;, class = \u0026#34;btn-lg btn-success\u0026#34;) ), fluidRow( actionButton(\u0026#34;eat\u0026#34;, \u0026#34;Eat me!\u0026#34;, class = \u0026#34;btn-block\u0026#34;) ) ) 连接sever general 添加输出对象在ui中 Shiny 提供了一系列函数，可以将 R 对象转换为用户界面的输出。每个函数创建特定类型的输出。\nTable 3: output R objects family Output function Creates dataTableOutput DataTable htmlOutput raw HTML imageOutput image plotOutput plot tableOutput table textOutput text uiOutput raw HTML verbatimTextOutput text 编写构建R对象的代码在serve中 Table 4: render R objects family render function creates renderDataTable DataTable renderImage images (saved as a link to a source file) renderPlot plots renderPrint any printed output renderTable data frame, matrix, other table like structures renderText character strings renderUI a Shiny tag object or HTML 使用input，output连接 以下代码就可以将selectInput选择的值var通过input$var的形式传递到renderText()，再render成output$selected_var的形式，最后通过textOutput()输出到ui界面。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 library(shiny) ui \u0026lt;- fluidPage( titlePanel(\u0026#34;censusVis\u0026#34;), sidebarLayout( sidebarPanel( selectInput(\u0026#34;var\u0026#34;, label = \u0026#34;Choose a variable to display\u0026#34;, choices = c(\u0026#34;Percent White\u0026#34;, \u0026#34;Percent Black\u0026#34;, \u0026#34;Percent Hispanic\u0026#34;, \u0026#34;Percent Asian\u0026#34;), selected = \u0026#34;Percent White\u0026#34;) ), mainPanel( textOutput(\u0026#34;selected_var\u0026#34;) ) ) ) server \u0026lt;- function(input, output) { output$selected_var \u0026lt;- renderText({ paste(\u0026#34;You have selected\u0026#34;, input$var) }) } shinyApp(ui, server) 更多：\n使用 textOutput() 输出常规文本，使用 verbatimTextOutput() 输出固定代码和控制台输出。 renderText() 将结果组合成一个字符串，通常与 textOutput() 配对 renderPrint() 打印结果，就像您在 R 控制台中一样，并且通常与 verbatimTextOutput() 配对。\ntableOutput() 和 renderTable() 呈现静态数据表，同时显示所有数据。 dataTableOutput() 和 renderDataTable() 呈现一个动态表，显示固定数量的行以及用于更改哪些行可见的控件。 tableOutput() 对于小型、固定的摘要（例如模型系数）最有用；如果您想向用户公开完整的数据框，则 dataTableOutput() 是最合适的。\n默认情况下，plotOutput() 将占据其容器的整个宽度（稍后会详细介绍），并且高度为 400 像素。您可以使用高度和宽度参数覆盖这些默认值。我们建议始终设置 res = 96，因为这将使您的 Shiny 图与您在 RStudio 中看到的尽可能接近。\nreactive expressions This difference between commands and recipes is one of the key differences between two important styles of programming:\nIn imperative programming, you issue a specific command and it’s carried out immediately. This is the style of programming you’re used to in your analysis scripts: you command R to load your data, transform it, visualise it, and save the results to disk.\nIn declarative programming, you express higher-level goals or describe important constraints, and rely on someone else to decide how and/or when to translate that into action. This is the style of programming you use in Shiny.\nShiny 中声明式编程的优势之一是它允许应用程序非常懒惰。 Shiny 应用程序只会执行更新您当前可以看到的输出控件所需的最少工作量，优点是快速，缺点是如果你不运行所有代码，可能不会发现错误。\n反应式表达式比常规 R 函数更聪明。它们缓存值并知道它们的值何时会变。第一次运行反应式表达式时，表达式会将其结果保存在计算机的内存中。下次调用反应式表达式时，它可以返回保存的结果而不进行任何计算（这将使您的应用程序更快）。\n如果反应式表达式知道结果是最新的，它只会返回保存的结果。如果反应式表达式得知结果已过时（因为小部件已更改），则表达式将重新计算结果。然后它返回新结果并保存一个新副本。反应式表达式将使用这个新副本，直到它也变得过时为止。 让我们总结一下这种行为：\n反应式表达式会在您第一次运行时保存其结果。\n下次调用反应式表达式时，它会检查保存的值是否已过时（即，它所依赖的小部件是否已更改）。\n如果该值已过期，反应对象将重新计算它（然后保存新结果）。\n如果该值是最新的，反应式表达式将返回保存的值而不进行任何计算。\n建议把文件导入等不需要每次更新的代码放在reactive里。\n在 Shiny 中，应该考虑一个规则：每当复制和粘贴一次东西时，就应该考虑将重复的代码提取到一个反应表达式中，因为反应式表达式不仅让人类更容易理解代码，它们还提高了 Shiny 高效重新运行代码的能力。\n考虑以下代码，reactive()确保只有在改变n或lambda时才会重新计算：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 library(ggplot2) freqpoly \u0026lt;- function(x1, x2, binwidth = 0.1, xlim = c(-3, 3)) { df \u0026lt;- data.frame( x = c(x1, x2), g = c(rep(\u0026#34;x1\u0026#34;, length(x1)), rep(\u0026#34;x2\u0026#34;, length(x2))) ) ggplot(df, aes(x, colour = g)) + geom_freqpoly(binwidth = binwidth, size = 1) + coord_cartesian(xlim = xlim) } ui \u0026lt;- fluidPage( fluidRow( column(3, numericInput(\u0026#34;lambda1\u0026#34;, label = \u0026#34;lambda1\u0026#34;, value = 3), numericInput(\u0026#34;lambda2\u0026#34;, label = \u0026#34;lambda2\u0026#34;, value = 5), numericInput(\u0026#34;n\u0026#34;, label = \u0026#34;n\u0026#34;, value = 1e4, min = 0) ), column(9, plotOutput(\u0026#34;hist\u0026#34;)) ) ) server \u0026lt;- function(input, output, session) { x1 \u0026lt;- reactive(rpois(input$n, input$lambda1)) x2 \u0026lt;- reactive(rpois(input$n, input$lambda2)) output$hist \u0026lt;- renderPlot({ freqpoly(x1(), x2(), binwidth = 1, xlim = c(0, 40)) }, res = 96) } shinyApp(ui,server) 控制reactive行为：\n计时器 reactiveTimer() 是一个响应式表达式，它依赖于隐藏的输入：当前时间。 将server里的reactive修改成下列形式可以让图形每500ms刷新一次。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 server \u0026lt;- function(input, output, session) { timer \u0026lt;- reactiveTimer(500) x1 \u0026lt;- reactive({ timer() rpois(input$n, input$lambda1) }) x2 \u0026lt;- reactive({ timer() rpois(input$n, input$lambda2) }) output$hist \u0026lt;- renderPlot({ freqpoly(x1(), x2(), binwidth = 1, xlim = c(0, 40)) }, res = 96) } 点击刷新 当你的serve运行一次需要庞大计算和时间时，可能希望要求用户通过单击按钮来选择执行昂贵的计算。这是 actionButton() 的一个很好的用例： 并且我们需要 eventReactive()，它有两个参数：第一个参数指定依赖什么，第二个参数指定计算什么。\n修改ui和serve，添加了按键，用户点击按键即可出现新的模拟结果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ui \u0026lt;- fluidPage( fluidRow( column(3, numericInput(\u0026#34;lambda1\u0026#34;, label = \u0026#34;lambda1\u0026#34;, value = 3), numericInput(\u0026#34;lambda2\u0026#34;, label = \u0026#34;lambda2\u0026#34;, value = 5), numericInput(\u0026#34;n\u0026#34;, label = \u0026#34;n\u0026#34;, value = 1e4, min = 0), actionButton(\u0026#34;simulate\u0026#34;, \u0026#34;Simulate!\u0026#34;) ), column(9, plotOutput(\u0026#34;hist\u0026#34;)) ) ) server \u0026lt;- function(input, output, session) { x1 \u0026lt;- eventReactive(input$simulate, { rpois(input$n, input$lambda1) }) x2 \u0026lt;- eventReactive(input$simulate, { rpois(input$n, input$lambda2) }) output$hist \u0026lt;- renderPlot({ freqpoly(x1(), x2(), binwidth = 1, xlim = c(0, 40)) }, res = 96) } observeEvent() 与 eventReactive() 非常相似。它有两个重要的参数：eventExpr 和 handlerExpr。第一个参数是要依赖的输入或表达式；第二个参数是将要运行的代码。例如，对 server() 的以下修改意味着每次更新该名称时，都会向控制台发送一条消息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 ui \u0026lt;- fluidPage( textInput(\u0026#34;name\u0026#34;, \u0026#34;What\u0026#39;s your name?\u0026#34;), textOutput(\u0026#34;greeting\u0026#34;) ) server \u0026lt;- function(input, output, session) { string \u0026lt;- reactive(paste0(\u0026#34;Hello \u0026#34;, input$name, \u0026#34;!\u0026#34;)) output$greeting \u0026lt;- renderText(string()) observeEvent(input$name, { message(\u0026#34;Greeting performed\u0026#34;) }) } file up/download 使用fileInput在ui中上传文件后，得到的input是一个列表，其中的datapath是文件上传后的路径，需要使用read.csv等函数读取file$datapath。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ui \u0026lt;- fluidPage( sidebarLayout( sidebarPanel( fileInput(\u0026#34;file1\u0026#34;, \u0026#34;Choose CSV File\u0026#34;, accept = \u0026#34;.csv\u0026#34;), checkboxInput(\u0026#34;header\u0026#34;, \u0026#34;Header\u0026#34;, TRUE) ), mainPanel( tableOutput(\u0026#34;contents\u0026#34;) ) ) ) server \u0026lt;- function(input, output) { output$contents \u0026lt;- renderTable({ file \u0026lt;- input$file1 ext \u0026lt;- tools::file_ext(file$datapath) req(file) validate(need(ext == \u0026#34;csv\u0026#34;, \u0026#34;Please upload a csv file\u0026#34;)) read.csv(file$datapath, header = input$header) }) } shinyApp(ui, server) 您可以让用户使用 downloadButton() 或 downloadLink() 下载文件。这些都需要服务器功能中的新技术，因此我们将在第 9 章中回过头来讨论。\nShare 文件形式 任何拥有 R 的人都可以运行Shiny 应用程序，分享你的app.R 文件副本，以及您的应用程序中使用的任何补充材料（例如，www 文件夹或 helpers.R 文件）即可，最好写上代码运行的依赖包安装代码。\nrunUrl() will download and launch a Shiny app straight from a weblink. runGitHub( \u0026quot;\u0026lt;your repository name\u0026gt;\u0026quot;, \u0026quot;\u0026lt;your user name\u0026gt;\u0026quot;) 网页形式 上述方法要求用户在他们的计算机上安装 R 和 Shiny。但如果我们自己搭建好了服务器，也可以直接用浏览器使用我们的APP。\nShinyapps.io 将 Shiny 应用程序转换为网页的最简单方法是使用 shinyapps.io，这是 RStudio 为 Shiny 应用程序提供的托管服务。 2. Shiny Server\nRStudio Connect Reference 官方教程：Shiny Learning Resources\n参考书：Mastering Shiny\n","date":"2023-03-27T00:00:00Z","image":"/p/rshiny/Rshiny_hu0864f99837a84b3a0d6c26cdeddf969b_6899_120x120_fill_q75_box_smart1.jpg","permalink":"/p/rshiny/","title":"开发我的Rshiny应用"},{"content":"Introduction 宏基因组（Metagenome）是指对一个生态系统中的所有微生物进行DNA分析的过程，可以帮助研究人员了解微生物的多样性、功能和互作关系。\n宏基因组的应用非常广泛，包括：\n生物多样性研究：通过对宏基因组进行分析，可以了解不同生态系统中微生物的多样性和分布情况。\n生态学研究：宏基因组可以帮助研究人员了解微生物在生态系统中的功能、互作关系和生态位等。\n生物技术：宏基因组可以用于筛选具有特定功能的微生物，例如，寻找能够降解有害物质的微生物。\n宏基因组的分析一般包括以下步骤：\nDNA提取与建库。\n高通量测序：使用高通量测序技术对扩增后的DNA进行测序，得到原始序列数据。\n数据清洗和组装：对原始数据进行质量控制、去除低质量序列和冗余序列，将序列拼接成较长的连续序列（contigs）。\n基因注释：将contigs中的基因进行注释，得到基因功能信息。\n数据分析：了解微生物多样性、群落结构、功能特征等信息（更多是指获取了物种丰度表或功能丰度表之后的进一步分析）。\nMAGs binning， 进化动态等进一步分析\n这是我常用的一套基本流程(Figure 1)，当然在面对不同项目时应该有不同的侧重点和适用的分析方法，可以在此基础上添加或修改。\n最早这方面的分析我都是参考刘永鑫老师的EasyMetagenome,现在这套流程也发文章了 (1)，值得参考，对上手16S测序数据或宏基因组数据都很有帮助。\nFigure 1: Basic workflow preprocess 绝大多数这里介绍的软件都是仅支持linux平台的，我们做测序文件的上游分析也肯定是在服务器上做，个人PC一般很难满足需求，所以在做这些分析前必须先学习linux基础知识如文件系统，shell脚本编写，软件安装等。\n安装软件建议使用conda或mamba（新建环境和管理），有很多参考方法。\n我们服务器使用的是slurm作业管理系统，尽量先学习一下slurm的使用再尝试提交作业。\n一般把所有样本的测序双端文件放在一个文件夹下\n质控：fastp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #!/bin/bash #SBATCH --job-name=fastp #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/fastp/log/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/fastp/log/%x_%a.err #SBATCH --array=1-33 #SBATCH --partition=short #SBATCH --cpus-per-task=8 echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` echo \u0026#34;SLURM_ARRAY_TASK_ID: \u0026#34; $SLURM_ARRAY_TASK_ID sample=$(head -n $SLURM_ARRAY_TASK_ID ~/work/asthma/data/namelist | tail -1) #sample=$(head -n 1 namelist | tail -1) echo handling: $sample #################### fastp -w 8 -i ~/work/asthma/data/$sample/$sample\u0026#39;_f1.fastq\u0026#39; -o ${sample}_1 \\ -I ~/work/asthma/data/$sample/$sample\u0026#39;_r2.fastq\u0026#39; -O ${sample}_2 -j ~/work/asthma/fastp/${i}.json #################### echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s 后面接一个python脚本就可以统计常用指标了。\n把所有的.json文件移到一个文件夹里，report/下，就可以统计了。\n去宿主：bowtie2 其实就是将序列比对到人类基因组上，没有比对到的序列整合成新文件就是去宿主后的了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #!/bin/bash #SBATCH --job-name=rm_human #SBATCH --output=/share/home/jianglab/pengchen/work/meta/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/meta/%x_%a.err #SBATCH --cpus-per-task=32 #SBATCH --partition=short echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` ############# for i in C1 C2 do bowtie2 -p 32 -x ~/db/humangenome/hg38 -1 seq/${i}_1.fq.gz \\ -2 seq/${i}_2.fq.gz -S ${i}.sam --un-conc ${i}.fq --very-sensitive done ############## echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s 基本信息统计 可以用FastqCount：\n1 2 3 4 ~/biosoft/FastqCount-master/FastqCount_v0.5 xx.fastq.gz Total Reads Total Bases N Bases Q20 Q30 GC 11568822 (11.57 M) 1702829127 (1.70 G) 0.00% 98.00% 94.00% 54.00% reads-based 物种注释：kraken2 Kraken 2是一个用于对高通量测序数据进行分类和标识物种的软件。它使用参考数据库中的基因组序列来进行分类，并使用k-mer方法来实现快速和准确的分类。\n使用Kraken 2进行基本分类的简单步骤：\n准备参考数据库：Kraken 2需要一个参考数据库，以便对测序数据进行分类。可以从NCBI、Ensembl或其他数据库下载相应的基因组序列，并使用Kraken 2内置的工具来构建数据库。\n安装Kraken 2：可以从Kraken 2官方网站下载并安装Kraken 2软件。\n运行Kraken 2：使用Kraken 2对测序数据进行分类需要使用以下命令：\nkraken2 \\--db \\\u0026lt;path_to_database\\\u0026gt; \\\u0026lt;input_file\\\u0026gt; \\--output \\\u0026lt;output_file\\\u0026gt;\n这里，**\u0026lt;path_to_database\u0026gt;是参考数据库的路径，\u0026lt;input_file\u0026gt;是需要进行分类的输入文件，\u0026lt;output_file\u0026gt;**是输出文件的名称。Kraken 2将输出一个分类报告文件和一个序列文件。\nkraken2-build --standard --threads 24 --db ./\n\u0026ndash;standard标准模式下只下载5种数据库：古菌archaea、细菌bacteria、人类human、载体UniVec_Core、病毒viral。也可选直接下载作者构建的索引，还包括bracken的索引。\n这个kraken数据库是可以自己构建的，所以适用于各种项目的物种注释，我做的比较多的是环境样本的宏基因组，就可能需要更全面的物种数据库（甚至除了各种微生物，还要动植物数据等），实验室的WX师姐收集构建了一个超大的物种库。\n需要注意的是kraken运行至少要提供数据库大小的内存大小（运行内存），因为它会把整个数据库载入内存后进行序列的注释，所以如果发现无法载入数据库的报错，可以尝试调大内存资源。\nkraken软件运行时载入数据库是一个十分耗时的步骤，而每条序列的鉴定时间差不多，所以我们可以将很多样本的fastq文件合并成一个大文件后输入kraken注释，之后再按照序列的数量拆分结果文件，这样多个样本也只需要载入一次数据库，节省时间。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #!/bin/bash #SBATCH --job-name=kraken2M #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/kraken/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/kraken/%x_%a.err #SBATCH --time=14-00:00:00 #SBATCH --partition=mem #SBATCH --cpus-per-task=32 #SBATCH --mem-per-cpu=100G fqp=~/work/asthma/data/CRR205159/ python /share/home/jianglab/shared/krakenDB/K2ols/kraken2M.py -t 32 \\ -i ${fqp} \\ -c 0.05 \\ -s _f1.fastq,_r2.fastq \\ -o ~/work/asthma/kraken/ \\ -d /share/home/jianglab/shared/krakenDB/mydb2 \\ -k ~/miniconda3/envs/waste/bin/kraken2 \\ -kt /share/home/jianglab/shared/krakenDB/K2ols/KrakenTools 输出文件格式 Kraken标准输出格式\n五列表 output\nC/U代表分类classified或非分类unclassifed\n序列ID\n物种注释\n比序列注释的区域，如98|94代表左端98bp，右端94bp比对至数据库\nLCA比对结果，如”562:13 561:4”代表13 k-mer比对至物种#562，4 k-mer比对至#561物种\n报告输出格式 report\n包括6列，方便整理下游分析。\n百分比\ncount\ncount最优\n(U)nclassified, (R)oot, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. “G2”代表位于属一种间\nNCBI物种ID\n科学物种名\n常用的物种丰度表格式除了kraken report，还有mpa，spf，krona等格式，关于kraken结果的整理以及格式转换方式，有一些现成的脚本或者自己写。\nKrakenTools (jhu.edu) 就是一套很好用的kraken工具包，其中常用的有：\nextract_kraken_reads.py 此程序提取读取在任何用户指定的分类id处分类的内容。用户必须指定Kraken输出文件、序列文件和至少一个分类法ID。下面指定了其他选项。截至2021年4月19日，此脚本与KrakenUniq/Kraken2Uniq报告兼容。\ncombine_kreports.py This script combines multiple Kraken reports into a combined report file.\npython combine_kreports.py\n-r 1.KREPORT 2.KREPORT\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;Kraken-style reports to combine\n-o COMBINED.KREPORT\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;Output file\nkreport2krona.py This program takes a Kraken report file and prints out a krona-compatible TEXT file\n换成krona文件好画图。嘿嘿\nkrona装了一个excel的插件可以很容易画图\npython kreport2krona.py\n-r/\u0026ndash;report MYFILE.KREPORT\u0026hellip;.….Kraken report file\n-o/\u0026ndash;output MYFILE.KRONA\u0026hellip;\u0026hellip;.…Output Krona text file\nthen, ktImportText MYSAMPLE.krona -o MYSAMPLE.krona.html\n好看的网页就出来了。\nkreport2mpa.py This program takes a Kraken report file and prints out a mpa (MetaPhlAn) -style TEXT file\npython kreport2mpa.py\n-r/\u0026ndash;report MYFILE.KREPORT\u0026hellip;.….Kraken report file\n-o/\u0026ndash;output MYFILE.MPA.TXT\u0026hellip;.….Output MPA-STYLE text file\ncombine_mpa.py python combine_mpa.py\n-i/\u0026ndash;input MYFILE1.MPA MYFILE2.MPA\u0026hellip;.…Multiple MPA-STYLE text files (separated by spaces)\n-o/\u0026ndash;output MYFILE.COMBINED.MPA\u0026hellip;\u0026hellip;.…Output MPA-STYLE text file\nThis program combines multiple outputs from kreport2mpa.py. Files to be combined must have been generated using the same kreport2mpa.py options.\npython combine_mpa.py -i -o --intermediate-ranks\nHUMAnN HUMAnN2（The HMP Unified Metabolic Analysis Network 2）是一款用于分析人类微生物组的功能和代谢能力的工具。它通过将宏基因组序列与参考基因组数据库比对，利用MetaCyc代谢通路数据库和UniRef蛋白质序列数据库，分析微生物组在功能和代谢通路水平上的组成和活性。HUMAnN2还提供了多样性分析、关联分析和可视化工具，可用于深入研究人类微生物组对宿主健康的影响和治疗策略的制定等方面。\nHUMAnN2是由美国国家人类微生物组计划（HMP）开发的，目前最新版本为HUMAnN3，于2020年发布。与HUMAnN2相比，HUMAnN3改进了基因家族注释的方法，提高了注释精度和速度，并提供了新的功能和工具，如功能韧度分析、代谢指纹识别和多样性分析等。\n但是HUMAnN2的数据库基本都是与人相关的微生物，比较适合做各种人体微生物组（肠道，肺部，口腔，皮肤等等），对于环境样本可能unclassified比较多。\nHUMAnN2要求双端序列合并的文件作为输入，for循环根据实验设计样本名批量双端序列合并。\n物种组成调用MetaPhlAn2, bowtie2比对至核酸序列，解决有哪些微生物存在的问题；\n功能组成为humann2调用diamond比对至蛋白库11Gb，解决这些微生物参与哪些功能通路的问题；\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 cd alldata for i in `cat ~/work/asthma/data/namelist` do echo $i cat ${i}_f1.fastq ${i}_r2.fastq \u0026gt;${i}_paired.fastq done #!/bin/bash #SBATCH --job-name=humann2 #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/humann/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/humann/%x_%a.err #SBATCH --array=1-32 #SBATCH --cpus-per-task=24 #SBATCH --partition=cpu echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` ############## myarray=(`cat ~/work/asthma/data/namelist`) echo $SLURM_ARRAY_TASK_ID #this is your single file name sample=${myarray[${SLURM_ARRAY_TASK_ID}]} echo handling: $sample humann2 --input data/alldata/${sample}_paired.fastq \\ --output temp/humann2/ --threads 24 ## 链接重要文件至humann2目录 ln temp/humann2/${sample}_paired_humann2_temp/${sample}_paired_metaphlan_bugs_list.tsv temp/humann2/ ## 删除临时文件 rm -rf temp/humann2/${sample}_paired_humann2_temp ############## echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s ## 合并、修正样本名、预览 merge_metaphlan_tables2.py \\ temp/humann2/*_metaphlan_bugs_list.tsv | \\ sed \u0026#39;s/_metaphlan_bugs_list//g\u0026#39; \\ \u0026gt; metaphlan2/taxonomy.tsv contigs-based 组装：megahit MegaHit是一个用于对高通量测序数据进行de novo组装的软件。它使用了一种基于短读比对和图形构建的算法来组装基因组，可以高效地处理大规模的数据集。以下是MegaHit的一些优点和适用情况：\n速度快：MegaHit的算法非常高效，可以处理大规模的数据集，通常比其他de novo组装工具更快。\n高质量的组装：MegaHit在组装结果的连通性和准确性方面表现优异，尤其在处理高GC含量基因组时效果显著。\n适用于不同类型的测序数据：MegaHit支持多种不同类型的测序数据，包括 Illumina HiSeq/MiSeq、IonTorrent和PacBio等平台。\n易于使用：MegaHit具有简单的命令行语法，方便用户进行组装操作，且具有中断点，避免失败后全部重跑。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #!/bin/bash #SBATCH --job-name=asthma_megahit #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/megahit/log/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/megahit/log/%x_%a.err #SBATCH --array=1-33 #SBATCH --partition=cpu #SBATCH --cpus-per-task=32 echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` echo \u0026#34;SLURM_ARRAY_TASK_ID: \u0026#34; $SLURM_ARRAY_TASK_ID sample=$(head -n $SLURM_ARRAY_TASK_ID ~/work/asthma/data/namelist | tail -1) #sample=$(head -n 1 namelist | tail -1) echo handling: $sample\t#################### megahit -t 32 -1 ~/work/asthma/data/$sample/$sample\u0026#39;_f1.fastq\u0026#39; \\ -2 ~/work/asthma/data/$sample/$sample\u0026#39;_r2.fastq\u0026#39; -o ~/work/asthma/megahit/$sample --out-prefix $sample #################### echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s 组装评估：QUAST QUAST代表质量评估工具。 QUAST可以使用参考基因组以及不使用参考基因组来评估装配。 QUAST生成详细的报告，表格和图解，以显示装配的不同方面。\n基因预测：Prodigal 输入文件：拼装好的序列文件 megahit/final.contigs.fa\n输出文件：prodigal预测的基因序列 prodigal/gene.fa\nprodigal不支持多线程运行，所以我们可以自行分割序列文件调用多个prodigal程序分别跑实现伪多线程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #!/bin/bash #SBATCH --job-name=prodigal #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/prodigal/log/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/prodigal/log/%x_%a.err #SBATCH --array=1-33 #SBATCH --partition=cpu #SBATCH --cpus-per-task=1 echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` echo \u0026#34;SLURM_ARRAY_TASK_ID: \u0026#34; $SLURM_ARRAY_TASK_ID sample=$(head -n $SLURM_ARRAY_TASK_ID ~/work/asthma/data/namelist | tail -1) #sample=$(head -n 1 namelist | tail -1) echo handling: $sample #################### prodigal -i ~/work/asthma/megahit/contigs/$sample.fa \\ -d ~/work/asthma/prodigal/$sample.gene.fa \\ -o ~/work/asthma/prodigal/$sample.gene.gff \\ -p meta -f gff grep \u0026#39;partial=00\u0026#39; ~/work/asthma/prodigal/$sample.gene.fa | cut -f1 -d \u0026#39; \u0026#39;| sed \u0026#39;s/\u0026gt;//\u0026#39; \u0026gt; ~/work/asthma/prodigal/$sample.fullid seqkit grep -f ~/work/asthma/prodigal/$sample.fullid ~/work/asthma/prodigal/$sample.gene.fa \u0026gt; ~/work/asthma/prodigal/fullgene/$sample.gene.fa #################### echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s ============================================================================ ## 统计基因数量 grep -c \u0026#39;\u0026gt;\u0026#39; temp/prodigal/gene.fa ## 统计完整基因数量，数据量大可只用完整基因部分 grep -c \u0026#39;partial=00\u0026#39; temp/prodigal/gene.fa ## 提取完整基因(完整片段获得的基因全为完整，如成环的细菌基因组) grep \u0026#39;partial=00\u0026#39; temp/prodigal/gene.fa | cut -f1 -d \u0026#39; \u0026#39;| sed \u0026#39;s/\u0026gt;//\u0026#39; \u0026gt; temp/prodigal/full_length.id seqkit grep -f temp/prodigal/full_length.id temp/prodigal/gene.fa \u0026gt; temp/prodigal/full_length.fa seqkit stat temp/prodigal/full_length.fa 去冗余 上面产生了n个样本的基因预测结果文件，gene.fa文件要想办法整合为一个文件再去去冗余。\n1 2 3 4 5 6 7 8 9 10 #!/bin/bash #修改每条序列的名称，加上样本名 for i in `cat ~/work/asthma/data/namelist` do echo $i sed -i \u0026#34;/\u0026gt;/s/\u0026gt;/\u0026gt;${i}_/\u0026#34; $i.gene.fa done echo \u0026#39;start merge\u0026#39; cat *.gene.fa\u0026gt;all.fullgene.fa echo \u0026#39;done\u0026#39; Cd-hit 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #!/bin/bash #SBATCH --job-name=cdhit #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/%x_%a.err #SBATCH --cpus-per-task=32 #SBATCH --partition=short echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` ############## ## 输入文件：prodigal预测的基因序列 all.fullgene.fa ## 输出文件：去冗余后的基因和蛋白序列：NR/nucleotide.fa;NR/protein.fa mkdir NR ## aS覆盖度，c相似度，G局部比对，g最优解，T多线程，M内存0不限制 ## 2万基因2m，2千万需要2000h，多线程可加速\tcd-hit-est -i prodigal/fullgene/all.fullgene.fa \\ -o NR/nucleotide.fa \\ -aS 0.9 -c 0.9 -G 0 -g 0 -T 0 -M 0 ## 统计非冗余基因数量，单次拼接结果数量下降不大，多批拼接冗余度高 echo \u0026#39;after remove, the number of genes: \u0026#39; grep -c \u0026#39;\u0026gt;\u0026#39; NR/nucleotide.fa ## 翻译核酸为对应蛋白序列，emboss ## emboss transeq工具，93.9 MB conda install emboss -y transeq -sequence NR/nucleotide.fa \\ -outseq NR/protein.fa -trim Y ## 序列名自动添加了_1，为与核酸对应要去除 sed -i \u0026#39;s/_1 / /\u0026#39; NR/protein.fa ############## echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s mmseq2 这个要比cd-hit快非常多\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #mmseqs2 ##min-seq-id:identity，c覆盖度，rescore-mode3: global alignment #聚类 input_fa=tmp_com.gene mmseqs easy-linclust $input_fa lin_res tmp \\ --min-seq-id 0.9 -c 0.9 --cov-mode 1 --threads 8 #或者 mmseqs easy-cluster $input_fa lin_res tmp \\ --min-seq-id 0.9 -c 0.9 --cov-mode 1 --threads 8 ##以下是为了理解步骤做的，使用上面👆的easy模式即可 #建库 input_fa=../prodigal/C1.gene.fa DB=C1.geneDB DB_clu=mmseq_out mmseqs createdb $input_fa $DB #聚类 mmseqs cluster $DB $DB_clu tmp \\ --min-seq-id 0.9 -c 0.9 --cov-mode 1 --threads 8 --rescore-mode 3 #输出转换 #第一列是代表序列id，第二列是成员序列id #mmseqs createtsv $DB $DB mmseq_out mmseq_out.tsv #mmseqs createseqfiledb $DB $DB_clu ${DB_clu}_seq #mmseqs result2flat $DB $DB ${DB_clu}_seq ${DB_clu}_seq.fasta #获取represent sequence mmseqs createsubdb $DB_clu $DB ${DB_clu}_rep mmseqs convert2fasta ${DB_clu}_rep ${DB_clu}_rep.fasta 基因定量：salmon 建立索引 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/bash #SBATCH --job-name=salmon-index #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/%x_%a.err #SBATCH --cpus-per-task=32 #SBATCH --partition=short echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` ############## mkdir -p temp/salmon ## 建索引, -t序列, -i 索引，10s salmon index \\ -t NR/nucleotide.fa \\ -p 32 \\ -i temp/salmon/index ############## echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s 对每个样本定量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #!/bin/bash #SBATCH --job-name=salmon #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/salmon/log/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/salmon/log/%x_%a.err #SBATCH --array=1-33 #SBATCH --partition=cpu #SBATCH --cpus-per-task=32 echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` echo \u0026#34;SLURM_ARRAY_TASK_ID: \u0026#34; $SLURM_ARRAY_TASK_ID sample=$(head -n $SLURM_ARRAY_TASK_ID ~/work/asthma/data/namelist | tail -1) #sample=$(head -n 1 namelist | tail -1) echo handling: $sample #################### ## 输入文件：去冗余后的基因和蛋白序列：NR/nucleotide.fa ## 输出文件：Salmon定量后的结果：salmon/gene.count;salmon/gene.TPM ## 定量，l文库类型自动选择，p线程，--meta宏基因组模式 salmon quant \\ -i temp/salmon/index -l A -p 32 --meta \\ -1 data/alldata/${sample}_f1.fastq \\ -2 data/alldata/${sample}_r2.fastq \\ -o temp/salmon/${sample}.quant #################### echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s 合并各样本结果 1 2 3 4 5 6 7 8 9 10 11 12 ## 合并 mkdir -p salmon salmon quantmerge \\ --quants temp/salmon/*.quant \\ -o salmon/gene.TPM salmon quantmerge \\ --quants temp/salmon/*.quant \\ --column NumReads -o salmon/gene.count sed -i \u0026#39;1 s/.quant//g\u0026#39; salmon/gene.* ## 预览结果表格 head -n3 salmon/gene.* 功能基因注释 上一步已经有了所有的基因和每个样本所有基因的read count定量结果，我们只需要对上一步的基因序列（或蛋白质序列）进行不同数据库的注释（很多软件都是用diamond比对，如果没有专用软件的数据库我们也可以自己用diamond比对），合并注释结果得到的就是功能丰度表。\ndiamond选择\u0026ndash;outfmt 6的输出结果和blastp一样：\n1. qseqid query sequence id 2. sseqid subject (e.g., reference genome) sequence id 3. pident percentage of identical matches 4. length alignment length 5. mismatch number of mismatches 6. gapopen number of gap openings 7. qstart start of alignment in query 8. qend end of alignment in query 9. sstart start of alignment in subject 10. send end of alignment in subject 11. evalue expect value 12. bitscore bit score 1 eggNOG(COG/KEGG/CAZy) EggNOG数据库收集了COG（Clusters of Orthologous Groups of proteins，直系同源蛋白簇）,构成每个COG的蛋白都是被假定为来自于一个祖先蛋白，因此是orthologs或者是paralogs。通过把所有完整基因组的编码蛋白一个一个的互相比较确定的。在考虑来自一个给定基因组的蛋白时，这种比较将给出每个其他基因组的一个最相似的蛋白（因此需要用完整的基因组来定义COG），这些基因的每一个都轮番地被考虑。如果在这些蛋白（或子集）之间一个相互的最佳匹配关系被发现，那么那些相互的最佳匹配将形成一个COG。这样，一个COG中的成员将与这个COG中的其他成员比起被比较的基因组中的其他蛋白更相像。\nEggNOG里面包含了GO，KEGG，CAZy等。\n1 2 3 4 5 6 7 8 9 ## 下载常用数据库，注意设置下载位置 mkdir -p ${db}/eggnog5 \u0026amp;\u0026amp; cd ${db}/eggnog5 ## -y默认同意，-f强制下载，eggnog.db.gz 7.9G+4.9G download_eggnog_data.py -y -f --data_dir ./ ## 下载方式2(可选)：链接直接下载 wget -c http://eggnog5.embl.de/download/emapperdb-5.0.0/eggnog.db.gz ## 7.9G wget -c http://eggnog5.embl.de/download/emapperdb-5.0.0/eggnog_proteins.dmnd.gz ## 4.9G gunzip *.gz 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #!/bin/bash #SBATCH --job-name=eggo #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/%x_%j.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/%x_%j.err #SBATCH --cpus-per-task=32 #SBATCH --partition=cpu echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` ############## #切换环境 ## diamond比对基因至eggNOG 5.0数据库, 1~9h，默认diamond 1e-3 mkdir -p temp/eggnog emapper.py --no_annot --no_file_comments --override \\ --data_dir ~/db/eggnog5 \\ -i NR/protein.fa \\ --cpu 32 -m diamond \\ -o temp/eggnog/protein ## 比对结果功能注释, 1h emapper.py \\ --annotate_hits_table temp/eggnog/protein.emapper.seed_orthologs \\ --data_dir ~/db/eggnog5 \\ --cpu 32 --no_file_comments --override \\ -o temp/eggnog/output ## 添表头, 1列为ID，9列KO，16列CAZy，21列COG，22列描述 sed \u0026#39;1 i Name\\tortholog\\tevalue\\tscore\\ttaxonomic\\tprotein\\tGO\\tEC\\tKO\\tPathway\\tModule\\tReaction\\trclass\\tBRITE\\tTC\\tCAZy\\tBiGG\\ttax_scope\\tOG\\tbestOG\\tCOG\\tdescription\u0026#39; \\ temp/eggnog/output.emapper.annotations \\ \u0026gt; temp/eggnog/output ############## echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s 2 碳水化合物dbCAN2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## dbCAN2 http://bcb.unl.edu/dbCAN2 ## 创建数据库存放目录并进入 mkdir -p ${db}/dbCAN2 \u0026amp;\u0026amp; cd ${db}/dbCAN2 ## 下载序列和描述 wget -c http://bcb.unl.edu/dbCAN2/download/CAZyDB.07312020.fa wget -c http://bcb.unl.edu/dbCAN2/download/Databases/CAZyDB.07302020.fam-activities.txt ## 备用数据库下载地址并解压 #wget -c http://210.75.224.110/db/dbcan2/CAZyDB.07312020.fa.gz #gunzip CAZyDB.07312020.fa.gz ## diamond建索引，800M，1m diamond --version ## 0.8.22/2.0.5 time diamond makedb \\ --in CAZyDB.07312020.fa \\ --db CAZyDB.07312020 1 2 3 4 5 6 7 8 ## 比对CAZy数据库, 用时2~18m; 加--sensitive更全但慢至1h mkdir -p temp/dbcan2 diamond blastp \\ --db ~/db/dbcan2/CAZyDB.07312020 \\ --query NR/protein.fa \\ --threads 64 -e 1e-5 --outfmt 6 \\ --max-target-seqs 1 --quiet \\ --out temp/dbcan2/gene_diamond.f6 3 ARGs（CARD） ## Github: https://github.com/arpcard/rgi\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #!/bin/bash #SBATCH --job-name=rgi #SBATCH --output=/share/home/jianglab/pengchen/work/asthma/%x_%a.out #SBATCH --error=/share/home/jianglab/pengchen/work/asthma/%x_%a.err #SBATCH --cpus-per-task=32 #SBATCH --partition=cpu echo start: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` start=`date +%s` ############## mkdir -p card rgi main --input_sequence ~/work/asthma/temp/protein.fa \\ --output_file card/protein \\ --input_type protein --num_threads 32 \\ --clean --alignment_tool DIAMOND ############## echo end: `date +\u0026#39;%Y-%m-%d %T\u0026#39;` end=`date +%s` echo TIME:`expr $end - $start`s 4 毒力因子VFDB 官网地址：http://www.mgc.ac.cn/VFs/ 在官网下载数据库时，带有setA 的库为VFDB数据库核心库(set A)，而setB为全库(setB), 其中setA仅包含经实验验证过的毒力基因，而setB则在setA的基础上增加了预测的毒力基因，选择好数据库后，直接用blast/diamond即可完成注释。\n1 2 3 4 5 6 7 mkdir -p temp/vfdb diamond blastp \\ --db ~/db/VFDB/VFDB_setB_pro \\ --query NR/protein.fa \\ --threads 32 -e 1e-5 --outfmt 6 \\ --max-target-seqs 1 --quiet \\ --out temp/vfdb/gene_diamond.f6 5 其他各种数据库 功能注释合并 写一个python脚本，将表1（基因-功能的对应表）与表2（基因丰度表）合并，即不同基因可能注释到相同功能，把它们的丰度加在一起得到新表3（功能丰度表）\nbinning 宏基因组binning是指将不同的序列集合（如metagenome序列集合）根据它们的物种归类到不同的bins中，以便进一步研究它们的组成和功能。这个过程可以将类似的序列组合在一起，形成代表不同物种或基因组的bins，以便进行后续分析，如物种注释、基因组组装等。\n以下是常用的宏基因组binning方法：\n基于聚类的方法：该方法使用序列聚类将相似序列分到同一个bin中。一般来说，聚类算法可分为两类：无监督聚类（如k-means、DBSCAN等）和有监督聚类（如CAMI、MyCC等）。\n基于组装的方法：该方法使用de novo组装来将相似序列组装成连续的序列，再根据这些序列的基因组信息来将其分类到不同的bins中。这种方法的优点是可以更好地处理重复序列，缺点是需要大量的计算资源和时间。\n基于分类器的方法：该方法使用机器学习分类器来将序列分配到不同的bins中。这种方法的优点是可以自动学习特征并在处理大规模数据时效率高，缺点是需要先建立一个分类器并进行训练。\n在进行宏基因组binning时，通常需要使用多个方法进行比较，以选择最适合数据集的方法。可以使用一些流行的工具来进行binning，如MetaBAT、MaxBin、CONCOCT和MEGAN等。这些工具通常包含各种binning方法，可以根据数据集和分析目的选择适合的方法。\n篇幅限制，具体的方法放在另一篇里面讲解吧。\nReference 1. Y.-X. Liu, Y. Qin, T. Chen, M. Lu, X. Qian, X. Guo, Y. Bai, A practical guide to amplicon and metagenomic analysis of microbiome data. Protein \u0026amp; Cell. 12, 315–330 (2021).\n","date":"2023-03-26T00:00:00Z","image":"/p/metagenomic-workflow/images/workflow_hu65ad38ebc366cd14800eee2145e15bd0_361464_120x120_fill_box_smart1_3.png","permalink":"/p/metagenomic-workflow/","title":"Metagenomic workflow"},{"content":"春分刚过，校园许多花儿都争相绽放。天气还是有点冷，但还是想跟群青同学一起逛逛紫金港，赏赏花，观观鸟。\n这个时候校园里最多的便是蔷薇科的花了，虽然我向来是”桃李杏梅樱，傻傻分不清”的😂，这次在群青同学的帮助下收集到了九宫格：菊花桃，碧桃，垂丝海棠，西府海棠，湖北海棠，大岛樱，日本晚樱，日本樱花，蓬蘽(疑似)\n日本樱花 大岛樱 日本晚樱 西府海棠 湖北海棠 垂丝海棠 菊花桃 碧桃 蓬蘽(疑似) 湖心岛出发，大片的二月兰，刚冒芽的水杉，偷偷绽放的水仙和蝴蝶花。\n二月兰 水仙 蝴蝶花 水杉 早樱 鸟儿大多还是老朋友，快步走的白鹡鸰，悠闲的黑水鸡和斑嘴鸭，大喜鹊和红嘴蓝鹊，一只歌声婉转的乌鸫。本来这次最想看到的就是罕见旅客黑翅长脚鹬的，但是这儿果然只是它们歇歇脚的地方，没能见到😭，偷浙大官微的美图，哈哈。南花园还是夜鹭与白鹭们的领地。\n白鹡鸰 黑水鸡 斑嘴鸭 喜鹊 夜鹭 乌鸫 黑翅长脚鹬 （浙江大学B站号） 红嘴蓝鹊 生物实验中心之前每年这个时候都是大片油菜花，打卡圣地，但今年的试验田搬到了银泉，这里居然种满了麦子，绿油油的麦田非常治愈。去年的油菜花应该还留下了一些种子，这个我叫做单双子叶的碰撞。\n单双子叶的碰撞 惊喜出现在麦田的拐角处，几只可爱的啾啾正在偷麦子吃，原来是白腰文鸟，上次见它也是在偷吃生科院的实验水稻。站在紫荆丛中的文鸟超有气质。\n其他一些瞬间，医学院的一排樱花，LY同学最赞的一株樱花，已经吃了四年还要再吃四年的麦香，晶莹的水珠…\n医学院 药学院 医图 麦香 感谢群青同学的摄影，越来越强了👍。下一个季节再来看看。\n","date":"2023-03-25T00:00:00Z","image":"/p/%E7%B4%AB%E9%87%91%E6%98%A5%E6%97%A5/images/%E8%A5%BF%E5%BA%9C_hu9cec70a3d2a230bf39151bcb5362fd23_124766_120x120_fill_q75_box_smart1.jpg","permalink":"/p/%E7%B4%AB%E9%87%91%E6%98%A5%E6%97%A5/","title":"紫金春日"},{"content":"Introduction 统计分析在生物信息学中具有非常重要的意义，因为生物信息学研究的数据量庞大、复杂性高，而统计分析可以帮助我们更好地理解和解释这些数据。下面是统计分析对生物信息学的几个重要意义：\n数据清洗和预处理：生物信息学研究中经常需要处理大规模的数据，而这些数据可能存在噪声、错误和缺失值等问题。统计分析可以帮助我们对数据进行清洗和预处理，以确保数据的质量和可靠性。 数据可视化：统计分析可以帮助我们将复杂的数据转化为可视化图形，从而更好地理解数据的分布、关系和趋势。这些图形可以帮助我们发现隐藏在数据中的模式和规律。 数据分析：生物信息学研究中需要对大量的数据进行分析，例如比较基因组学、转录组学、蛋白质组学等。统计分析可以帮助我们对数据进行建模和预测，从而深入探究生物学的复杂现象和机制。 数据挖掘：生物信息学研究中需要挖掘大量的数据来发现新的生物学现象和机制。统计分析可以帮助我们从数据中提取出有用的信息和知识，进而推动生物学的研究和发展。 R语言是一个专门用于数据分析和统计建模的编程语言，它有以下几个优点，使其成为做统计分析的理想选择：\n免费和开源：R语言是一个免费和开源的软件，可以在不付出额外成本的情况下使用和定制。这使得许多学生、学者和数据分析师选择R语言作为他们的首选统计分析工具。\n强大的数据处理能力：R语言具有强大的数据处理能力，支持多种数据结构和数据类型，可以轻松地进行数据清洗、整合、变换和分析。\n丰富的统计分析函数库：R语言具有丰富的统计分析函数库，包括线性回归、逻辑回归、聚类分析、主成分分析、时间序列分析等等。这些函数库提供了许多常用的统计分析方法，可以满足不同数据分析需求。\n图形可视化功能：R语言具有强大的图形可视化功能，可以轻松地创建各种类型的图表，包括散点图、条形图、折线图、热图等。这些图表可以帮助数据分析师更好地理解数据、发现规律和提取信息。\n社区支持和生态系统：R语言拥有庞大的用户社区和生态系统，用户可以轻松地找到并使用数千种可用的统计分析工具和R包，这些工具和R包可以帮助用户更加高效地完成统计分析任务。\n我想在这里稍微记录一下我使用R常用的一些初等统计分析方法，如回归，方差分析，广义线性模型等，主要参考资料是《R语言教程》 VII部分统计分析的内容。\nStatistics 基础用法 单样本均值检验 1 2 3 4 5 6 7 ?t.test() #install.packages(\u0026#34;ggstatsplot\u0026#34;,dependencies = T) library(ggstatsplot) #这个包会在画图的过程中计算很多统计量，帮我们更好地把握数据的性质 #比如我们想看tooth length均值是否和25有显著差异 t.test(ToothGrowth$len,mu = 25,alternative = \u0026#34;two.sided\u0026#34;) ## ## One Sample t-test ## ## data: ToothGrowth$len ## t = -6.2648, df = 59, p-value = 0.00000004681 ## alternative hypothesis: true mean is not equal to 25 ## 95 percent confidence interval: ## 16.83731 20.78936 ## sample estimates: ## mean of x ## 18.81333 1 2 3 4 5 6 gghistostats( data = ToothGrowth, x = len, xlab = \u0026#34;Tooth length\u0026#34;, test.value = 25 ) 检验的功效， 是指对立假设成立时检验拒绝$H_0$的概率$1-\\beta$， 其中$\\beta$是第二类错误， 即当对立假设成立时错误地接受$H_0$的概率。 需要足够大的样本量才能使得检验能够发现实际存在的显著差异。\n1 2 3 4 5 6 7 pwr::pwr.t.test( type = \u0026#34;one.sample\u0026#34;, alternative=\u0026#34;greater\u0026#34;, d = (7.25 - 7.0)/1.052, sig.level = 0.05, power = 0.80 ) |\u0026gt; plot() 均值比较 独立两样本t检验\n1 t.test(mpg~am,data = mtcars) ## ## Welch Two Sample t-test ## ## data: mpg by am ## t = -3.7671, df = 18.332, p-value = 0.001374 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -11.280194 -3.209684 ## sample estimates: ## mean in group 0 mean in group 1 ## 17.14737 24.39231 1 ggbetweenstats(mtcars, am, mpg) 比例检验 1 2 #抽查400个样本100个异常，异常比例是否显著大于0.2 prop.test(100, 400, p=0.20, alternative = \u0026#34;greater\u0026#34;) ## ## 1-sample proportions test with continuity correction ## ## data: 100 out of 400, null probability 0.2 ## X-squared = 5.9414, df = 1, p-value = 0.007395 ## alternative hypothesis: true p is greater than 0.2 ## 95 percent confidence interval: ## 0.2149649 1.0000000 ## sample estimates: ## p ## 0.25 1 2 #两次抽查的比例是否一致 prop.test(c(35,27), c(250,300), alternative = \u0026#34;two.sided\u0026#34;) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: c(35, 27) out of c(250, 300) ## X-squared = 2.9268, df = 1, p-value = 0.08712 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.007506845 0.107506845 ## sample estimates: ## prop 1 prop 2 ## 0.14 0.09 方差的假设检验 检查两组数据的方差有误显著差异？\n1 2 3 4 var.test(c( 20.5, 18.8, 19.8, 20.9, 21.5, 19.5, 21.0, 21.2), c( 17.7, 20.3, 20.0, 18.8, 19.0, 20.1, 20.2, 19.1), alternative = \u0026#34;two.sided\u0026#34;) ## ## F test to compare two variances ## ## data: c(20.5, 18.8, 19.8, 20.9, 21.5, 19.5, 21, 21.2) and c(17.7, 20.3, 20, 18.8, 19, 20.1, 20.2, 19.1) ## F = 1.069, num df = 7, denom df = 7, p-value = 0.9322 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.214011 5.339386 ## sample estimates: ## ratio of variances ## 1.068966 拟合优度检验 1 2 #6类face的count比例是否相等，卡方检验 chisq.test(c(168, 159, 168, 180, 167, 158)) ## ## Chi-squared test for given probabilities ## ## data: c(168, 159, 168, 180, 167, 158) ## X-squared = 1.892, df = 5, p-value = 0.8639 1 2 3 4 5 6 7 ggpiestats( data = data.frame(face=1:6, counts=c(168, 159, 168, 180, 167, 158)), x = face, counts = counts, title = \u0026#34;Dice equality\u0026#34; ) 1 2 #各类比例是否为指定值 chisq.test(c(48, 98, 54), p=c(0.3, 0.5, 0.2)) ## ## Chi-squared test for given probabilities ## ## data: c(48, 98, 54) ## X-squared = 7.34, df = 2, p-value = 0.02548 检验分布类型 vcd包提供了一个goodfit函数， 可以用来拟合指定的某种理论分布(包括泊松、二项、负二项分布）， 并检验服从该理论分布的零假设。\n1 2 3 set.seed(101) datax \u0026lt;- rpois(100, 2) summary(vcd::goodfit(datax, \u0026#34;poisson\u0026#34;)) ## ## Goodness-of-fit test for poisson distribution ## ## X^2 df P(\u0026gt; X^2) ## Likelihood Ratio 4.289456 5 0.5085374 独立性卡方检验 1 2 3 4 5 6 ctab.beer \u0026lt;- rbind(c( 20, 40, 20), c(30,30,10)) colnames(ctab.beer) \u0026lt;- c(\u0026#34;Light\u0026#34;, \u0026#34;Regular\u0026#34;, \u0026#34;Dark\u0026#34;) rownames(ctab.beer) \u0026lt;- c(\u0026#34;Male\u0026#34;, \u0026#34;Female\u0026#34;) addmargins(ctab.beer) ## Light Regular Dark Sum ## Male 20 40 20 80 ## Female 30 30 10 70 ## Sum 50 70 30 150 1 2 #列联表独立性检验： chisq.test(ctab.beer) ## ## Pearson's Chi-squared test ## ## data: ctab.beer ## X-squared = 6.1224, df = 2, p-value = 0.04683 1 #在0.05水平下认为啤酒类型偏好与性别有关 非参数检验 常用的有独立两样本比较的Wilcoxon秩和检验， 单样本的符号秩检验和符号检验等\n1 2 3 x \u0026lt;- c(0.80, 0.83, 1.89, 1.04, 1.45, 1.38, 1.91, 1.64, 0.73, 1.46) y \u0026lt;- c(1.15, 0.88, 0.90, 0.74, 1.21) wilcox.test(x,mu = 0.7) ## ## Wilcoxon signed rank exact test ## ## data: x ## V = 55, p-value = 0.001953 ## alternative hypothesis: true location is not equal to 0.7 1 wilcox.test(x, y, alternative = \u0026#34;g\u0026#34;) ## ## Wilcoxon rank sum exact test ## ## data: x and y ## W = 35, p-value = 0.1272 ## alternative hypothesis: true location shift is greater than 0 回归分析 相关分析 Pearson相关系数： $$ \\rho(X,Y)=\\frac{E[(X-E(X))(Y-E(Y))]}{\\sqrt{Var(X)Var(Y)}} $$ 相关系数绝对值在0.8以上认为高度相关。 在0.5到0.8之间认为中度相关。 在0.3到0.5之间认为低度相关。 在0.3以下认为不相关或相关性很弱以至于没有实际价值。 当然，在特别重要的问题中， 只要经过检验显著不等于零的相关都认为是有意义的。\n相关系数检验：\n检验统计量: $$ t=\\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}} $$ p值为：$P(|t(n-2)|\u0026gt;|t_0|)$\n1 2 3 4 5 6 7 set.seed(1) x \u0026lt;- runif(30, 0, 10) xx \u0026lt;- seq(0, 10, length.out = 100) y \u0026lt;- 40 - (x-7)^2 + rnorm(30) yy \u0026lt;- 40 - (xx-7)^2 plot(x, y, pch=16) lines(xx, yy) 1 cor(x,y,method = \u0026#34;pearson\u0026#34;) ## [1] 0.8244374 1 cor.test(x,y,method = \u0026#34;pearson\u0026#34;) ## ## Pearson's product-moment correlation ## ## data: x and y ## t = 7.7083, df = 28, p-value = 0.00000002136 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.6602859 0.9134070 ## sample estimates: ## cor ## 0.8244374 1 2 3 4 5 ggstatsplot::ggscatterstats( data = data.frame(x,y), x = x, y = y ) 相关性矩阵，`n*n`或`n*m`的$r$ 和$p-value$矩阵 1 2 3 ggstatsplot::ggcorrmat( data = mtcars ) 1 pcutils::cor_plot(mtcars) 1 corrplot::corrplot(cor(mtcars)) 1 ggcorrplot::ggcorrplot(cor(mtcars),method = \u0026#34;circle\u0026#34;) Spearman秩相关系数 Spearman rho系数， 是两个变量的秩统计量的相关系数\nKendall tau系数 当变量正相关性很强时， 任意两个观测的X值的大小顺序应该与Y值的大小顺序相同； 如果独立， 一对观测的X值比较和Y值比较顺序相同与顺序相反的数目应该基本相同。Kandall tau系数也是取值于区间[-1,1]， 用这样的思想表示两个变量的相关性和正负。\n1 cor.test(x,y,method = \u0026#34;spearman\u0026#34;) ## ## Spearman's rank correlation rho ## ## data: x and y ## S = 922, p-value = 0.000001339 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.7948832 1 cor.test(x,y,method = \u0026#34;kendall\u0026#34;) ## ## Kendall's rank correlation tau ## ## data: x and y ## T = 354, p-value = 0.000000159 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 0.6275862 一元回归 $$ Y=a+bX+\\varepsilon, \\varepsilon \\sim N(0,\\sigma^2) $$\n最小二乘法 $$ \\hat{b}=\\frac{\\sum_i(x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_i{(x-x_i)}^2}=r_{xy}\\frac{S_y}{S_x} $$\n$$ \\hat{a}=\\overline{y}-\\hat{b}\\overline{x} $$ 回归有效性可以用$R^2$和$p-valuie$来度量， $R^2=1-\\frac{SSE}{SST}$\n统计量$F=\\frac{SSR}{SSE/(n-2)}$,$p-value$为$P(F(1,n-2)\u0026gt;c)$,c为F的值。\n1 2 lm1 \u0026lt;- lm(y ~ x) summary(lm1) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.856 -4.549 2.141 6.048 9.664 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 9.9855 2.6930 3.708 0.000914 *** ## x 3.5396 0.4592 7.708 0.0000000214 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 7.302 on 28 degrees of freedom ## Multiple R-squared: 0.6797,\tAdjusted R-squared: 0.6683 ## F-statistic: 59.42 on 1 and 28 DF, p-value: 0.00000002136 1 2 #prediction predict(lm1,newdata =data.frame(x=c(5,10,15))) ## 1 2 3 ## 27.68364 45.38182 63.08000 1 pcutils::my_lm(y,x) 多元回归 1 2 lm2 \u0026lt;- lm(mpg ~ cyl + disp, data=mtcars) summary(lm2) ## ## Call: ## lm(formula = mpg ~ cyl + disp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4213 -2.1722 -0.6362 1.1899 7.0516 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 34.66099 2.54700 13.609 4.02e-14 *** ## cyl -1.58728 0.71184 -2.230 0.0337 * ## disp -0.02058 0.01026 -2.007 0.0542 . ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 3.055 on 29 degrees of freedom ## Multiple R-squared: 0.7596,\tAdjusted R-squared: 0.743 ## F-statistic: 45.81 on 2 and 29 DF, p-value: 0.000000001058 1 ggstatsplot::ggcoefstats(lm2) 1 2 #回归自变量筛选 lm3 \u0026lt;- step(lm(mpg ~ cyl + disp+hp+drat+vs, data=mtcars)) ## Start: AIC=77.08 ## mpg ~ cyl + disp + hp + drat + vs ## ## Df Sum of Sq RSS AIC ## - vs 1 0.3134 244.90 75.124 ## - cyl 1 7.6839 252.27 76.073 ## - drat 1 14.3330 258.92 76.905 ## - disp 1 14.6709 259.26 76.947 ## \u0026lt;none\u0026gt; 244.59 77.083 ## - hp 1 19.8255 264.41 77.577 ## ## Step: AIC=75.12 ## mpg ~ cyl + disp + hp + drat ## ## Df Sum of Sq RSS AIC ## - cyl 1 8.444 253.35 74.209 ## - disp 1 14.765 259.67 74.997 ## \u0026lt;none\u0026gt; 244.90 75.124 ## - drat 1 16.467 261.37 75.206 ## - hp 1 19.613 264.51 75.589 ## ## Step: AIC=74.21 ## mpg ~ disp + hp + drat ## ## Df Sum of Sq RSS AIC ## \u0026lt;none\u0026gt; 253.35 74.209 ## - drat 1 30.148 283.49 75.806 ## - disp 1 38.107 291.45 76.693 ## - hp 1 49.550 302.90 77.925 多重共线性\n狭义的多重共线性（multicollinearity）： 自变量的数据存在线性组合近似地等于零， 使得解线性方程组求解回归系数时结果不稳定， 回归结果很差。\n广义的多重共线性： 自变量之间存在较强的相关性， 这样自变量是联动的， 互相之间有替代作用。 甚至于斜率项的正负号都因为这种替代作用而可能是错误的方向。\n1 2 #car包的vif()函数计算方差膨胀因子 car::vif(lm3) ## disp hp drat ## 4.621988 2.868264 2.166843 非参数回归 所谓参数回归， 是指回归函数有预先确定的公式， 仅需要估计的未知参数； 非参数回归， 就是没有预先确定的公式， 的形式本身也依赖于输入的样本, 。 下面描述的核回归就是这样典型的非参数回归， 样条平滑、样条函数回归一般也看作是非参数回归。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #样条平滑 set.seed(1) nsamp \u0026lt;- 30 x \u0026lt;- runif(nsamp, -10, 10) xx \u0026lt;- seq(-10, 10, length.out=100) x \u0026lt;- sort(x) y \u0026lt;- 10*sin(x/10*pi)^2 + rnorm(nsamp,0,0.3) plot(x, y) curve(10*sin(x/10*pi)^2, -10, 10, add=TRUE, lwd=2) library(splines) res \u0026lt;- smooth.spline(x, y) lines(spline(res$x, res$y), col=\u0026#34;red\u0026#34;) res2 \u0026lt;- loess(y ~ x, degree=2, span=0.3) lines(xx, predict(res2, newdata=data.frame(x=xx)), col=\u0026#34;blue\u0026#34;) legend(\u0026#34;top\u0026#34;, lwd=c(2,1,1), col=c(\u0026#34;black\u0026#34;, \u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;), legend=c(\u0026#34;real data\u0026#34;, \u0026#34;smooth.spline\u0026#34;, \u0026#34;local lm\u0026#34;)) 1 2 3 4 ## 线性可加模型 ## R扩展包mgcv的gam()函数可以执行这样的可加模型的非参数回归拟合。 lm.rock \u0026lt;- lm(log(perm) ~ area + peri + shape, data=rock) summary(lm.rock) ## ## Call: ## lm(formula = log(perm) ~ area + peri + shape, data = rock) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.8092 -0.5413 0.1734 0.6493 1.4788 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 5.33314499 0.54867792 9.720 1.59e-12 *** ## area 0.00048498 0.00008657 5.602 1.29e-06 *** ## peri -0.00152661 0.00017704 -8.623 5.24e-11 *** ## shape 1.75652601 1.75592362 1.000 0.323 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.8521 on 44 degrees of freedom ## Multiple R-squared: 0.7483,\tAdjusted R-squared: 0.7311 ## F-statistic: 43.6 on 3 and 44 DF, p-value: 3.094e-13 1 2 gam.rock1 \u0026lt;- mgcv::gam(log(perm) ~ s(area) + s(peri) + s(shape), data=rock) summary(gam.rock1) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## log(perm) ~ s(area) + s(peri) + s(shape) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 5.1075 0.1222 41.81 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(area) 1.000 1.000 29.13 0.00000307 *** ## s(peri) 1.000 1.000 71.30 \u0026lt; 2e-16 *** ## s(shape) 1.402 1.705 0.58 0.437 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## R-sq.(adj) = 0.735 Deviance explained = 75.4% ## GCV = 0.78865 Scale est. = 0.71631 n = 48 1 plot(gam.rock1) 方差分析 单因素方差分析可以看成基础统计中两样本t检验的一个推广， 要比较试验观测值的某个因变量（称为“指标”）按照一个分组变量（称为“因素”）分组后， 各组的因变量均值有无显著差异。\n1 2 3 mtcars$cyl=as.factor(mtcars$cyl) aov.manu \u0026lt;- aov(mpg ~ cyl, data=mtcars) summary(aov.manu) ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## cyl 2 824.8 412.4 39.7 0.00000000498 *** ## Residuals 29 301.3 10.4 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 1 pcutils::group_box(mtcars[\u0026#34;mpg\u0026#34;],group = \u0026#34;cyl\u0026#34;,metadata = mtcars) 1 2 #非参数形式 kruskal.test(mpg ~ cyl, data=mtcars) ## ## Kruskal-Wallis rank sum test ## ## data: mpg by cyl ## Kruskal-Wallis chi-squared = 25.746, df = 2, p-value = 0.000002566 进行多个假设检验（如均值比较）的操作称为*“多重比较”*（multiple comparison， 或multiple testing）， 多次检验会使得总第一类错误概率增大。\n1 pcutils::multitest(mtcars$mpg,mtcars$cyl) ## ====================================1.ANOVA:==================================== ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## group 2 824.8 412.4 39.7 0.00000000498 *** ## Residuals 29 301.3 10.4 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ================================2.Kruskal.test:================================ ## ## Kruskal-Wallis rank sum test ## ## data: var by group ## Kruskal-Wallis chi-squared = 25.746, df = 2, p-value = 0.000002566 ## ## ==========================3.LSDtest, bonferroni p-adj:========================== ## var groups ## 4 26.66364 a ## 6 19.74286 b ## 8 15.10000 c ## ==================================4.tukeyHSD:================================== ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = var ~ group) ## ## $group ## diff lwr upr p adj ## 6-4 -6.920779 -10.769350 -3.0722086 0.0003424 ## 8-4 -11.563636 -14.770779 -8.3564942 0.0000000 ## 8-6 -4.642857 -8.327583 -0.9581313 0.0112287 ## ## =================================5.Wilcox-test:================================= ## 4 6 8 ## 4 1.00000000000 0.0006658148 0.00002774715 ## 6 0.00066581478 1.0000000000 0.00101304469 ## 8 0.00002774715 0.0010130447 1.00000000000 广义线性模型 泊松回归 1 2 3 4 5 6 counts \u0026lt;- c(18,17,15,20,10,20,25,13,12) outcome \u0026lt;- gl(3,1,9) treatment \u0026lt;- gl(3,3) D93=data.frame(treatment, outcome, counts) ## showing data ggplot(data = D93, mapping = aes(x = counts)) +geom_bar() 1 2 glm.D93 \u0026lt;- glm(counts ~ outcome + treatment,data = D93, family = poisson()) summary(glm.D93) ## ## Call: ## glm(formula = counts ~ outcome + treatment, family = poisson(), ## data = D93) ## ## Deviance Residuals: ## 1 2 3 4 5 6 7 8 ## -0.67125 0.96272 -0.16965 -0.21999 -0.95552 1.04939 0.84715 -0.09167 ## 9 ## -0.96656 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 3.045e+00 1.709e-01 17.815 \u0026lt;2e-16 *** ## outcome2 -4.543e-01 2.022e-01 -2.247 0.0246 * ## outcome3 -2.930e-01 1.927e-01 -1.520 0.1285 ## treatment2 -3.242e-16 2.000e-01 0.000 1.0000 ## treatment3 -2.148e-16 2.000e-01 0.000 1.0000 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 10.5814 on 8 degrees of freedom ## Residual deviance: 5.1291 on 4 degrees of freedom ## AIC: 56.761 ## ## Number of Fisher Scoring iterations: 4 逻辑斯谛回归 ….\n","date":"2023-03-24T00:00:00Z","image":"/p/r-statistics/index.en_files/figure-html/unnamed-chunk-4-1_hu6a7295393dc4d80def3bf7388e19d776_109022_120x120_fill_box_smart1_3.png","permalink":"/p/r-statistics/","title":"R-statistics"},{"content":"在Rblogdown使用stack-theme遇到了几个问题，应该是Rblogdown与原生hugo间的不太兼容。\n我很快发现了问题是Rmd产生的md文件可能跟stack主题不太匹配，一开始想看看yaml文件能不能修改解决，但找了不少地方都没找到😭。\n因为问题还比较确定，可以试试自己写脚本转换Rmd产生的md文件。\n公式问题 rblogdown 渲染Rmd成md会把我写的公式语法改变：\n例如： $a=sum_i^2$变成$a=sum_i^2$\n但是stack主题识别不了后面那种语法，这个比较简单：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import re # 定义一个正则表达式，匹配LaTeX数学公式中的起始和结束行内数学模式符号 pattern = r\u0026#39;\\\\\\((.*?)\\\\\\)\u0026#39; # 定义一个字符串，包含要处理的LaTeX字符串 latex_string = r\u0026#39;For the equation$\\sum_{i=1}^n i^2$\u0026#39; # 使用sub()函数替换匹配到的文本 processed_string = re.sub(pattern, r\u0026#39;$\\1$\u0026#39;, latex_string) # 输出处理后的字符串 print(processed_string) a=re.sub(r\u0026#39;`\\\\\\((.*?)\\\\\\)`\u0026#39;,r\u0026#39;$\\1$\u0026#39;,$a=sum_i^2$jjjh$a=sum_i^2$\u0026#34;) 图片大小调整 因为![from data to viz website](images/data2viz.png){width=60%}这种语法是stack主题不支持的， 但是\n1 2 3 \u0026#39;\u0026lt;p style=\u0026#34;text-align: center;\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;images/data2viz.png\u0026#34; width=\u0026#34;60%\u0026#34; title=\u0026#34;from data to viz website\u0026#34;/\u0026gt; \u0026lt;/p\u0026gt;\u0026#39; 这种语法是支持的，所以想写一个小脚本转换一下\n自己不太熟悉正则（上面那个公式转换的也是他帮我写的，哈哈），所以试试问了ChatGPT ：\n可以帮我写一个将插入图片的markdown语法转换为html语法的小脚本吗？ 比如：![A](B.png){width=60%}转换为\u0026lt;img src=\u0026quot;B.png\u0026quot; width=\u0026quot;60%\u0026quot; title=\u0026quot;A\u0026quot;/\u0026gt;\n它立马用python帮我写了：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import re def convert_markdown_to_html(markdown_text): # 将插入图片的Markdown语法转换为HTML语法 pattern = r\u0026#34;!\\[(.*?)\\]\\((.*?)\\){(.*?)}\u0026#34; matches = re.findall(pattern, markdown_text) for match in matches: title = match[0] src = match[1] attributes = match[2].split(\u0026#34;,\u0026#34;) attr_dict = dict([tuple(attribute.split(\u0026#34;=\u0026#34;)) for attribute in attributes]) html_tag = f\u0026#39;\u0026lt;img src=\u0026#34;{src}\u0026#34; title=\u0026#34;{title}\u0026#34;\u0026#39; for key, value in attr_dict.items(): html_tag += f\u0026#39; {key}=\u0026#34;{value}\u0026#34;\u0026#39; html_tag += \u0026#34;/\u0026gt;\u0026#34; markdown_text = markdown_text.replace(f\u0026#34;![{title}]({src}){{{match[2]}}}\u0026#34;, html_tag) return markdown_text def convert_latex_to_md(markdown_text): html_text=re.sub(r\u0026#39;`\\\\\\((.*?)\\\\\\)`\u0026#39;,r\u0026#39;$\\1$\u0026#39;,markdown_text) return html_text def convert_markdown_file_to_html(file_path): # 读取Markdown文件 with open(file_path, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: markdown_text = f.read() # 将Markdown语法转换为HTML语法 html_text = convert_markdown_to_html(markdown_text) html_text = convert_latex_to_md(html_text) # 将HTML文本写入文件 with open(file_path, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: f.write(html_text) if __name__ == \u0026#34;__main__\u0026#34;: file_path = \u0026#34;index.md\u0026#34; convert_markdown_file_to_html(file_path) 牛牛牛！ 再问一下，可以用R语言写一下吗？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 convert_markdown_to_html \u0026lt;- function(markdown_text) { # 通过正则表达式匹配图片语法 matches \u0026lt;- gregexpr(\u0026#34;!\\\\[(.*?)\\\\]\\\\((.*?)\\\\){(.*?)}\u0026#34;, markdown_text, perl = TRUE) # 遍历匹配结果，将Markdown语法替换为HTML语法 for (i in seq_along(matches[[1]])) { # 判断匹配结果是否为空列表 if (length(matches[[1]][i]) == 0) { next } match \u0026lt;- regmatches(markdown_text, matches[[1]][i]) title \u0026lt;- match[[2]] src \u0026lt;- match[[3]] attributes \u0026lt;- strsplit(match[[4]], \u0026#34;,\u0026#34;)[[1]] attr_dict \u0026lt;- setNames(sapply(strsplit(attributes, \u0026#34;=\u0026#34;), \u0026#34;[\u0026#34;, 2), sapply(strsplit(attributes, \u0026#34;=\u0026#34;), \u0026#34;[\u0026#34;, 1)) # 构造HTML标签 html_tag \u0026lt;- sprintf(\u0026#39;\u0026lt;img src=\u0026#34;%s\u0026#34; title=\u0026#34;%s\u0026#34;\u0026#39;, src, title) for (key in names(attr_dict)) { value \u0026lt;- attr_dict[[key]] html_tag \u0026lt;- paste(html_tag, sprintf(\u0026#39;%s=\u0026#34;%s\u0026#34;\u0026#39;, key, value), sep = \u0026#34; \u0026#34;) } html_tag \u0026lt;- paste(html_tag, \u0026#34;/\u0026gt;\u0026#34;, sep = \u0026#34;\u0026#34;) # 将Markdown语法替换为HTML语法 markdown_text \u0026lt;- sub(\u0026#34;!\\\\[(.*?)\\\\]\\\\((.*?)\\\\){(.*?)}\u0026#34;, html_tag, markdown_text, fixed = TRUE) } return(markdown_text) } markdown_text \u0026lt;- \u0026#34;![A](B.png){width=60%}\u0026#34; html_text \u0026lt;- convert_markdown_to_html(markdown_text) print(html_text) 唉，实际上这个代码没有用，我也不知道bug怎么搞，R的正则表达式一直感觉没有python好用。\n还是用写好的python脚本吧。\n修改新的md文件 我们还需要找到最新render出来的md文件按上述规则进行修改（否则一口气改全部的话，害怕会越来越慢，甚至损坏以前的md文件）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import os import datetime # 定义要查找的目录 directory = \u0026#39;./content/post/\u0026#39; # 存储所有的md文件 md_files = [] # 递归查找目录下的所有文件 for root, dirs, files in os.walk(directory): for file in files: if file.endswith(\u0026#39;.md\u0026#39;): file_path = os.path.join(root, file) md_files.append((file_path, os.path.getmtime(file_path))) # 找到最新修改的文件 latest_file = max(md_files, key=lambda x: x[1])[0] print(\u0026#34;最新保存的.md文件是：\u0026#34;, latest_file) 再问问，可以找到目录下所有新保存的.md文件吗，就是修改时间与上次不一致的？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import os import datetime # 定义要查找的目录 directory = \u0026#39;./content/\u0026#39; # 存储所有新保存的md文件 new_md_files = [] # 读取上一次检查的时间戳 if os.path.isfile(\u0026#39;last_check_time.txt\u0026#39;): with open(\u0026#39;last_check_time.txt\u0026#39;, \u0026#39;r\u0026#39;) as f: last_check_time = datetime.datetime.strptime(f.read(), \u0026#39;%Y-%m-%d %H:%M:%S.%f\u0026#39;) else: last_check_time = datetime.datetime.now() # 递归查找目录下的所有文件 for root, dirs, files in os.walk(directory): for file in files: if file.endswith(\u0026#39;.md\u0026#39;): file_path = os.path.join(root, file) mod_time = datetime.datetime.fromtimestamp(os.path.getmtime(file_path)) if mod_time \u0026gt; last_check_time: new_md_files.append(file_path) # 更新上一次检查的时间戳 last_check_time = datetime.datetime.now() # 将上一次检查的时间戳保存到文件中 with open(\u0026#39;last_check_time.txt\u0026#39;, \u0026#39;w\u0026#39;) as f: f.write(last_check_time.strftime(\u0026#39;%Y-%m-%d %H:%M:%S.%f\u0026#39;)) # 打印所有新保存的md文件 for file_path in new_md_files: print(\u0026#34;新保存的.md文件：\u0026#34;, file_path) #更新所有新保存的md文件 for file_path in new_md_files: convert_markdown_file_to_html(latest_file) 这个思路真不错，哈哈\n解决方法 调教了一下ChatGPT后，就得到了整个更新问题的方法了： 首先整理一个refresh_md_to_fit_stack.py放在网站根目录下， 然后在R文件夹的（如果你是用Rblogdown生成的网站，那就会有这个文件夹）的build2.R中加入一行：\nsystem(\u0026quot;python refresh_md_to_fit_stack.py\u0026quot;)\n即可。 因为Rblogdown会在每次实时渲染我们的网站后运行build2.R文件，相当于运行了refresh_md_to_fit_stack.py文件，而refresh_md_to_fit_stack.py文件将搜索所有更新的md文件并修改以上两个问题，这样hugo建立的网站就能够契合stack-theme。\n看看结果： 如果调试成功的话，下面显示的即是正常的latex公式:\n$y=\\frac{1}{2} $\n如果不是使用的Rblogdown生成的网站，这种思路的方法也可以帮助改变stack-theme。\n此外，如果是Rblogdown生成的其他主题的网站有类似问题，也可以这样修改。\n最后，ChatGPT真不错！再来一个攻略\n","date":"2023-03-23T00:00:00Z","image":"/p/chatgpt-stack/ChatGPT_huce8d906dd473ab038e4f784f810328ac_7886_120x120_fill_q75_box_smart1.jpg","permalink":"/p/chatgpt-stack/","title":"ChatGPT帮我调整stack主题"},{"content":"Introduction Zhou, J. \u0026amp; Ning, D. Stochastic Community Assembly: Does It Matter in Microbial Ecology? Microbiol Mol Biol Rev 81, e00002-17 (2017). This review is very comprehensive (1)！\n周集中老师实验室的长期研究兴趣集中在从基因组到生态系统的不同组织层面的环境微生物学，其中一项就是宏基因组学和微生物生态学：利用高通量基因组测序和相关基因组学技术检查不同栖息地的微生物群落多样性、微生物生物地理学和形成微生物多样性模式、分布和动态的机制；\nUnderstanding the mechanisms controlling community diversity, functions, succession, and biogeography is a central, but poorly understood, topic in ecology, particularly in microbial ecology. Traditional niche-based theory hypothesizes that deterministic factors such as species traits, inter-species interactions (e.g., competition, predation, mutualisms, and trade-offs), and environmental conditions (e.g., pH, temperature, salt, and moisture) govern community structure, which are often referred to as deterministic processes.\nIn contrast, neutral theory assumes that community structures are independent of species traits and governed by stochastic processes of birth, death, colonization, extinction, and speciation.\nAlthough, recently, it has been generally accepted that both deterministic and stochastic processes occur simultaneously in the assembly of local communities, a central debate is on their relative importance in controlling community structure, succession, and biogeography.\nMethods 目前在文章中普遍出现的计算群落构建的方法主要有三套：\n1.Stegen （βNTI \u0026amp; RCbray-based） 原始的文章 (2), (3)\n在确定性与随机性二分法的背景下塑造微生物群落多样性的生态过程。该方案显示了在文中讨论的假设下，基于系统发育和分类多样性划分各种生态过程的不同步骤。 NTI（最近分类单元指数）基于系统发育多样性指数 MNTD（平均最近分类单元距离）的空模型检验，RCBray（修正的 Raup-Crick 指数）基于 Bray-Curtis 的空模型检验分类多样性指数。这两个框分别表示确定性选择和未支配部分的主要组成部分。除了影响较小的选择外，未支配部分的弱选择也可能是由于抵消了有影响的选择因素和/或不同分类群的对比选择。该图主要是根据 Stegen 等人先前报告的数据制作的。\nbeta-NTI（nearest taxon index）： 使用 NTI/NRI 指示单个群落内共存的分类单元相比偶然预期的关系更为紧密还是分散，使用 βNTI/βNRI 指示两两群落间的变化受确定性或随机性因素影响的大小。 MNTD 即mean-nearest-taxon-distance (最近种间平均进化距离) ， 关于 NTI/NRI、βNTI/βNRI 的结果解释时其显著性的判断依据是其临界值是否大于|2|，实际上这是依据标准正态分布的 95%置信区间得到的，一般认为 NRI 或者 NTI 大于 1.96 或者小于-1.96 的结果，在 95%的水平是显著的。\nRCbray: 相异指数的一种。\ncode https://blog.csdn.net/weixin_43367441/article/details/118515090\n非常重要的一点：这类方法是基于系统发育树推断的，所以一定要有跟tax表对应的系统发育树。 但事实上，除非用的是16S/ITS测序，否则我们很难给自己的数据建一个树。 所以，替代方法是利用阶层分类关系（界门纲目科属种）来做一个拟系统发育树（枝长都为1） 参见自己写的df2tree函数\n1 2 3 4 5 data(otutab) df2tree(taxonomy)-\u0026gt;phylo #计算 nti_rc(otutab,phylo,metadata[,\u0026#34;Group\u0026#34;,drop=F])-\u0026gt;nti_res ## [1] \u0026quot;Result saved as nti_rc_res.rda\u0026quot; 1 2 3 4 5 #可视化 nti_res$type=factor(nti_res$type,levels = c(\u0026#34;Homo_S\u0026#34;,\u0026#34;Heter_S\u0026#34;,\u0026#34;Homo_D\u0026#34;,\u0026#34;D_limit\u0026#34;,\u0026#34;Undominated\u0026#34;)) table(nti_res$type,nti_res$variable)%\u0026gt;%reshape2::melt()-\u0026gt;com_p colnames(com_p)=c(\u0026#34;type\u0026#34;,\u0026#34;variable\u0026#34;,\u0026#34;n\u0026#34;) ggplot(com_p,aes(x=variable,y=n))+geom_bar(stat = \u0026#34;identity\u0026#34;,aes(fill=type),position = \u0026#34;fill\u0026#34;) iCAMP 现在大家经常用的iCAMP包就是基于这个理论框架进行分析的 (4)。\n2.ST，NST，MST （stochasticity ratio） PNAS：NST方法定量生态过程中的随机性\n周老师的文章，开发出新的方法对生态过程中的随机性进行了定量。提出了一个新的指数，normalized stochasticity ratio (NST)，作为确定性主导（deterministic, \u0026lt;50%）和随机性主导（stochastic, \u0026gt;50%）的边界点 (5)。\ncode 已经有相应的包了,NST\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #install.packages(\u0026#34;NST\u0026#34;) library(NST) data(tda) comm=tda$comm bray=beta.g(comm,dist.method=\u0026#34;bray\u0026#34;) bray.3col=dist.3col(bray) group=tda$group tnst=tNST(comm=comm, group=group, rand=20, output.rand=TRUE, nworker=1) #检验各组ST、NST的分布情况及各组ST、NST差异的显著性。 nst.bt=nst.boot(nst.result=tnst, group=NULL, rand=99, trace=TRUE, two.tail=FALSE, out.detail=FALSE, between.group=FALSE, nworker=1) #ST和NST组间进行Permutational multivariate ANOVA nst.pova=nst.panova(nst.result=tnst, rand=99) #可视化 pcutils::group_box(tnst$index.pair.grp,col = 8,group = tnst$index.pair.grp$group) example 地下水微生物群落演替过程中估计的 NST 动态变化对乳化植物油注入的响应。 NST 是基于 (A) Jaccard 和 (B) Ru ziˇ cka 指标使用空模型算法 PF 计算的。在零模型 PF 中，类群发生的概率与观察到的发生频率成正比，并且每个样本中的类群丰富度是固定的（19）。当使用基于丰度的指标 Ru ziˇ cka 时，每个样本中的空分类群丰度被计算为观察到的个体数量的随机抽取，其概率与样本中空分类群的区域相对丰度成比例（26）。 W8 是植物油对其没有影响或影响最小的对照井。\n3.Solan NCM 只有当物种死亡或离开这个系统时，群落结构才会发生改变。此时，离开个体的生态位就会空余出来，其它个体会通过来自群落外的迁移或群落内部的繁殖来填补空出的生态位。因此可以把群落的动态描述为死亡——繁殖/扩散——死亡这样的循环 (6)。\n计算公式：\n-$Pr(\\frac{N_i+1}{N_i})=(\\frac{N_T-N_i}{N_T})[mp_i+(1+\\alpha_i)(1-m)(\\frac{N_i}{N_T-1})$\n-$Pr(\\frac{N_i}{N_i})=\\frac{N_i}{N_T}[mp_i+{(1+\\alpha_i)}(1-m)(\\frac{N_i-1}{N_T-1})]+(\\frac{N_T-N_i}{N_T})[m(1-p_i)+{red}{(1-\\alpha_i)}(1-m)(\\frac{N_T-N_i-1}{N_T-1})]$\n-$Pr(\\frac{N_i-1}{N_i})=\\frac{N_i}{N_T}[m(1-p_i)+(1-\\alpha_i)(1-m)(\\frac{N_T-N_i}{N_T-1})]$\n物种 i 占居的频率（occurrence frequency: row sums of binary OTU table/number of sites）为其概率密度函数的积分。 此时该分布是一个beta分布，我们就可以在R语言中利用beta分布对其进行[拟合]，获得参数m的评估值。\ncode 1 2 3 4 library(devtools) install_github(\u0026#34;Russel88/MicEco\u0026#34;) library(MicEco) neutral.fit(t(otutab))#使用的是最大似然估计拟合模型，R2计算方法也不同 另一个代码来自https://mp.weixin.qq.com/s/opFXl-TvkJfmPcWKFwhCFA，比较经典，用的是非线性模型:\nN描述了宏群落规模（metacommunity size），在本文中为每个样本中所有OTU的总丰度。 m量化了群落层面的迁移率（migration rate），该值对于每个群落成员都是统一的（与物种无关），m值越小说明整个群落中物种扩散越受限制，反之m值越高则表明物种受到扩散限制越低。 Nm是元群落规模（N）与迁移率（m）的乘积 (Nm = N*m)，量化了对群落之间扩散的估计，决定了发生频率和区域相对丰度之间的相关性。\n自己的绘图代码：\n1 2 3 data(otutab) ncm(otutab)-\u0026gt;ncm_res plot(ncm_res) R2代表了中性群落模型的整体拟合优度，R2越高表明越接近中性模型，即群落的构建受随机性过程的影响越大，受确定性过程的影响越小。\n值得注意，R方不是某个数值的平方，可以是负值。因为拟合程度没有下限，可以无限差，R方的范围是(−∞,1]。R方的取值，有以下的可能性：\n等于1。理想状况，该模型对所有的真值预测准确，没有偏差。泼个冷水，如果某篇文章里出现了R方=1，要么是问题过于简单没有研究价值，要么是模型过于复杂，对数据进行了过度拟合。\n小于1大于0。这是常见状况，表明该模型的拟合水平比均值模型好。\n等于0。该模型的拟合水平接近于均值模型。该模型没有价值。\n小于0。该模型的拟合水平不如均值模型。同样，该模型没有价值。\nexample **随机过程，例如散布、出生、死亡、灭绝和移民，在呼气细菌和真菌群落的组装中发挥作用。**真菌微生物群的 SNM 拟合性能（图 2）优于细菌（细菌 R2 = 0.353，真菌 R2 = 0.683）。这种差异表明随机过程对于 EBC 真菌群落的组装可能相对更重要，而对 EBC 细菌群落的组装则不太重要。这种现象可能与细菌和真菌之间的大小差异有关，较小的细菌受扩散限制的影响较小，而受确定性过程的影响更大 (7)。\nStochastic processes played a role in assembling expiratory bacterial (A) and fungal (B) communities based on Sloan neutral model fitting. The black solid line represents the best fit, and the dotted lines represent the 95% CI (confidence interval) around the model fit. The blue dots refer to taxa that occur more frequently than predicted, and the red dots refer to taxa that occur less frequently than predicted. The green dots refer to taxa that occur in a manner consistent with predicted values.\n原核生物细胞直径大概在0.5 ~ 2.0 μm； 微真核生物个体大小大概在1 ~ 200 mm； 之前的研究已经报道了较小的生物不太可能受到扩散限制的影响，因为它们与较大的生物相比具有更高的扩散能力。因此假设在细菌群落中生态位过程的相对影响可能比在微真核生物群落中更强。微真核生物个体大，可能会影响其扩散过程。\nOthers 参考自历神公众号文章：https://mp.weixin.qq.com/s/nwNuPlY7x6VScJA44c0MjQ\nCompetitive lottery 基于竞争彩票模型(competitive lottery model)的群落构建 假设一个两步模型:在第一步中，样本的总丰度(100%)根据某个未知的过程在组之间分配。 然后在第二步中，分配给每个小组的丰度会根据一个竞争彩票模式在小组成员之间进行分配。 竞争彩票模型的概念说明:\n在第一阶段，每个样本的总丰度(100%)在一组预定义的组之间进行分割。\n在第二阶段，每一组的丰度分配根据竞争彩票模型在子组之间进行分配，其中一个子组获得了大部分的丰度。\nDNCI 在PER-SIMPER方法的基础上，提出了一个新的度量指标: dispersal–niche continuum index (DNCI)，该指数可估计是扩散过程还是生态位过程主导群落的构建，并便于不同数据集之间的比较。\nPER-SIMPER利用物种在不同站点之间的矩阵，在排列过程中生成三种不同的零模型： 通过约束行(生态位构建)、约束列(扩散构建)或两者都约束。 PER-SIMPER利用SIMPER方法对原始群落矩阵组成相似度模式进行建模，并与三个零模型比较。 PER-SIMPER通过识别哪个零模型与经验分析最匹配来进行定性评估。然而，大多数群落都是由生态位和扩散过程共同构成的，这限制了构建机制定性方法的敏感性。此外简单决策过程的定性特性使不同群落之间的构建过程难以进行精确比较。\nPER-SIMPER分析返回三个E-metric分布,这与SIMPER经验值与三个PER-SIMPER零模型之间的偏差相关。本文提出的新的DNCI是由这些计算的E值推导而来的。 定量结果是基于标准效应量En(即来自”生态位”模型的E-metric分布)减去标准效应量Ed (即来自”扩散”模型的E-metric分布)。 DNCI提供了一种方法来量化和比较跨数据集构建过程的强度。DNCI值正或负表明生态位或分散过程分别是群组构建的主要过程。指数的绝对值越高，代表占主导地位的构建过程的潜力越大。 $DNCI=SES_d-SES_n=\\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{E_{d(i)}-\\overline {E_{dn}}}{\\sigma E_{dn}}\\right)-\\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{E_{n(i)}-\\overline {E_{dn}}}{\\sigma E_{dn}}\\right)$\n如果DNCI与0差异不显著，则可以认为扩散过程和生态位过程对群落构建的影响是相等的。 当DNCI显著低于0时，扩散过程是群落构建的主导驱动因素; 如果DNCI显著高于0，生态位过程是群落构建的主要决定因素。 注意，表明扩散过程优势的负DNCI值并不能提供实际扩散速率的信息。\nReferences 1. J. Zhou, D. Ning, Stochastic Community Assembly: Does It Matter in Microbial Ecology? Microbiology and Molecular Biology Reviews. 81, e00002–17 (2017).\n2. J. C. Stegen, X. Lin, J. K. Fredrickson, X. Chen, D. W. Kennedy, C. J. Murray, M. L. Rockhold, A. Konopka, Quantifying community assembly processes and identifying features that impose them. The ISME Journal. 7, 2069–2079 (2013).\n3. J. C. Stegen, X. Lin, A. E. Konopka, J. K. Fredrickson, Stochastic and deterministic assembly processes in subsurface microbial communities. The ISME Journal. 6, 1653–1664 (2012).\n4. D. Ning, M. Yuan, L. Wu, Y. Zhang, X. Guo, X. Zhou, Y. Yang, A. P. Arkin, M. K. Firestone, J. Zhou, A quantitative framework reveals ecological drivers of grassland microbial community assembly in response to warming. Nature Communications. 11, 4717 (2020).\n5. D. Ning, Y. Deng, J. M. Tiedje, J. Zhou, A general framework for quantitatively assessing ecological stochasticity. Proceedings of the National Academy of Sciences. 116, 16892–16898 (2019).\n6. W. T. Sloan, M. Lunn, S. Woodcock, I. M. Head, S. Nee, T. P. Curtis, Quantifying the roles of immigration and chance in shaping prokaryote community structure. Environmental Microbiology. 8, 732–740 (2006).\n7. Y. Zhang, F. Shen, Y. Yang, M. Niu, D. Chen, L. Chen, S. Wang, Y. Zheng, Y. Sun, F. Zhou, H. Qian, Y. Wu, T. Zhu, Insights into the Profile of the Human Expiratory Microbiota and Its Associations with Indoor Microbiotas. Environmental Science \u0026amp; Technology. 56, 6282–6293 (2022).\n","date":"2023-03-17T00:00:00Z","image":"/p/community-assembly/comm_hub260f10f12bf64e0acb74c7ee8e930d5_36974_120x120_fill_q75_box_smart1.jpg","permalink":"/p/community-assembly/","title":"微生物群落构建（community assembly）"},{"content":"简介 linux, python和R语言应该是生信学习中最重要的几个部分。\nlinux是一种操作系统，由于许多生物信息学软件仅提供支持linux平台的命令行版本，而不是图形化界面，所以掌握该操作系统的基础文件管理，安装使用软件，输入输出逻辑，简单shell脚本编写等十分重要。\npython与R都是免费开源多平台的编程语言，并且都是较为容易学习上手的模仿自然语言语法的编程语言，且支持面向对象编程，两者在数据科学上都有着非常重要的应用。Python的生态系统非常丰富，拥有大量的数据科学库和框架，例如：NumPy、Pandas、Matplotlib、Seaborn、Scikit-learn、PyTorch、TensorFlow等，这些库可以帮助数据科学家快速处理和分析数据，构建模型。\nR是一种专门用于统计分析和数据可视化的语言，其生态系统也非常丰富，拥有大量的统计分析库和可视化工具，例如：ggplot2、dplyr、tidyr、shiny等。 R拥有非常强大的数据分析和可视化能力，它提供了很多专业的统计分析函数和图形，这些函数和图形可以直接用于数据分析和可视化，使分析和可视化变得更加轻松和高效。\n在此，我想先记录一下我使用的最多的R语言。\n常用功能 全面教程 小白上手首推北大李老师的课程讲义《R语言教程》，中文讲义，简单易读，甚至整本书就是用Rbookdown编写的，电子书也比较好复制代码进行学习。\n《R语言实战》,也有中文版，介绍地更加全面且有不少进阶知识，值得一看！\n然后是著名的英文书和参考手册：\n《An Introduction to R》 《R Cookbook》 更多关于R的书可以在Home | Bookdown这里找到。\n数据处理 我目前用的最多的是dplyr包进行数据处理，最开始是冲着类似linux下的|管道符去的，%\u0026gt;%符号真的挺好用的，可以让我们无需产生许多中间结果 (准确来说%\u0026gt;%来自magrittr包，并且R4.1+也支持了原生管道符|\u0026gt;，但用起来还是%\u0026gt;%顺手，还有%\u0026lt;\u0026gt;%等变体)：\n1 2 library(dplyr) head(iris)%\u0026gt;%select(Species)%\u0026gt;%unique() 随后便发现了tidyverse全家桶，其中包含ggplot2，tibble，dplyr，readr，stringr等常用的全面的数据处理包，能够很好的满足我们下有数据处理需求：\n读取数据\n清洗数据\n转换数据\n合并数据\n筛选数据\n可视化\n这里给出官方教程：《R for Data Science》，非常建议熟读并使用。\n可视化 可视化一直是R的突出优点，basic graphic和ggplot是我最常用的绘图系统（偶尔会用plotly画一些交互图形，但大多数是只需要presentation和文章里要用的静态图形），两者的绘图逻辑有所差异，推荐在学会R的基础使用和数据处理后就体验ggplot。\nggplot是由Hadley Wickham创建的，并被包含在R语言的ggplot2包中。主要思想是将数据可视化看作是图层的堆叠，其中每个图层代表了一个数据属性的可视化。ggplot提供了一系列的函数和参数，让用户能够轻松地组合这些图层，以及调整颜色、标签、轴等其他属性。\nggplot支持的图形类型包括散点图、折线图、直方图、密度图、箱线图等等，可以满足大多数数据可视化的需求。优点在于其语法简洁、易于理解和使用，并且生成的图表质量高、美观、易于理解。它也被广泛地应用于学术研究、数据分析、商业报告等领域。\n同样给出官方教程：《ggplot2: Elegant Graphics for Data Analysis》，优雅，太优雅了。这本书也是可以找到大佬们翻译的中文版的，可以当作作图工具书使用，许多无法直接Google到的细节问题可能能在书中找到。\n此外要提一下ggpubr包，其名为Publication Ready Plots，提供一些方便的做出出版级figure的函数。ggpubr: Publication Ready Plots - Articles - STHDA 这个网站提供了不少好看图的绘制流程。\n当掌握了基本的ggplot绘制逻辑后，还可以尝试各种基于同样逻辑的扩展包，如ggcor，ggtree等等。\ngg_extensions 这个网站收集了很多此类拓展包并支持制作上传自创的gg拓展（当然暂时有点难😮‍💨）\n最后，强烈推荐一个为可视化而建的网站From data to Viz | Find the graphic you need (data-to-viz.com)，就像网站名一样，丰富多彩的可视化形式帮我们找到适合自己数据的样式，并提供画某一类图的python，R甚至D3.js代码，真的非常方便地copy代码，替换我们的数据，少量的修改就能有很不错的效果😄。\nR编程 R是一个宝库，几乎总能在上面找到合适的包实现我们想要的功能，但他也是一种编程语言，熟悉其编程语法对进阶使用十分重要。\n基本的数据读取与输出，if else、switch、for循环与简单函数编写需要掌握。\n同时因为R是一个比较\u0026quot;慢\u0026quot;的语言，尽量使用向量式编程、apply替代for循环、RCpp编写函数等方法提高其效率（后面可能会专门讲讲）。\n这里推荐大佬的《Advanced R》，对R高级编程技术进行讲解。\nRmarkdown R Markdown是一种文本格式和工具，用于创建具有代码、文本、图形和输出结果的可重复性报告。它是在R语言环境中开发的，但也可以与其他编程语言（如Python）结合使用。\n使用R Markdown，您可以将代码、文本和图形组合到一个文件中，并使用R Markdown语法将它们组织成结构化的文档。然后，您可以使用R Markdown编译器将文档转换为HTML、PDF、Word或其他格式的报告。\nR Markdown具有以下优点：\n可重复性：报告中的所有代码和结果都是可重复的，这有助于确保研究结果的可靠性和透明度。\n效率：使用R Markdown，您可以在单个文件中组织和记录所有内容，从而节省时间和精力。\n灵活性：R Markdown支持多种格式和输出选项，因此您可以根据需要自定义报告的样式和布局。\nYihui Xie大佬的官方文档介绍：R Markdown: The Definitive Guide (bookdown.org)\n以及参考书：R Markdown Cookbook (bookdown.org)\nRbookdown R的bookdown扩展包(https://github.com/rstudio/bookdown) 是继knitr和rmarkdown扩展包之后， 另一个增强markdown格式的扩展， 使得Rmd格式可以支持公式、定理、图表自动编号和引用、链接， 文献引用和链接等适用于编写书籍的功能。相当方便，我也稍微尝试写了一下：MetaNet Tutorial (asa12138.github.io)。\n参考书：bookdown: Authoring Books and Technical Documents with R Markdown\nRblogdown R扩展包blogdown可以与Hugo软件配合制作简单的静态网站。 网站的所有文件都存在于一个目录中， 只要上传到任意的网站服务器就可以发布， 没有任何限制。这篇博客就是基于Rblogdown写的，还是非常友好易上手的。\n参考书：blogdown: Creating Websites with R Markdown (bookdown.org)\n3.21更新：\n大多数网站的修改要在themes中实现，比如我现在使用的主题不支持latex渲染，想要加上这个功能，我先找到了yihui大佬的主题实现这个功能的Javascript代码，即hugo-lithium/layouts/partials/footer_mathjax.html，然后把它复制到目前用的主题的footer目录下（可能没有这个目录，而是footer.html同目录下），记得在include.html或者其他layout文件中加入{{ partial \u0026quot;footer/footer_mathjax.html\u0026quot; . }}，这样就能顺利调用mathjax了。\nR Presentation R Markdown文件(.Rmd)支持生成网页格式的幻灯片(slidy_presentation)， 以及LaTeX beamer格式的PDF幻灯片(beamer_presentation)， 和Microsoft Office的PowerPoint幻灯片(powerpoint_presentation)格式。\n目前稍微尝试了一些Yihui大佬的\u0026quot;写轮眼\u0026quot;包，但没有试着做自己的slides，以后有机会一定试试。\n写R包 包是可重现 R 代码的基本单元，包括可重用的 R 函数、描述如何使用它们的文档和示例数据。一个包将代码、数据、文档和测试捆绑在一起，并且很容易与他人共享。\n积极尝试（以后讲讲）\n参考书：R Packages (2e) (r-pkgs.org)\n制作shiny应用 Shiny 是一个 R 包，可轻松地直接从 R 构建交互式 Web 应用程序。还可以在网页上托管独立应用程序或将它们嵌入 R Markdown 文档或构建仪表板，还可以使用 CSS 主题、html 小部件和 JavaScript 操作扩展您的 Shiny 应用程序。\n积极尝试（以后讲讲）\n官方教程：Shiny Learning Resources\n参考书：Mastering Shiny\n","date":"2023-03-16T00:00:00Z","image":"/p/r-learning/images/data2viz_hu2949f787b14e5432e8dbe93255a87ee8_671473_120x120_fill_box_smart1_3.png","permalink":"/p/r-learning/","title":"R语言学习"},{"content":"最近倒腾了不少新玩意，比如这个博客系统，在此写下第一篇博客。\n从刚开始接触生信到现在已经3年了，我也从本科生变成了博士生，专业从生物科学变成了生物信息学。走上科研道路，可能是种种巧合与选择的结果，但现在脑子里想的就是将二十来年的各种知识运用在研究上，做一些有意思的东西。\n希望自己有空可以多更新，帮助自己整理思路，记录一些实验和数据分析的方法技巧，调节一下心情😂。\n","date":"2023-03-14T00:00:00Z","image":"/p/%E6%98%A5%E4%B9%8B%E4%BC%8A%E5%A7%8B/spring_hu0533df8c600b6ebf5e775c57536fef00_394558_120x120_fill_q75_box_smart1.jpeg","permalink":"/p/%E6%98%A5%E4%B9%8B%E4%BC%8A%E5%A7%8B/","title":"春之伊始"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nFormula $a=sum_i^2$ $a=sum_i$\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nHyperlinked image The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-03-11T00:00:00Z","image":"/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash_hud7e36f7e20e71be184458283bdae4646_55974_120x120_fill_q75_box_smart1.jpg","permalink":"/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"}]